<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sharp-rookie.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

<script> 
   (function(){
          if(''){
              if (prompt('请输入文章密码') !== ''){
                  alert('密码错误！');
                  history.back();
              }
          }
      })();
  </script>
  <meta name="description" content="&amp;emsp;&amp;emsp;强化学习 (Reinforcement Learning) 是一个机器学习大家族中的分支, 由于近些年来的技术突破, 和深度学习 (Deep Learning) 的整合, 使得强化学习有了进一步的运用。比如让计算机学着玩游戏, AlphaGo 挑战世界围棋高手, 都是强化学习在行的事。强化学习让程序从对当前环境完全陌生, 成长为一个在环境中游刃有余的高手。">
<meta property="og:type" content="article">
<meta property="og:title" content="【强化学习】基础知识">
<meta property="og:url" content="https://sharp-rookie.github.io/2021/12/28/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/index.html">
<meta property="og:site_name" content="最忆是江南">
<meta property="og:description" content="&amp;emsp;&amp;emsp;强化学习 (Reinforcement Learning) 是一个机器学习大家族中的分支, 由于近些年来的技术突破, 和深度学习 (Deep Learning) 的整合, 使得强化学习有了进一步的运用。比如让计算机学着玩游戏, AlphaGo 挑战世界围棋高手, 都是强化学习在行的事。强化学习让程序从对当前环境完全陌生, 成长为一个在环境中游刃有余的高手。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211229085342.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090245.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211227234235.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085646.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085858.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085745.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085806.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228092413.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085947.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090008.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090154.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090245.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090541.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090616.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228091126.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228091621.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228140008.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228140030.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228140047.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228092413.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228135735.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228095438.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228095500.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228141900.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228135332.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085858.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228102751.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228110419.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228110505.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228110514.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228105615.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228112323.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211229100620.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228112010.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228142817.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228111541.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228111604.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228135203.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228134525.png">
<meta property="article:published_time" content="2021-12-28T06:03:10.934Z">
<meta property="article:modified_time" content="2022-01-03T08:25:54.110Z">
<meta property="article:author" content="Lrk612">
<meta property="article:tag" content="Reinforcement Learning">
<meta property="article:tag" content="Algorithm">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211229085342.png">

<link rel="canonical" href="https://sharp-rookie.github.io/2021/12/28/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>【强化学习】基础知识 | 最忆是江南</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
        <a target="_blank" rel="noopener" href="https://github.com/Sharp-rookie" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">最忆是江南</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Love actually is all around</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th-list fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-friendlink">

    <a href="/friendlink/" rel="section"><i class="fa fa-link fa-fw"></i>友链</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-download fa-fw"></i>资源</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sharp-rookie.github.io/2021/12/28/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Lrk612">
      <meta itemprop="description" content="Lrk's blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="最忆是江南">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【强化学习】基础知识
        </h1>

        <div class="post-meta">
         
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-28 14:03:10" itemprop="dateCreated datePublished" datetime="2021-12-28T14:03:10+08:00">2021-12-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/" itemprop="url" rel="index"><span itemprop="name">技术分享</span></a>
                </span>
            </span>

          
            <span id="/2021/12/28/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/" class="post-meta-item leancloud_visitors" data-flag-title="【强化学习】基础知识" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/12/28/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/12/28/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>&emsp;&emsp;强化学习 (Reinforcement Learning) 是一个机器学习大家族中的分支, 由于近些年来的技术突破, 和深度学习 (Deep Learning) 的整合, 使得强化学习有了进一步的运用。比如让计算机学着玩游戏, AlphaGo 挑战世界围棋高手, 都是强化学习在行的事。强化学习让程序从对当前环境完全陌生, 成长为一个在环境中游刃有余的高手。</p>
<span id="more"></span>

<p>&nbsp;</p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="在机器学习中的位置"><a href="#在机器学习中的位置" class="headerlink" title="在机器学习中的位置"></a>在机器学习中的位置</h3><p>&emsp;&emsp;强化学习是和监督学习，非监督学习并列的第三种机器学习方法。</p>
<p>&emsp;&emsp;强化学习和监督学习最大的区别是它没有监督学习已经准备好的训练数据输出值。强化学习只有奖励值，但是这个奖励值和监督学习的输出值不一样，它不是事先给出的，而是延后给出的。同时，强化学习的每一步与时间顺序前后关系紧密。而监督学习的训练数据之间一般都是独立的，没有这种前后的依赖关系。</p>
<p>&emsp;&emsp;强化学习和非监督学习的区别。也还是在奖励值这个地方。非监督学习是没有输出值也没有奖励值的，它只有数据特征。同时和监督学习一样，数据之间也都是独立的，没有强化学习这样的前后依赖关系。</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211229085342.png" alt="image-20211229085342314" style="zoom:50%;" />

<h3 id="基本模型"><a href="#基本模型" class="headerlink" title="基本模型"></a>基本模型</h3><p><strong>目标：</strong>找到一个策略 a = π(s) 来确定在状态 s 下选择的动作 a，从而最大化价值函数：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090245.png" alt="image-20211228090244956" style="zoom:50%;" />

<table>
<thead>
<tr>
<th>组成</th>
<th>内容</th>
</tr>
</thead>
<tbody><tr>
<td>两个主体</td>
<td>Agent、Environment</td>
</tr>
<tr>
<td>状态、行动和策略</td>
<td>Agent依据策略Policy做出在当前状态State下采取的行动Action，环境Environment给出相应的新状态State</td>
</tr>
<tr>
<td>状态转化模型</td>
<td>相当于一个概率状态机、一个概率模型，即在状态s下采取动作a，转到下一个状态s′的概率，表示为Pss′a或P(s’|s,a)</td>
</tr>
<tr>
<td>两种奖励</td>
<td>行动价值Action_Value（当前奖励+衰减折算后的后续奖励，有远见性）、行动奖励Reward（仅当前的奖励R(s,a)）</td>
</tr>
<tr>
<td>衰减因子与探索率</td>
<td>衰减因子γ决定Action_Value的远见性，探索率决定Agent采取Action时的创新性(ε-greedy)</td>
</tr>
</tbody></table>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211227234235.png" alt="image-20211227234234718" style="zoom: 33%;" />

<h3 id="所解决问题的特点"><a href="#所解决问题的特点" class="headerlink" title="所解决问题的特点"></a>所解决问题的特点</h3><ul>
<li>智能体Agent和环境Environment之间不断进行交互</li>
<li>搜索和试错</li>
<li>延迟奖励（当前所做的动作可能很多步之后才会产生相应的结果）</li>
</ul>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h2 id="马尔可夫决策过程MDP"><a href="#马尔可夫决策过程MDP" class="headerlink" title="马尔可夫决策过程MDP"></a>马尔可夫决策过程MDP</h2><p>&emsp;&emsp;在强化学习中，agent与environment一直在互动。在每个时刻t，agent会接收到来自环境的状态s，基于这个状态s，agent会做出动作a，然后这个动作作用在环境上，于是agent可以接收到一个奖赏Rt+1Rt+1，并且agent就会到达新的状态。所以，其实agent与environment之间的交互就是产生了一个序列：</p>
<p>&emsp;&emsp;<code>S0,A0,R1,S1,A1,R2,...</code></p>
<p>&emsp;&emsp;我们称这个为序列决策过程。而马尔科夫决策过程（Morkov Decision Process）就是一个典型的序列决策过程的一种公式化。有了马尔科夫的假设，在求解强化学习模型时才比较方便。</p>
<h4 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h4><h5 id="MDP定义"><a href="#MDP定义" class="headerlink" title="MDP定义"></a>MDP定义</h5><p>下一个状态的产生只和当前的状态有关，即：</p>
<p>&emsp;&emsp;<code>P(St+1 | St) = P(St+1 | S1,...,St)</code></p>
<p>直观上讲，下一个状态的产生跟所有历史状态是有关的，也就是等式右边所示。但是Markov的定义则是忽略掉历史信息，只保留了当前状态的信息来预测下一个状态，这称为无后效性/马尔可夫性。</p>
<p>一个 MDP 可以由一个 5 元组表示为 <code>M = &lt;S,A,P(s&#39;|s,a),R,γ&gt;</code>：</p>
<p><strong>S</strong> → 一个状态（state）的集合<br><strong>P</strong> → 一个受行为影响的状态转移概率矩阵<br><strong>A</strong> →一个有限动作集<br><strong>R</strong> →  一个用于计算回报的函数<br><strong>γ</strong> → 一个折扣因子，用于做未来回报计算时的衰减系数，γ ∈ ( 0 , 1 )<br>&nbsp;</p>
<h5 id="状态转移概率Pss′a"><a href="#状态转移概率Pss′a" class="headerlink" title="状态转移概率Pss′a"></a>状态转移概率Pss′a</h5><p>对于一个具体的状态s和它的下一个状态s’ ，它们的状态转移概率(就是从s转移到s’的概率)定义为：</p>
<p>&emsp;&emsp;<code>Pss′a = P(St+1=s′|St=s,At=a)</code></p>
<p>也就是说，下一个状态的产生只受到当前状态和动作的影响。</p>
<p>&nbsp;</p>
<h5 id="策略π"><a href="#策略π" class="headerlink" title="策略π"></a>策略π</h5><p>&emsp;&emsp;<code>Policy： π(a|s) = P(At=a|St=s)</code></p>
<p>policy π表示的是在给定的state下，一个关于action的概率分布。即表示在一个状态s下，agent接下来可能会采取的任意一个action的概率分布（可能一开始不知道概率是多少）。对于每一个状态s都会有这样一个π(a|s)，所有状态的π(a|s）就形成整体策略π。策略π是指所有状态都要使用这个策略，不是单独指某一个状态。</p>
<p>无论怎样，我们的目标是最大化累积奖赏，所以我们可以通过不断地改进我们的策略，使得我们最后能够获得最大累积奖赏。</p>
<p>&nbsp;</p>
<h5 id="状态价值函数Vπ"><a href="#状态价值函数Vπ" class="headerlink" title="状态价值函数Vπ"></a>状态价值函数Vπ</h5><p>在策略π和状态s下，采取一系列行动会到达终止状态。期间得到的累计折扣回报就是该策略下状态s的价值，用Vπ(s)表示。因为从状态s开始到终止状态的路径不唯一，为了确定性的衡量状态s的价值，Vπ(s)一般是一个期望函数：<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085646.png" alt="image-20211228085646607" style="zoom: 50%;" /></p>
<p>物理意义：如果到达了状态s，那么接下来直到到达终止状态的回报值就是Vπ(s) 。</p>
<p>t时刻状态St和t+1时刻状态St+1之间的递推关系（贝尔曼方程)：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085858.png" alt="image-20211228085858501" style="zoom:50%;" />

<p>计算机通常使用第二个递推式迭代地求出MDP中每个状态的价值函数，而不是用公式一硬算。</p>
<p>&nbsp;</p>
<h5 id="动作价值函数Qπ"><a href="#动作价值函数Qπ" class="headerlink" title="动作价值函数Qπ"></a>动作价值函数<em>Qπ</em></h5><p>在状态s下做动作a后的即时奖励虽然是确定的，但是依概率转移到下一个状态s′这个s′是不确定的。因此通过求期望来消除价值函数的不确定性：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085745.png" alt="image-20211228085745128" style="zoom:50%;" />

<p>​                                                                              <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085806.png" alt="image-20211228085806768" style="zoom:50%;" /></p>
<p>t时刻动作At和t+1时刻动作At+1之间的递推关系（贝尔曼方程)：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228092413.png" alt="image-20211228092413197" style="zoom: 25%;" />

<p>计算机通常使用第二个递推式迭代地求出MDP中每个状态的价值函数，而不是用公式一硬算。</p>
<p>&nbsp;</p>
<h5 id="状态价值与动作价值的递推关系"><a href="#状态价值与动作价值的递推关系" class="headerlink" title="状态价值与动作价值的递推关系"></a>状态价值与动作价值的递推关系</h5><p>状态价值函数是所有动作价值函数基于策略π的期望：<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085947.png" style="zoom:50%;" /></p>
<p>利用上贝尔曼方程，也很容易从状态价值函数vπ(s)vπ(s)表示动作价值函数qπ(s,a)qπ(s,a)，即：</p>
<p>​                                                                    <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090008.png" style="zoom:50%;" /></p>
<p>二者转化关系如图：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090154.png" alt="image-20211228090154012" style="zoom: 33%;" />

<p>&emsp;&emsp;总而言之，状态 / 动作价值有两部分相加组成，第一部分是即时奖励，第二部分是环境所有可能出现的下一个状态的概率乘以该下一状态的状态价值，最后求和，并加上衰减。两式联立得：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090245.png" alt="image-20211228090244956" style="zoom:50%;" />

<p>&nbsp;</p>
<h4 id="最优价值函数"><a href="#最优价值函数" class="headerlink" title="最优价值函数"></a>最优价值函数</h4><p>&emsp;&emsp;解决强化学习问题意味着要寻找一个最优的策略让个体在与环境交互过程中获得始终比其它策略都要多的收获，这个最优策略可以用 π∗表示。一旦找到这个最优策略π∗，那么就解决了这个强化学习问题。一般来说，找到一个最优策略很难，但是可以通过比较若干不同策略的优劣来确定一个较好的策略，也就是局部最优解。而策略的优劣可由价值函数反映，因此最优策略——&gt;最优价值函数：</p>
<ol>
<li><strong>最优状态价值函数</strong>是所有策略下产生的众多状态价值函数中的最大者：<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090541.png" alt="image-20211228090541867" style="zoom:50%;" /></li>
<li><strong>最优动作价值函数</strong>是所有策略下产生的众多动作状态价值函数中的最大者：<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090616.png" alt="image-20211228090616747" style="zoom: 50%;" /></li>
</ol>
<p><strong>转化关系：</strong><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228091126.png" alt="image-20211228091126423" style="zoom: 50%;" /></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h2 id="模型求解"><a href="#模型求解" class="headerlink" title="模型求解"></a>模型求解</h2><h3 id="强化学习两个基本问题"><a href="#强化学习两个基本问题" class="headerlink" title="强化学习两个基本问题"></a>强化学习两个基本问题</h3><h6 id="预测问题"><a href="#预测问题" class="headerlink" title="预测问题"></a>预测问题</h6><p>给定：状态集S, 动作集A, 模型状态转化概率矩阵P, 即时奖励R，衰减因子γ,  策略π，求解该策略的状态价值函数v(π)</p>
<h6 id="控制问题"><a href="#控制问题" class="headerlink" title="控制问题"></a>控制问题</h6><p>给定：状态集S, 动作集A, 模型状态转化概率矩阵P, 即时奖励R，衰减因子γ，求解最优策略π和最优状态价值函数v(π)</p>
<h6 id="贝尔曼期望方程"><a href="#贝尔曼期望方程" class="headerlink" title="贝尔曼期望方程"></a>贝尔曼期望方程</h6><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228091621.png" alt="image-20211228091621142" style="zoom: 67%;" />

<p>&nbsp;</p>
<h3 id="动态规划DP（Model-based）"><a href="#动态规划DP（Model-based）" class="headerlink" title="动态规划DP（Model-based）"></a>动态规划DP（Model-based）</h3><h4 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h4><p>问题的最优解可以由若干小问题的最优解构成，即通过寻找子问题的最优解来得到问题的最优解。</p>
<p>可以找到子问题状态之间的递推关系，通过较小的子问题状态递推出较大的子问题的状态。</p>
<p>&nbsp;</p>
<h4 id="同步动态规划算法"><a href="#同步动态规划算法" class="headerlink" title="同步动态规划算法"></a>同步动态规划算法</h4><h5 id="策略评估求解预测问题"><a href="#策略评估求解预测问题" class="headerlink" title="策略评估求解预测问题"></a>策略评估求解预测问题</h5><p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228140008.png" alt="image-20211228140008098"></p>
<p>&emsp;&emsp;从任意一个状态价值函数开始，依据给定的策略，结合贝尔曼期望方程、状态转移概率和奖励同步迭代更新状态价值函数，直至其收敛，得到该策略下最终的状态价值函数。（反复迭代至平稳分布）</p>
<h5 id="策略迭代求解控制问题"><a href="#策略迭代求解控制问题" class="headerlink" title="策略迭代求解控制问题"></a>策略迭代求解控制问题</h5><p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228140030.png" alt="image-20211228140030255"></p>
<p>&emsp;&emsp;根据之前基于任意一个给定策略评估得到的状态价值来及时调整动作策略，这个方法叫做策略迭代(Policy Iteration)。最简单的调整方法就是贪婪法：个体在某个状态下选择的行为是其能够到达后续所有可能的状态中状态价值最大的那个状态。</p>
<ol>
<li>使用当前策略π∗评估计算当前策略的最终状态价值v∗</li>
<li>根据状态价值v∗以一定的方法（比如贪婪法）更新策略π∗</li>
<li>回到第一步，一直迭代下去，最终得到收敛的策略π∗和状态价值v∗</li>
</ol>
<h5 id="价值迭代求解控制问题"><a href="#价值迭代求解控制问题" class="headerlink" title="价值迭代求解控制问题"></a>价值迭代求解控制问题</h5><p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228140047.png" alt="image-20211228140046899"></p>
<p>&emsp;&emsp;相比策略迭代，不需要等到状态价值收敛才调整策略，而是随着状态价值的迭代及时调整策略, 这样可以大大减少迭代次数。此时的状态价值的更新方法也和策略迭代不同，此时贝尔曼方程迭代式如下：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228092413.png" alt="image-20211228092413197" style="zoom: 25%;" />

<p>&emsp;&emsp;可见由于策略调整，现在价值每次更新倾向于贪婪法选择的最优策略对应的后续状态价值，这样收敛更快。</p>
<p>&nbsp;</p>
<h4 id="异步动态规划算法"><a href="#异步动态规划算法" class="headerlink" title="异步动态规划算法"></a>异步动态规划算法</h4><p>&emsp;&emsp;上面几个都是同步动态规划算法，即每轮迭代我会计算出所有的状态价值并保存起来，在下一轮中使用这些保存起来的状态价值来计算新一轮的状态价值。另一种动态规划求解是异步动态规划算法，在这些算法里，每一次迭代并不对所有状态的价值进行更新，而是依据一定的原则有选择性的更新部分状态的价值，这类算法有自己的一些独特优势，当然有额会有一些额外的代价。</p>
<h5 id="原位动态规划-in-place-DP"><a href="#原位动态规划-in-place-DP" class="headerlink" title="原位动态规划 (in-place DP)"></a>原位动态规划 (in-place DP)</h5><p>&emsp;&emsp;此时我们不会另外保存一份上一轮计算出的状态价值。而是即时计算即时更新。这样可以减少保存的状态价值的数量，节约内存。代价是收敛速度可能稍慢。</p>
<h5 id="优先级动态规划-prioritized-sweeping-DP"><a href="#优先级动态规划-prioritized-sweeping-DP" class="headerlink" title="优先级动态规划 (prioritized sweeping DP)"></a>优先级动态规划 (prioritized sweeping DP)</h5><p>&emsp;&emsp;该算法对每一个状态进行优先级分级，优先级越高的状态其状态价值优先得到更新。通常使用贝尔曼误差来评估状态的优先级，贝尔曼误差即新状态价值与前次计算得到的状态价值差的绝对值。这样可以加快收敛速度，代价是需要维护一个优先级队列。</p>
<h5 id="实时动态规划-real-time-DP"><a href="#实时动态规划-real-time-DP" class="headerlink" title="实时动态规划 (real-time DP)"></a>实时动态规划 (real-time DP)</h5><p>&emsp;&emsp;实时动态规划直接使用个体与环境交互产生的实际经历来更新状态价值，对于那些个体实际经历过的状态进行价值更新。这样个体经常访问过的状态将得到较高频次的价值更新，而与个体关系不密切、个体较少访问到的状态其价值得到更新的机会就较少。收敛速度可能稍慢。</p>
<p>&nbsp;</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>&emsp;&emsp;动态规划算法主要就是利用贝尔曼方程来迭代更新状态价值，用贪婪法之类的方法迭代更新最优策略。使用全宽度（full-width）的回溯机制来进行状态价值的更新，也就是说，无论是同步还是异步动态规划，在每一次回溯更新某一个状态的价值时，都要回溯到该状态的所有可能的后续状态，并利用贝尔曼方程更新该状态的价值。这种全宽度的价值更新方式对于状态数较少的强化学习问题还是比较有效的，但是当问题规模很大的时候，动态规划算法将会因贝尔曼维度灾难而无法使用。因此我们还需要寻找其他的针对复杂问题的强化学习问题求解方法。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h3 id="蒙特卡罗法（Model-free）"><a href="#蒙特卡罗法（Model-free）" class="headerlink" title="蒙特卡罗法（Model-free）"></a>蒙特卡罗法（Model-free）</h3><h4 id="不基于模型的强化学习问题"><a href="#不基于模型的强化学习问题" class="headerlink" title="不基于模型的强化学习问题"></a>不基于模型的强化学习问题</h4><p>&emsp;&emsp;在上面动态规划法中，模型状态转化概率矩阵P始终是已知的，即MDP已知，一般称这样的强化学习问题为Model-based的强化学习问题。Model-based强化学习问题可以通过动态规划来评估一个给定的策略，通过不断迭代最终得到最优价值函数，具体的做法有两个：一个是策略迭代，一个是值迭代。</p>
<p>&emsp;&emsp;然而有很多强化学习问题没有办法事先得到模型状态转化概率Pss’a，即不基于模型的<strong>Model-free</strong>强化学习（面向黑盒子学习），为了能够从环境中学习，需要让agent与environment交互，得到一些经历（样本）。然后通过这些经历来进行策略评估与策略迭代，从而最终得到最优策略。这种做法的理论是从蒙特卡罗方法（Monte-Carlo）中来的。</p>
<p>&nbsp;</p>
<h4 id="蒙特卡罗法"><a href="#蒙特卡罗法" class="headerlink" title="蒙特卡罗法"></a>蒙特卡罗法</h4><p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228135735.png" alt="image-20211228135716095"></p>
<p>&emsp;&emsp;蒙特卡罗法又叫做统计模拟方法，通过采样若干经历完整的状态序列(episode)后求均值来估计状态的真实价值。所谓的经历完整，就是这个序列<strong>必须是达到终点</strong>的，这样才能拿到最终回报。比如下棋问题要分出输赢，驾车问题要成功到达终点或者失败。有了很多组这样经历完整的状态序列，我们不需要借助模型本身就可以近似的估计状态价值，进而求解预测和控制问题。</p>
<p>&emsp;&emsp;采样得到的完整序列示例如下：</p>
<p>&emsp;&emsp;<code>S1,A1,R2,S1,A2,R3,...St,At,Rt+1,...,RT,ST</code></p>
<p><strong>对比动态规划</strong></p>
<ul>
<li>不需要依赖于模型状态转化概率</li>
<li>从经历过的完整序列学习，完整的经历越多，学习效果越好</li>
</ul>
<p>&nbsp;</p>
<h4 id="求解预测问题（基于价值）"><a href="#求解预测问题（基于价值）" class="headerlink" title="求解预测问题（基于价值）"></a>求解预测问题（基于价值）</h4><p>&emsp;&emsp;对于蒙特卡罗法来说，如果要求某一个状态的状态价值，只需要求出所有的完整序列（采到的样本）中该状态的收获R的平均值即可近似求解：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228095438.png" alt="image-20211228095438741" style="zoom:50%;" />

<p>其中</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228095500.png" alt="image-20211228095500637" style="zoom:50%;" />

<p>&emsp;&emsp;动作价值求解方法同上。</p>
<p><strong>算法优化</strong></p>
<ol>
<li><p>同样一个状态可能在一个完整的状态序列中重复出现，第一种应对方法是仅把状态序列中第一次出现该状态时的收获值纳入到收获平均值的计算中；另一种是针对一个状态序列中每次出现的该状态，都计算对应的收获值并纳入到收获平均值的计算中。两种方法对应的蒙特卡罗法分别称为：<strong>首次访问(first visit)</strong> 和<strong>每次访问(every visit)</strong> 蒙特卡罗法。第二种方法比第一种的计算量要大一些，但是在完整的经历样本序列少的场景下会比第一种方法适用。</p>
</li>
<li><p>**累进更新平均值（incremental mean)**。在上面预测问题的求解公式里，有一个average的公式，意味着要保存所有该状态的收获值之和最后取平均，这样浪费了太多的存储空间。一个较好的方法是在迭代计算收获均值，即每次保存上一轮迭代得到的收获均值与次数，当计算得到当前轮的收获时，即可计算当前轮收获均值和次数。</p>
</li>
</ol>
<p>&nbsp;</p>
<h4 id="求解控制问题（基于价值）"><a href="#求解控制问题（基于价值）" class="headerlink" title="求解控制问题（基于价值）"></a>求解控制问题（基于价值）</h4><p>&emsp;&emsp;蒙特卡罗法求解控制问题的思路和动态规划价值迭代的的思路类似。每轮迭代先做策略评估，计算出价值vk(s)，然后基于据一定的方法（比如贪婪法）更新当前策略π。最后得到最优价值函数v∗和最优策略π∗。</p>
<p>和动态规划比，蒙特卡罗法不同之处体现在三点：</p>
<p>​        1.  预测问题策略评估的方法不同，动态规划用贝尔曼方程迭代，蒙特卡罗法用采样数据求平均</p>
<p>​        2. 蒙特卡罗法一般是优化最优动作价值函数q∗，而不是状态价值函数v∗</p>
<p>​        3. 动态规划一般基于贪婪法更新策略，而蒙特卡罗法一般采用<strong>ϵ−greedy</strong>法更新（m是可选行动的个数）</p>
<p>​             <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228141900.png" alt="image-20211228141900761" style="zoom: 25%;" /></p>
<p>（在实际求解控制问题时，为了使算法可以收敛，一般ϵ会随着算法的迭代过程逐渐减小并趋于0。这样在迭代前期，我们鼓励探索，而在后期，由于我们有了足够的探索量，开始趋于保守，以贪婪为主，使算法可以稳定收敛）</p>
<p><strong>算法流程：（以在线学习on-policy的every-visit蒙特卡罗法为例）</strong></p>
<p>输入：状态集S、动作集A、即时奖励R、衰减因子γ、探索率ϵ</p>
<p>输出：最有动作价值函数q∗、最优策略π∗</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1. 初始化所有的动作价值Q(s, a)=0，状态次数N(s, a)=0，采样次数k=0，随机初始化一个策略π</span><br><span class="line"></span><br><span class="line">2. k=k+1，基于策略π进行第k次蒙特卡罗采样，得到一个完整的状态序列:</span><br><span class="line">			S1,A1,R2,S1,A2,R3,...St,At,Rt+1,...,RT,ST</span><br><span class="line"></span><br><span class="line">3. 对于该状态序列里出现的每一状态行为对(St,At)，计算其收获Gt, 更新其计数N(s,a)和行为价值函数Q(s,a)：</span><br><span class="line">			    Gt = 对n累加到T[(γ^(t-n))*(Rt+n)]</span><br><span class="line">			    N(s, a) ++</span><br><span class="line">			    Q(s, a) += (Gt-Q(s, a))/N(s,a)</span><br><span class="line"></span><br><span class="line">4. 基于新计算出的动作价值，更新当前的ϵ−greedy策略：</span><br><span class="line">			    ϵ = 1/k</span><br><span class="line">			    π = ϵ−greedy()</span><br><span class="line"></span><br><span class="line">5. 若所有的Q(s,a)收敛，则对应的所有Q(s,a)即为最优的动作价值函数q∗，对应的策略π(a|s)即为最优策略π∗，否则转到第二步</span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h4 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h4><p>&emsp;&emsp;蒙特卡罗法是不基于模型的强化问题求解方法，它可以避免动态规划求解过于复杂，同时还可以不事先知道环境转化模型，因此可以用于海量数据和复杂模型。但是它每次采样都需要一个完整的状态序列，如果没有完整的状态序列，或者很难拿到较多的完整的状态序列，因此在实际问题中蒙特卡罗法用的不多，所以还需要寻找其他的更灵活的不基于模型的强化问题求解方法。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h3 id="时序差分法TD（Model-free）"><a href="#时序差分法TD（Model-free）" class="headerlink" title="时序差分法TD（Model-free）"></a>时序差分法TD（Model-free）</h3><h4 id="时序差分法"><a href="#时序差分法" class="headerlink" title="时序差分法"></a>时序差分法</h4><p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228135332.png" alt="image-20211228135332814"></p>
<p>&emsp;&emsp;时序差分法和蒙特卡罗法类似，都是不基于模型的强化学习问题求解方法。蒙特卡罗法中Gt的计算需要完整序列，而时序差分法没有完整的状态序列，只有部分的状态序列，由贝尔曼方程：</p>
<p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085858.png" alt="image-20211228085858501" style="zoom:50%;" /> 可知，可以用<code>Rt+1+γv(St+1)</code>来近似的代替收获Gt</p>
<p>一般我们把<code>Rt+1+γV(St+1)</code>称为<strong>目标值</strong>，<code>Rt+1+γV(St+1)−V(St)</code>称为<strong>误差</strong>，将用目标值近似代替收获G(t)的过程称为<strong>引导</strong>(bootstrapping)。这样只需要两个连续的状态与对应的奖励，就有了近似收获Gt的表达式，可以去求解时序差分的预测问题和控制问题了。</p>
<p>&nbsp;</p>
<h4 id="求解预测问题（基于价值）-1"><a href="#求解预测问题（基于价值）-1" class="headerlink" title="求解预测问题（基于价值）"></a>求解预测问题（基于价值）</h4><p>&emsp;&emsp;时序差分的预测问题求解和蒙特卡罗法类似，但是主要有两个不同点：</p>
<ol>
<li> 收获Gt的表达式不同</li>
</ol>
<ul>
<li>蒙特卡罗 G(t) 需要遍历完整过程才能得出：<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228102751.png" alt="image-20211228102751434" style="zoom: 50%;" /></li>
<li>时序差分 G(t)记录连续两个状态就可得出，称为 <strong>TD Target</strong> ：<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228110419.png" alt="image-20211228110419330" style="zoom: 50%;" /></li>
</ul>
<ol start="2">
<li> 迭代式子的系数不同，时序差分没有完整的序列，也就没有对应的次数N(St, At)，一般用一个[0,1]的系数α代替：</li>
</ol>
<ul>
<li>蒙特卡罗:    <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228110505.png" alt="image-20211228110505164" style="zoom:50%;" /></li>
<li>时序差分:   <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228110514.png" alt="image-20211228110514567" style="zoom:50%;" /></li>
</ul>
<p>&emsp;&emsp;（并且蒙特卡罗法一般只优化动作价值函数Q，时序差分状态价值、动作价值函数都会优化）</p>
<p>&nbsp;</p>
<h5 id="对比蒙特卡罗法"><a href="#对比蒙特卡罗法" class="headerlink" title="对比蒙特卡罗法"></a>对比蒙特卡罗法</h5><ol>
<li>时序差分法在知道最终结果之前就可以学习，甚至没有最终结果也可以学习，还可以在持续进行的环境中学习。而蒙特卡罗法则要等到最后结果才能学习，时序差分法可以更快速灵活的更新状态的价值估计，这在某些情况下有着非常重要的实际意义。</li>
<li>时序差分法在更新状态价值时使用的是<strong>TD目标值</strong>，即基于即时奖励和下一状态的预估价值来替代当前状态在状态序列结束时可能得到的收获，是当前状态价值的<strong>有偏估计</strong>，而蒙特卡罗法则使用实际的收获来更新状态价值，是某一策略下状态价值的<strong>无偏估计</strong>，这一点蒙特卡罗法占优。</li>
<li>虽然时序差分法得到的价值是有偏估计，但是其方差却比蒙特卡罗法得到的方差要低，且对初始值敏感，通常比蒙特卡罗法更加高效。</li>
</ol>
<h5 id="n步时序差分："><a href="#n步时序差分：" class="headerlink" title="n步时序差分："></a>n步时序差分：</h5><p>&emsp;&emsp;上面用TD Target代替收获Gt的公式中，St只向前一步到St+1，若用向前2步、n步来近似代替Gt，则为n步时序差分：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228105615.png" alt="image-20211228105615429" style="zoom: 33%;" />

<p>&emsp;&emsp;（当n越来越大，趋于无穷，或者说趋于使用完整的状态序列时，n步时序差分就等价于蒙特卡罗法了）</p>
<p>&nbsp;</p>
<h4 id="求解控制问题（基于价值）-1"><a href="#求解控制问题（基于价值）-1" class="headerlink" title="求解控制问题（基于价值）"></a>求解控制问题（基于价值）</h4><p>&emsp;&emsp;蒙特卡罗法在线控制的方法使用的是ϵ−greedy法来做价值迭代，时序差分也可以用ϵ−greedy法来价值迭代，和蒙特卡罗法在线控制的区别主要只是在于收获的计算方式不同。时序差分的在线控制(on-policy)算法最常见的是Sarsa算法。</p>
<p>&emsp;&emsp;除了在线控制，还可以做离线控制(off-policy)。离线控制和在线控制的区别主要在于在线控制一般只有一个策略(最常见的是ϵ−greedy法)。而离线控制一般有两个策略，其中一个策略(最常见的是ϵ−greedy法)用于选择新的动作，另一个策略(最常见的是贪婪法)用于更新价值函数。时序差分的离线控制算法最常见的是Q-Learning算法。</p>
<p>&nbsp;</p>
<h4 id="TD-λ"><a href="#TD-λ" class="headerlink" title="TD(λ)"></a>TD(λ)</h4><p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228112323.png" alt="image-20211228112323589"></p>
<p>&emsp;&emsp;n步时序差分选择多少步数是一个超参数调优问题(一般3~10比较好)。为了能在不增加计算复杂度的情况下综合考虑所有步数的预测，引入一个[0,1]新的参数λ，定义收获是n从1到∞所有步的收获乘以权重的和：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211229100620.png" alt="image-20211229100620726" style="zoom:50%;" />

<p>每一步的权重是<code>(1−λ)*λ^(n−1)</code></p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228112010.png" alt="image-20211228112009881" style="zoom: 33%;" />

<p>&emsp;&emsp;进而有价值函数：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228142817.png" alt="image-20211228142817279" style="zoom:25%;" />

<p>&emsp;&emsp;从前向来看，一个状态的价值V(St)由Gt得到，而Gt又间接由所有后续状态价值计算得到，因此可以认为更新一个状态的价值需要知道所有后续状态的价值。也就是说，必须要经历完整的状态序列获得包括终止状态的每一个状态的即时奖励才能更新当前状态的价值。这和蒙特卡罗法的要求一样，因此TD(λ)有着和蒙特卡罗法一样的劣势。当λ=0 时，就是上面的普通时序差分法；当λ=1时，就是蒙特卡罗法。</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228111541.png" alt="image-20211228111540959" style="zoom:25%;" />

<p>&emsp;&emsp;从反向来看，它可以分析我们状态对后续状态的影响。比如老鼠在依次连续接受了3 次响铃和1 次亮灯信号后遭到了电击，那么在分析遭电击的原因时，到底是响铃的因素较重要还是亮灯的因素更重要呢？如果把老鼠遭到电击的原因认为是之前接受了较多次数的响铃，则称这种归因为<strong>频率启发(frequency heuristic) 式</strong>；而把电击归因于最近少数几次状态的影响，则称为<strong>就近启发(recency heuristic) 式</strong>。</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228111604.png" alt="image-20211228111604749" style="zoom:25%;" />



<p><strong>效用迹E</strong></p>
<p>&emsp;&emsp;给每一个状态引入一个数值：<strong>效用值</strong> 来表示该状态对后续状态的影响，就可以同时利用到上述两个启发(同事反映二者)。而所有状态的效用值总称为**效用迹(eligibility traces, ES)**。定义为：</p>
<p>​                        <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228135203.png" alt="image-20211228135203250" style="zoom:25%;" />    </p>
<p>&emsp;&emsp;此时价值函数为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228134525.png" alt="image-20211228134525809" style="zoom: 25%;" />

<p>&nbsp;</p>
<h4 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h4><p>&emsp;&emsp;时序差分和蒙特卡罗法比它更加灵活，学习能力更强，因此是目前主流的强化学习求解问题的方法，现在绝大部分强化学习乃至深度强化学习的求解都是以时序差分的思想为基础的。</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Lrk612
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://sharp-rookie.github.io/2021/12/28/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/" title="【强化学习】基础知识">https://sharp-rookie.github.io/2021/12/28/【强化学习】基础概念/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
              <a href="/tags/Algorithm/" rel="tag"># Algorithm</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/12/27/%E3%80%90%E8%AE%BA%E6%96%87%E3%80%91Network%20Traffic%20%20Characterization%20%20Using%20Token%20%20Bucket%20%20Model/" rel="prev" title="Token Bucket最优参数的数学推导">
      <i class="fa fa-chevron-left"></i> Token Bucket最优参数的数学推导
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/12/29/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%EF%BC%9ASarsa&Q-Lreaning/" rel="next" title="【强化学习】时序差分模型：SARSA & Q-Learning">
      【强化学习】时序差分模型：SARSA & Q-Learning <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-text">基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE"><span class="nav-text">在机器学习中的位置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B"><span class="nav-text">基本模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%80%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%E7%9A%84%E7%89%B9%E7%82%B9"><span class="nav-text">所解决问题的特点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8BMDP"><span class="nav-text">马尔可夫决策过程MDP</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5-1"><span class="nav-text">基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#MDP%E5%AE%9A%E4%B9%89"><span class="nav-text">MDP定义</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E8%BD%AC%E7%A7%BB%E6%A6%82%E7%8E%87Pss%E2%80%B2a"><span class="nav-text">状态转移概率Pss′a</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%CF%80"><span class="nav-text">策略π</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0V%CF%80"><span class="nav-text">状态价值函数Vπ</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8A%A8%E4%BD%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0Q%CF%80"><span class="nav-text">动作价值函数Qπ</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC%E4%B8%8E%E5%8A%A8%E4%BD%9C%E4%BB%B7%E5%80%BC%E7%9A%84%E9%80%92%E6%8E%A8%E5%85%B3%E7%B3%BB"><span class="nav-text">状态价值与动作价值的递推关系</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-text">最优价值函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%B1%82%E8%A7%A3"><span class="nav-text">模型求解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%A4%E4%B8%AA%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98"><span class="nav-text">强化学习两个基本问题</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E9%97%AE%E9%A2%98"><span class="nav-text">预测问题</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%8E%A7%E5%88%B6%E9%97%AE%E9%A2%98"><span class="nav-text">控制问题</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%9F%E6%9C%9B%E6%96%B9%E7%A8%8B"><span class="nav-text">贝尔曼期望方程</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92DP%EF%BC%88Model-based%EF%BC%89"><span class="nav-text">动态规划DP（Model-based）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92"><span class="nav-text">动态规划</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%8C%E6%AD%A5%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95"><span class="nav-text">同步动态规划算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0%E6%B1%82%E8%A7%A3%E9%A2%84%E6%B5%8B%E9%97%AE%E9%A2%98"><span class="nav-text">策略评估求解预测问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E6%B1%82%E8%A7%A3%E6%8E%A7%E5%88%B6%E9%97%AE%E9%A2%98"><span class="nav-text">策略迭代求解控制问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3%E6%B1%82%E8%A7%A3%E6%8E%A7%E5%88%B6%E9%97%AE%E9%A2%98"><span class="nav-text">价值迭代求解控制问题</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%82%E6%AD%A5%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95"><span class="nav-text">异步动态规划算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8E%9F%E4%BD%8D%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-in-place-DP"><span class="nav-text">原位动态规划 (in-place DP)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BC%98%E5%85%88%E7%BA%A7%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-prioritized-sweeping-DP"><span class="nav-text">优先级动态规划 (prioritized sweeping DP)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9E%E6%97%B6%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-real-time-DP"><span class="nav-text">实时动态规划 (real-time DP)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%B3%95%EF%BC%88Model-free%EF%BC%89"><span class="nav-text">蒙特卡罗法（Model-free）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98"><span class="nav-text">不基于模型的强化学习问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%B3%95"><span class="nav-text">蒙特卡罗法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3%E9%A2%84%E6%B5%8B%E9%97%AE%E9%A2%98%EF%BC%88%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%EF%BC%89"><span class="nav-text">求解预测问题（基于价值）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3%E6%8E%A7%E5%88%B6%E9%97%AE%E9%A2%98%EF%BC%88%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%EF%BC%89"><span class="nav-text">求解控制问题（基于价值）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93-1"><span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%B3%95TD%EF%BC%88Model-free%EF%BC%89"><span class="nav-text">时序差分法TD（Model-free）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%B3%95"><span class="nav-text">时序差分法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3%E9%A2%84%E6%B5%8B%E9%97%AE%E9%A2%98%EF%BC%88%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%EF%BC%89-1"><span class="nav-text">求解预测问题（基于价值）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%B3%95"><span class="nav-text">对比蒙特卡罗法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#n%E6%AD%A5%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%EF%BC%9A"><span class="nav-text">n步时序差分：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3%E6%8E%A7%E5%88%B6%E9%97%AE%E9%A2%98%EF%BC%88%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%EF%BC%89-1"><span class="nav-text">求解控制问题（基于价值）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TD-%CE%BB"><span class="nav-text">TD(λ)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93-2"><span class="nav-text">小结</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Lrk612"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Lrk612</p>
  <div class="site-description" itemprop="description">Lrk's blog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.csdn.net/" title="https:&#x2F;&#x2F;www.csdn.net&#x2F;" rel="noopener" target="_blank">CSDN</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.google.com/" title="https:&#x2F;&#x2F;www.google.com" rel="noopener" target="_blank">Google</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <!-- 访问量 -->

    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


<!-- 版权 -->
<div class="copyright">
  
  &copy; 2021-12-1 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lrk612</span>
</div>

<!-- ICP备案 -->
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP备案 </a>
      <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211209170739.png" style="display: inline-block;"><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=2021035798" rel="noopener" target="_blank">豫ICP备2021035798号 </a>
  </div>

<!-- 不知道是什么 -->
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'fH6OdGBcCG8D6jC8NeeJlX9b-gzGzoHsz',
      appKey     : '1UoIb6vszMw6wl0n7kreb4Xz',
      placeholder: "Just go go ^_^",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
    var infoEle = document.querySelector('#comments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
      infoEle.childNodes.forEach(function(item) {
        item.parentNode.removeChild(item);
      });
    }
  }, window.Valine);
});
</script>

</body>
</html>
