<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sharp-rookie.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

<script> 
   (function(){
          if(''){
              if (prompt('请输入文章密码') !== ''){
                  alert('密码错误！');
                  history.back();
              }
          }
      })();
  </script>
  <meta name="description" content="&amp;emsp;&amp;emsp;强化学习 (Reinforcement Learning) 是一个机器学习大家族中的分支, 由于近些年来的技术突破, 和深度学习 (Deep Learning) 的整合, 使得强化学习有了进一步的运用。比如让计算机学着玩游戏, AlphaGo 挑战世界围棋高手, 都是强化学习在行的事。强化学习让程序从对当前环境完全陌生, 成长为一个在环境中游刃有余的高手。">
<meta property="og:type" content="article">
<meta property="og:title" content="【强化学习】强化学习基础模型及求解">
<meta property="og:url" content="https://sharp-rookie.github.io/2021/12/28/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B1%82%E8%A7%A3/index.html">
<meta property="og:site_name" content="最忆是江南">
<meta property="og:description" content="&amp;emsp;&amp;emsp;强化学习 (Reinforcement Learning) 是一个机器学习大家族中的分支, 由于近些年来的技术突破, 和深度学习 (Deep Learning) 的整合, 使得强化学习有了进一步的运用。比如让计算机学着玩游戏, AlphaGo 挑战世界围棋高手, 都是强化学习在行的事。强化学习让程序从对当前环境完全陌生, 成长为一个在环境中游刃有余的高手。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220126150606.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220126150457.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211229085342.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211227234235.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220126113400.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220126150458.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085646.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085646.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085858.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085745.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085806.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228092413.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085947.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090008.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090154.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090245.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090541.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090616.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228091126.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228140030.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228140047.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228092413.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228135735.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220126150459.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228095500.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220126143244.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228141900.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228135332.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085858.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228102751.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228110419.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228110505.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228110514.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228105615.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228112323.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211229100620.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228112010.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228142817.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228111541.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228111604.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228135203.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20211228134525.png">
<meta property="article:published_time" content="2021-12-28T06:03:10.934Z">
<meta property="article:modified_time" content="2022-01-30T14:48:21.487Z">
<meta property="article:author" content="Lrk612">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Reinforcement Learning">
<meta property="article:tag" content="Algorithm">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220126150606.png">

<link rel="canonical" href="https://sharp-rookie.github.io/2021/12/28/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B1%82%E8%A7%A3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>【强化学习】强化学习基础模型及求解 | 最忆是江南</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
        <a target="_blank" rel="noopener" href="https://github.com/Sharp-rookie" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">最忆是江南</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Love actually is all around</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th-list fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-friendlink">

    <a href="/friendlink/" rel="section"><i class="fa fa-link fa-fw"></i>友链</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-download fa-fw"></i>资源</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sharp-rookie.github.io/2021/12/28/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B1%82%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Lrk612">
      <meta itemprop="description" content="Lrk's blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="最忆是江南">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【强化学习】强化学习基础模型及求解
        </h1>

        <div class="post-meta">
         
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-28 14:03:10" itemprop="dateCreated datePublished" datetime="2021-12-28T14:03:10+08:00">2021-12-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/" itemprop="url" rel="index"><span itemprop="name">技术分享</span></a>
                </span>
            </span>

          
            <span id="/2021/12/28/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B1%82%E8%A7%A3/" class="post-meta-item leancloud_visitors" data-flag-title="【强化学习】强化学习基础模型及求解" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/12/28/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B1%82%E8%A7%A3/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/12/28/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B1%82%E8%A7%A3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>&emsp;&emsp;强化学习 (Reinforcement Learning) 是一个机器学习大家族中的分支, 由于近些年来的技术突破, 和深度学习 (Deep Learning) 的整合, 使得强化学习有了进一步的运用。比如让计算机学着玩游戏, AlphaGo 挑战世界围棋高手, 都是强化学习在行的事。强化学习让程序从对当前环境完全陌生, 成长为一个在环境中游刃有余的高手。</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220126150606.png" alt="image-20220126150606364" style="zoom: 67%;" />

<span id="more"></span>

<p>&nbsp;</p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="在机器学习中的位置"><a href="#在机器学习中的位置" class="headerlink" title="在机器学习中的位置"></a>在机器学习中的位置</h3><p>&emsp;&emsp;强化学习是和监督学习，非监督学习并列的第三种机器学习方法，是机器学习中的一个独立领域。与后两者相同，强化学习有自己一套比较完整的数据处理、建模、训练、调优的套路，成为了一种方法论或模型体系。</p>
<h4 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h4><p>&emsp;&emsp;监督学习是比较传统的一个机器学习领域，简而言之是给定（输入，输出）对，反向推导从输入到输出这个映射中的参数值或组合，即给定映射形式：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220126150457.png" alt="image-20220126110940334" style="zoom: 67%;" />

<p>然后根据已有数据集(x, y)键值对推导参数 <code>θ</code> ，这个过程称为训练。</p>
<h4 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h4><p>&emsp;&emsp;非监督学习也是比较传统的一个机器学习领域，比较常见的是聚类模型，例如：K-Means算法，只给定输入 x，输出并没有给定目标值。</p>
<h4 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h4><p>&emsp;&emsp;强化学习和监督学习最大的区别是它没有监督学习已经准备好的训练数据输出值。强化学习只有奖励值，但是这个奖励值和监督学习的输出值不一样，它不是事先给出的，而是延后给出的。同时，强化学习的每一步与时间顺序前后关系紧密。而监督学习的训练数据之间一般都是独立的，没有这种前后的依赖关系。</p>
<p>&emsp;&emsp;强化学习和非监督学习的区别。也还是在奖励值这个地方。非监督学习是没有输出值也没有奖励值的，它只有数据特征。同时和监督学习一样，数据之间也都是独立的，没有强化学习这样的前后依赖关系。</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211229085342.png" alt="image-20211229085342314" style="zoom:50%;" />

<p>&nbsp;</p>
<h3 id="强化学习建模"><a href="#强化学习建模" class="headerlink" title="强化学习建模"></a>强化学习建模</h3><h4 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h4><table>
<thead>
<tr>
<th>组件</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Agent / Brain</td>
<td>强化学习中做决策的智能体(机器人)</td>
</tr>
<tr>
<td>Environment</td>
<td>被学习的环境或对象，一般对Agent是未知的“黑盒子”</td>
</tr>
<tr>
<td>State 、 Observation</td>
<td>Agent对Environment的感知，是做出决策的信息来源</td>
</tr>
<tr>
<td>Action</td>
<td>Agent基于感知信息和决策算法，对决定在当下对Environment执行的动作</td>
</tr>
<tr>
<td>Reward</td>
<td>Environment对Agent执行Action的反馈</td>
</tr>
</tbody></table>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211227234235.png" alt="image-20211227234234718" style="zoom: 33%;" />

<p>&emsp;&emsp;由上，强化学习模型其实就是智能体<code>Agent</code>观测环境<code>Environment</code>后得到环境当前所处的状态信息<code>State</code>，然后基于自身决策算法决定对环境执行一个动作<code>Action</code>，这个动作使环境的状态发生了变化，并由此反馈给智能体一个信息<code>Reward</code>，以告知其动作的好坏，智能体基于反馈优化自己做决策的算法。不断重复这个过程，从而让智能体形成能够带来最大价值的决策算法。</p>
<h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p>&emsp;&emsp;注意这里说的模型与强化学习本身数学模型并不是同一个概念，这里的模型是指环境Environment，通常用两个式子就可以描述一个模型：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220126113400.png" alt="image-20220126113400549" style="zoom:67%;" />

<p>&emsp;&emsp;第一个公式表示<code>t</code>时刻状态为<code>s</code>，执行动作<code>a</code>后转换为状态<code>s’</code>的概率；第二个公式表示<code>t</code>时刻状态为<code>s</code>，执行动作<code>a</code>的奖励的期望值（因为可能转换到不同的状态）。如果知道这两个环境的这两个公式，那么这个环境对于智能体来说就不是“黑盒子”，因为它的一切输入、输出及奖励都可以直接数学推导得出，而不再需要通过试错的方式来观察。因此，最佳策略算法的问题转化为最优化问题，不止或者不再需要用强化学习来求解，使用其他机器学习算法都可以解决。</p>
<h5 id="Model-Based"><a href="#Model-Based" class="headerlink" title="Model-Based"></a>Model-Based</h5><p>&emsp;&emsp;顾名思义，Model-Based的强化学习算法就是必须已知环境的上述两个信息才能求解。但这时完全可以用其他更优的算法代替强化学习了。</p>
<h5 id="Model-Free"><a href="#Model-Free" class="headerlink" title="Model-Free"></a>Model-Free</h5><p>&emsp;&emsp;Model-Free的强化学习算法不需要已知环境模型就可以求解，而这也是实际工程中最常用的，是强化学习的价值所在。</p>
<p>&nbsp;</p>
<h3 id="模型求解"><a href="#模型求解" class="headerlink" title="模型求解"></a>模型求解</h3><p>&emsp;&emsp;强化学习模型求解过程本质就是智能体在不断与环境交互，产生序列：<code>S0,A0,R1,S1,A1,R2,...</code>，并在这个过程中推导出最优的决策算法，从而实现价值最大化。</p>
<h4 id="直接法"><a href="#直接法" class="headerlink" title="直接法"></a>直接法</h4><p>&emsp;&emsp;直接法的前提是策略形式为： <code>a=π(s|θ)</code> ，即由参数 <code>θ</code> 描述策略，优化策略就是优化参数 <code>θ</code>。那么既然是要优化，就一定有优化的目标或评价函数，在这里是所有状态State下动作Action的奖励期望值：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220126150458.png" alt="image-20220126115026053" style="zoom:70%;" />

<p>&emsp;&emsp;但实际上，大多策略都不是仅靠参数<code>θ</code>就能描述的，因此强化学习策略优化用的主要是下面的间接法。</p>
<h4 id="间接法"><a href="#间接法" class="headerlink" title="间接法"></a>间接法</h4><p>&emsp;&emsp;间接法的思想就是，计算出各个状态在策略<code>π</code>下的价值，然后决策时选择能够达到最大价值的下一状态的动作，这就是最优策略。其中，状态价值公式如下：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085646.png" alt="image-20211228085646607" style="zoom: 50%;" />

<center>γ是折扣因子，反映价值的远见性</center>

<p>&emsp;&emsp;可见，优化策略的问题转化为了如何根据公式计算每个状态价值的问题。事实上，后面的三种经典求解强化学习的算法思想也都是在间接法求最优策略，使用不同的方法计算状态价值。</p>
<h4 id="On-Line-和-Off-Line-学习"><a href="#On-Line-和-Off-Line-学习" class="headerlink" title="On-Line 和 Off-Line 学习"></a>On-Line 和 Off-Line 学习</h4><p>&emsp;&emsp;强化学习的有两种学习方式，在线学习是指Agent一边与环境交互收集样本，一遍更新策略；离线学习是指Agent事先像监督学习一样得到完整的训练，然后用最终策略与环境交互。虽然离线学习在现成数据集下可以快速训练得到较好的策略，不需要像在线学习那样一个动作得到一次反馈，然后花时间更新策略，训练地较慢。但是在现实场景中，并没有机会存储足够多的样本供离线学习训练，绝大多数基于时间差分法的强化学习训练都倾向于在线学习。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h2 id="动态规划DP（Model-Based）"><a href="#动态规划DP（Model-Based）" class="headerlink" title="动态规划DP（Model-Based）"></a>动态规划DP（Model-Based）</h2><p>&emsp;&emsp;动态规划是一门相对独立的学科，是运筹学的一个分支。运筹学本身的学科体系就比较复杂且不断壮大，包括：线性规划、非线性规划、组合规划、图论、决策分析、排队论、博弈论、搜索论等，动态规划关注如何用数学方法求解一个决策过程的最优化问题。将问题的最优解分解为若干小问题的最优解，即通过寻找子问题的最优解之间的递推关系来得到问题的最优解。而动态规划求解强化学习的算法是建立在马尔可夫决策过程基础上的。</p>
<p>&nbsp;</p>
<h3 id="马尔可夫决策过程MDP"><a href="#马尔可夫决策过程MDP" class="headerlink" title="马尔可夫决策过程MDP"></a>马尔可夫决策过程MDP</h3><p>&emsp;&emsp;在强化学习中，agent与environment一直在互动。在每个时刻t，agent会接收到来自环境的状态s，基于这个状态s，agent会做出动作a，然后这个动作作用在环境上，于是agent可以接收到一个奖赏Rt+1，并且agent就会到达新的状态。所以，其实agent与environment之间的交互就是产生了一个序列：<code>S0,A0,R1,S1,A1,R2,...</code></p>
<p>&emsp;&emsp;称这个为序列决策过程。而马尔科夫决策过程（Morkov Decision Process）就是一个典型的序列决策过程的一种公式化。有了马尔科夫的假设，才能推导出状态价值的贝尔曼方程式，从而用递推的方式计算状态价值。</p>
<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>&emsp;&emsp;下一个状态的产生只和当前的状态有关，即：<code>P(St+1 | St) = P(St+1 | S1,...,St)</code></p>
<p>&emsp;&emsp;直观上讲，下一个状态的产生跟所有历史状态是有关的，也就是等式右边所示。但是Markov的定义则是忽略掉历史信息，只保留了当前状态的信息来预测下一个状态，这称为无后效性/马尔可夫性。一个 MDP 可以由一个 5 元组表示为 <code>M = &lt;S,A,P(s&#39;|s,a),R,γ&gt;</code>：</p>
<blockquote>
<p><strong>S</strong> → 一个状态（state）的集合<br><strong>P</strong> → 一个受行为影响的状态转移概率矩阵<br><strong>A</strong> →一个有限动作集<br><strong>R</strong> →  一个用于计算回报的函数<br><strong>γ</strong> → 一个折扣因子，用于做未来回报计算时的衰减系数，γ ∈ ( 0 , 1 )</p>
</blockquote>
<p>&nbsp;</p>
<h4 id="重要概念"><a href="#重要概念" class="headerlink" title="重要概念"></a>重要概念</h4><p><strong>状态转移概率Pss′a</strong></p>
<p>对于一个具体的状态s和它的下一个状态s’，它们的状态转移概率(就是从s转移到s’的概率)定义为：</p>
<p>&emsp;&emsp;<code>Pss′a = P(St+1=s′|St=s,At=a)</code></p>
<p>也就是说，下一个状态的产生只受到当前状态和动作的影响。</p>
<p><strong>策略π</strong></p>
<p>&emsp;&emsp;<code>Policy： π(a|s) = P(At=a|St=s)</code></p>
<p>policy π表示的是在给定的state下，一个关于action的概率分布。即表示在一个状态s下，agent接下来可能会采取的任意一个action的概率分布（可能一开始不知道概率是多少）。对于每一个状态s都会有这样一个π(a|s)，所有状态的π(a|s）就形成整体策略π。策略π是指所有状态都要使用这个策略，不是单独指某一个状态。</p>
<p>无论怎样，我们的目标是最大化累积奖赏，所以我们可以通过不断地改进我们的策略，使得我们最后能够获得最大累积奖赏。</p>
<p><strong>状态价值函数Vπ</strong></p>
<p>定义式：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085646.png" alt="image-20211228085646607" style="zoom: 50%;" />

<p>递推形式：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085858.png" alt="image-20211228085858501" style="zoom:50%;" />

<p>计算机通常使用递推式迭代地求出MDP中每个状态的价值函数，而不是用定义式硬算。</p>
<p><strong>动作价值函数<em>Qπ</em></strong></p>
<p>定义式：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085745.png" alt="image-20211228085745128" style="zoom:50%;" />

<p>​                                                                              <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085806.png" alt="image-20211228085806768" style="zoom:50%;" /></p>
<p>递推形式：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228092413.png" alt="image-20211228092413197" style="zoom: 25%;" />

<p>计算机通常使用递推式迭代地求出MDP中每个状态的价值函数，而不是用定义式硬算。</p>
<p><strong>状态价值与动作价值的递推关系</strong></p>
<p>状态价值函数是所有动作价值函数基于策略π的期望：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085947.png" style="zoom:50%;" />

<p>利用上递推式，也很容易从状态价值函数vπ(s)表示动作价值函数qπ(s,a)，即：</p>
<p>​                                                                    <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090008.png" style="zoom:50%;" /></p>
<p>二者转化关系如图：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090154.png" alt="image-20211228090154012" style="zoom: 33%;" />

<p>&emsp;&emsp;总而言之，状态 / 动作价值有两部分相加组成，第一部分是即时奖励，第二部分是环境所有可能出现的下一个状态的概率乘以该下一状态的状态价值，最后求和，并加上衰减。两式联立得：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090245.png" alt="image-20211228090244956" style="zoom:50%;" />

<center>这两个表达式又称为贝尔曼方程</center>

<p>&nbsp;</p>
<h4 id="最优价值函数"><a href="#最优价值函数" class="headerlink" title="最优价值函数"></a>最优价值函数</h4><p>&emsp;&emsp;根据间接法的思想，解决强化学习问题意味着要寻找一个最优的策略让个体在与环境交互过程中获得始终比其它策略都要多的收获，这个最优策略可以用 π∗表示。一旦找到这个最优策略π∗，那么就解决了这个强化学习问题。而策略的优劣可由价值函数反映，因此最优策略问题转化为最优价值函数问题。</p>
<ol>
<li><p><strong>最优状态价值函数</strong>是所有策略下产生的众多状态价值函数中的最大者：</p>
 <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090541.png" alt="image-20211228090541867" style="zoom:50%;" /></li>
<li><p><strong>最优动作价值函数</strong>是所有策略下产生的众多动作状态价值函数中的最大者：</p>
 <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228090616.png" alt="image-20211228090616747" style="zoom: 50%;" /></li>
</ol>
<p><strong>转化关系：</strong></p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228091126.png" alt="image-20211228091126423" style="zoom: 50%;" />

<p>&nbsp;</p>
<h3 id="策略迭代-求解强化学习"><a href="#策略迭代-求解强化学习" class="headerlink" title="策略迭代 求解强化学习"></a>策略迭代 求解强化学习</h3><p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228140030.png" alt="image-20211228140030255"></p>
<p>&emsp;&emsp;根据之前基于任意一个给定策略<code>π</code>与环境进行交互，然后用贝尔曼方程计算状态价值，然后根据状态价值调整动作策略<code>π</code>，这个方法叫做策略迭代(Policy Iteration)。最简单的调整方法就是贪婪法：个体在某个状态下选择的行为是其能够到达后续所有可能的状态中状态价值最大的那个状态。</p>
<p><strong>流程</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 使用当前策略π∗与环境交互，用贝尔曼方程计算当前策略π*对应的状态价值v∗</span><br><span class="line">2. 根据刚刚计算的状态价值v∗以一定的方法（比如贪婪法）更新策略π∗</span><br><span class="line">3. 重复第1、2步，一直迭代下去，直到策略π∗和状态价值v∗收敛为止</span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h3 id="价值迭代-求解强化学习"><a href="#价值迭代-求解强化学习" class="headerlink" title="价值迭代 求解强化学习"></a>价值迭代 求解强化学习</h3><p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228140047.png" alt="image-20211228140046899"></p>
<p>&emsp;&emsp;相比策略迭代，价值迭代不需要等到状态价值收敛才调整策略，而是随着状态价值的迭代及时调整策略, 这样可以大大减少迭代次数。此时的状态价值的更新方法也和策略迭代不同，此时贝尔曼方程迭代式如下：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228092413.png" alt="image-20211228092413197" style="zoom: 25%;" />

<p>&emsp;&emsp;可见由于策略调整，现在价值每次更新倾向于贪婪法选择的最优策略对应的后续状态价值，这样收敛更快。</p>
<p><strong>流程</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 使用当前策略π∗与环境交互，用贝尔曼方程计算当前策略π*对应的状态价值v∗</span><br><span class="line">2. 根据刚刚计算的状态价值v∗以上述公式更新策略v∗</span><br><span class="line">3. 重复第1、2步，一直迭代下去，直到状态价值v∗收敛为止</span><br><span class="line">4. 根据收敛的v*，以贪婪思想更新得到最优策略π∗</span><br></pre></td></tr></table></figure>

<p>对比策略迭代可知，价值迭代中，策略π始终不变，更新的只有价值v*，等到v*收敛后才更新策略π，一步到位；</p>
<p>而策略迭代中，更新的主体的策略π，每轮更新后计算新的价值v*，然后继续更新π，直到策略π收敛。</p>
<p>&nbsp;</p>
<h3 id="异步动态规划"><a href="#异步动态规划" class="headerlink" title="异步动态规划"></a>异步动态规划</h3><p>&emsp;&emsp;上面几个都是<strong>同步动态规划算法</strong>，即每轮迭代会计算出所有的状态价值并保存起来，在下一轮中使用这些保存起来的状态价值来计算新一轮的状态价值。另一种动态规划求解是异步动态规划算法，在这些算法里，每一次迭代并不对所有状态的价值进行更新，而是依据一定的原则有选择性的更新部分状态的价值，这类算法有自己的一些独特优势，当然有额会有一些额外的代价。</p>
<p><strong>原位动态规划 (in-place DP)</strong></p>
<p>&emsp;&emsp;此时我们不会另外保存一份上一轮计算出的状态价值。而是即时计算即时更新。这样可以减少保存的状态价值的数量，节约内存。代价是收敛速度可能稍慢。</p>
<p><strong>优先级动态规划 (prioritized sweeping DP)</strong></p>
<p>&emsp;&emsp;该算法对每一个状态进行优先级分级，优先级越高的状态其状态价值优先得到更新。通常使用贝尔曼误差来评估状态的优先级，贝尔曼误差即新状态价值与前次计算得到的状态价值差的绝对值。这样可以加快收敛速度，代价是需要维护一个优先级队列。</p>
<p><strong>实时动态规划 (real-time DP)</strong></p>
<p>&emsp;&emsp;实时动态规划直接使用个体与环境交互产生的实际经历来更新状态价值，对于那些个体实际经历过的状态进行价值更新。这样个体经常访问过的状态将得到较高频次的价值更新，而与个体关系不密切、个体较少访问到的状态其价值得到更新的机会就较少。收敛速度可能稍慢。</p>
<p>&nbsp;</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>&emsp;&emsp;动态规划算法主要就是利用贝尔曼方程来迭代更新状态价值，用贪婪法之类的方法迭代更新最优策略。使用全宽度（full-width）的回溯机制来进行状态价值的更新，也就是说，无论是同步还是异步动态规划，在每一次回溯更新某一个状态的价值时，都要回溯到该状态的所有可能的后续状态，并利用贝尔曼方程更新该状态的价值。这种全宽度的价值更新方式对于状态数较少的强化学习问题还是比较有效的，但是当问题规模很大的时候，动态规划算法将会因贝尔曼维度灾难而无法使用。因此还需要寻找其他的针对复杂问题的强化学习问题求解方法。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h2 id="蒙特卡罗法MC（Model-free）"><a href="#蒙特卡罗法MC（Model-free）" class="headerlink" title="蒙特卡罗法MC（Model-free）"></a>蒙特卡罗法MC（Model-free）</h2><p>&emsp;&emsp;蒙特卡罗法又叫做统计模拟方法，是一种以概率统计理论为指导的数值计算方法。其核心思想是：多次实验逼近客观分布真实值。</p>
<p>&nbsp;</p>
<h3 id="不基于模型的强化学习问题"><a href="#不基于模型的强化学习问题" class="headerlink" title="不基于模型的强化学习问题"></a>不基于模型的强化学习问题</h3><p>&emsp;&emsp;在上面动态规划法中，模型状态转化概率矩阵P始终是已知的，即MDP已知，一般称这样的强化学习问题为Model-based的强化学习问题。Model-based强化学习问题可以通过动态规划来评估一个给定的策略，通过不断迭代最终得到最优价值函数，具体的做法有两个：一个是策略迭代，一个是值迭代。</p>
<p>&emsp;&emsp;然而有很多强化学习问题没有办法事先得到模型状态转化概率Pss’a，即不基于模型的<strong>Model-free</strong>强化学习（面向黑盒子学习），为了能够从环境中学习，需要让agent与environment交互，得到一些经历（样本）。然后通过这些经历来进行策略评估与策略迭代，从而最终得到最优策略。这种做法的理论是从蒙特卡罗方法（Monte-Carlo）中来的。</p>
<p>&nbsp;</p>
<h3 id="蒙特卡罗思想"><a href="#蒙特卡罗思想" class="headerlink" title="蒙特卡罗思想"></a>蒙特卡罗思想</h3><p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228135735.png" alt="image-20211228135716095"></p>
<p>&emsp;&emsp;通过采样若干个<code>经历完整的状态序列(episode)</code>后求均值来估计状态的真实价值。所谓的经历完整，就是这个序列<strong>必须是达到终点</strong>的，这样才能拿到最终回报。比如下棋问题要分出输赢，驾车问题要成功到达终点或者失败。有了很多组这样经历完整的状态序列，不需要借助模型本身就可以近似的估计状态价值，进而求解预测和控制问题。</p>
<p>&emsp;&emsp;采样得到的完整序列示例：<code>S1,A1,R2,S1,A2,R3,...St,At,Rt+1,...,RT,ST</code></p>
<p>&emsp;&emsp;借助大数定律思想以统计的方式算出状态价值后，策略的优化基本和动态规划一样，用<strong>价值迭代</strong>。</p>
<p><strong>对比动态规划</strong></p>
<ul>
<li>关键区别是状态价值的计算方法不同</li>
<li>不依赖于模型状态转化概率</li>
<li>从经历过的完整序列学习，完整的经历越多，学习效果越好</li>
</ul>
<p><strong>首次访问与每次访问</strong></p>
<p>&emsp;&emsp;同样一个状态可能在一个完整的状态序列（episode）中重复出现，第一种应对方法是仅把状态序列中第一次出现该状态时的收获值纳入到收获平均值的计算中；另一种是针对一个状态序列中每次出现的该状态，都计算对应的收获值并纳入到收获平均值的计算中。两种方法对应的蒙特卡罗法分别称为：<strong>首次访问(first visit)</strong> 和<strong>每次访问(every visit)</strong> 蒙特卡罗法。第二种方法比第一种的计算量要大一些，但是在完整的经历样本序列少的场景下会比第一种方法适用。</p>
<p><strong>累进更新平均值（incremental mean)</strong></p>
<p>&emsp;&emsp;按照蒙特卡洛求平均值来代表状态价值的公式里，每个状态都需要维护一个计数器和累加器，分别保存其出现次数和价值之和用于最后求平均，这样浪费了太多的存储空间：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220126150459.png" alt="image-20220126143115081" style="zoom: 67%;" />

<p>注意，这里的 <code>Gt</code> 不能用贝尔曼方程计算，只能由采样的奖励值序列按定义式计算：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228095500.png" alt="image-20211228095500637" style="zoom:50%;" />

<p>&emsp;&emsp;数学上的优化方法是在迭代计算收获均值，即每次保存上一轮迭代得到的收获均值与次数，当计算得到当前轮的收获时，即可计算当前轮收获均值和次数：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220126143244.png" alt="image-20220126143244239" style="zoom:67%;" />

<p>其中，<code>k</code>采样序列episode的个数，可以证明这样计更新价值V和上面的结果是一致的，但大大节省空间。</p>
<p>&nbsp;</p>
<h3 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h3><p>&emsp;&emsp;蒙特卡罗法求解强化学习问题的思路和动态规划价值迭代的的思路类似。区别在于价值vk(s)的计算方法（蒙特卡洛使用采样然后取平均来算），然后基于据一定的方法（比如贪婪法）更新当前策略π。最后得到最优价值函数v∗和最优策略π∗。</p>
<p><strong>对比动态规划：</strong></p>
<p>​        1.  状态价值的计算方法不同，动态规划用贝尔曼方程迭代，蒙特卡罗法用采样数据求平均</p>
<p>​        2. 蒙特卡罗法一般是优化最优动作价值函数q∗，而不是状态价值函数v∗</p>
<p>​        3. 动态规划一般基于贪婪法更新策略，而蒙特卡罗法一般采用<strong>ϵ−greedy</strong>法更新：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228141900.png" alt="image-20211228141900761" style="zoom: 18%;" />

<center>m是可选行动的个数</center>

<p>在实际求解时，为了使算法可以收敛，一般ϵ会随着算法的迭代过程逐渐减小并趋于0。这样在迭代前期，鼓励探索；而后期由于已经有了足够的探索量，开始趋于保守，以贪婪为主，使算法可以稳定收敛。</p>
<p><strong>算法：（以every-visit为例）</strong></p>
<p>输入：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">状态集S、动作集A、即时奖励R、衰减因子γ、探索率ϵ</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最有动作价值函数q∗、最优策略π∗</span><br></pre></td></tr></table></figure>

<p>流程：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1. 初始化所有的动作价值Q(s, a)=0，状态次数N(s, a)=0，采样次数k=0，随机初始化一个策略π</span><br><span class="line">2. k=k+1，基于策略π进行第k次蒙特卡罗采样，得到一个完整的状态序列:</span><br><span class="line">			S1,A1,R2,S1,A2,R3,...St,At,Rt+1,...,RT,ST</span><br><span class="line">3. 对于该状态序列里出现的每一状态行为对(St,At)，计算其收获Gt, 更新其计数N(s,a)和行为价值函数Q(s,a)：</span><br><span class="line">			    Gt = 对n累加到T[(γ^(t-n))*(Rt+n)]</span><br><span class="line">			    N(s, a) ++</span><br><span class="line">			    Q(s, a) += (Gt-Q(s, a))/N(s,a)</span><br><span class="line">4. 基于新计算出的动作价值，更新当前的ϵ−greedy策略：</span><br><span class="line">			    ϵ = 1/k</span><br><span class="line">			    π = ϵ−greedy()</span><br><span class="line">5. 若所有的Q(s,a)收敛，则对应的所有Q(s,a)即为最优的动作价值函数q∗，对应的策略π(a|s)即为最优策略π∗，否则转到第二步</span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>&emsp;&emsp;蒙特卡罗法是不基于模型的强化问题求解方法，它可以避免动态规划求解过于复杂，同时不需要事先知道环境转化模型，因此常用于海量数据和复杂模型。但是它每次采样都需要一个完整的状态序列，如果没有完整的状态序列，或者很难拿到较多的完整的状态序列，因此在实际问题中蒙特卡罗法用的不多，所以还需要寻找其他的更灵活的不基于模型的强化问题求解方法。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h2 id="时序差分法TD（Model-free）"><a href="#时序差分法TD（Model-free）" class="headerlink" title="时序差分法TD（Model-free）"></a>时序差分法TD（Model-free）</h2><h3 id="时序差分法"><a href="#时序差分法" class="headerlink" title="时序差分法"></a>时序差分法</h3><p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228135332.png" alt="image-20211228135332814"></p>
<p>&emsp;&emsp;时序差分法和蒙特卡罗法类似，都是不基于模型的强化学习问题求解方法。蒙特卡罗法中<code>Gt</code>的计算需要完整序列，而时序差分法没有完整的状态序列，只有部分的状态序列，由贝尔曼方程：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228085858.png" alt="image-20211228085858501" style="zoom:50%;" />

<p>可知，可以用<code>Rt+1+γv(St+1)</code>来近似的代替收获Gt。</p>
<p>一般我们把<code>Rt+1+γV(St+1)</code>称为<strong>目标值</strong>，<code>Rt+1+γV(St+1)−V(St)</code>称为<strong>TD误差</strong>，将用目标值近似代替收获G(t)的过程称为<strong>引导</strong>(bootstrapping)。这样只需要两个连续的状态与对应的奖励，就有了近似收获Gt的表达式，可以去求解时序差分的预测问题和控制问题了。</p>
<p>&nbsp;</p>
<p><strong>对比蒙特卡洛法：</strong></p>
<p>时序差分的预测问题求解和蒙特卡罗法类似，但是主要有两个不同点：</p>
<ul>
<li><p>收获Gt的表达式不同</p>
<ul>
<li>蒙特卡罗 G(t) 需要遍历完整过程才能得出：<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228102751.png" alt="image-20211228102751434" style="zoom: 50%;" /></li>
<li>时序差分 G(t)记录连续两个状态就可得出，称为 <strong>TD Target</strong> ：<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228110419.png" alt="image-20211228110419330" style="zoom: 50%;" /></li>
</ul>
</li>
<li><p>迭代式子的系数不同，时序差分没有完整的序列，也就没有对应的次数N(St, At)，一般用一个[0,1]的系数α代替：</p>
<ul>
<li><p>蒙特卡罗:    </p>
  <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228110505.png" alt="image-20211228110505164" style="zoom:50%;" /></li>
<li><p>时序差分: </p>
  <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228110514.png" alt="image-20211228110514567" style="zoom:50%;" /></li>
</ul>
</li>
</ul>
<p>&emsp;&emsp;（并且蒙特卡罗法一般只优化动作价值函数Q，时序差分状态价值、动作价值函数都会优化）</p>
<ul>
<li>时序差分法在知道最终结果之前就可以学习，甚至没有最终结果也可以学习，还可以在持续进行的环境中学习。而蒙特卡罗法则要等到最后结果才能学习，因此时序差分法可以更快速灵活的更新状态的价值估计，这在某些情况下有着非常重要的实际意义。</li>
<li>时序差分法在更新状态价值时使用的是<strong>TD目标值</strong>，即基于即时奖励和下一状态的预估价值来替代当前状态在状态序列结束时可能得到的收获，是当前状态价值的<strong>有偏估计</strong>，而蒙特卡罗法则使用实际的收获来更新状态价值，是某一策略下状态价值的<strong>无偏估计</strong>，这一点蒙特卡罗法占优。</li>
<li>虽然时序差分法得到的价值是有偏估计，但是其方差却比蒙特卡罗法得到的方差要低，且对初始值敏感，通常比蒙特卡罗法更加高效。</li>
</ul>
<p>&nbsp;</p>
<h3 id="n步时序差分："><a href="#n步时序差分：" class="headerlink" title="n步时序差分："></a>n步时序差分：</h3><p>&emsp;&emsp;上面用TD Target代替收获Gt的公式中，St只向前一步到St+1，若用向前2步、n步来近似代替Gt，则为n步时序差分：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228105615.png" alt="image-20211228105615429" style="zoom: 33%;" />

<p>&emsp;&emsp;（当n越来越大，趋于无穷，或者说趋于使用完整的状态序列时，n步时序差分就等价于蒙特卡罗法了）</p>
<p>&nbsp;</p>
<h3 id="TD-λ"><a href="#TD-λ" class="headerlink" title="TD(λ)"></a>TD(λ)</h3><p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228112323.png" alt="image-20211228112323589"></p>
<p>&emsp;&emsp;n步时序差分选择多少步数是一个超参数调优问题(一般3~10比较好)。为了能在不增加计算复杂度的情况下综合考虑所有步数的预测，引入一个[0,1]新的参数λ，定义收获是n从1到∞所有步的收获乘以权重的和：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211229100620.png" alt="image-20211229100620726" style="zoom:50%;" />

<p>每一步的权重是<code>(1−λ)*λ^(n−1)</code></p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228112010.png" alt="image-20211228112009881" style="zoom: 33%;" />

<p>&emsp;&emsp;进而有价值函数：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228142817.png" alt="image-20211228142817279" style="zoom:25%;" />

<p>&emsp;&emsp;从前向来看，一个状态的价值V(St)由Gt得到，而Gt又间接由所有后续状态价值计算得到，因此可以认为更新一个状态的价值需要知道所有后续状态的价值。也就是说，必须要经历完整的状态序列获得包括终止状态的每一个状态的即时奖励才能更新当前状态的价值。这和蒙特卡罗法的要求一样，因此TD(λ)有着和蒙特卡罗法一样的劣势。当λ=0 时，就是上面的普通时序差分法；当λ=1时，就是蒙特卡罗法。</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228111541.png" alt="image-20211228111540959" style="zoom:25%;" />

<p>&emsp;&emsp;从反向来看，它可以分析我们状态对后续状态的影响。比如老鼠在依次连续接受了3 次响铃和1 次亮灯信号后遭到了电击，那么在分析遭电击的原因时，到底是响铃的因素较重要还是亮灯的因素更重要呢？如果把老鼠遭到电击的原因认为是之前接受了较多次数的响铃，则称这种归因为<strong>频率启发(frequency heuristic) 式</strong>；而把电击归因于最近少数几次状态的影响，则称为<strong>就近启发(recency heuristic) 式</strong>。</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228111604.png" alt="image-20211228111604749" style="zoom:25%;" />



<p><strong>效用迹E</strong></p>
<p>&emsp;&emsp;给每一个状态引入一个数值：<strong>效用值</strong> 来表示该状态对后续状态的影响，就可以同时利用到上述两个启发(同事反映二者)。而所有状态的效用值总称为**效用迹(eligibility traces, ES)**。定义为：</p>
<p>​                        <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228135203.png" alt="image-20211228135203250" style="zoom:25%;" />    </p>
<p>&emsp;&emsp;此时价值函数为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211228134525.png" alt="image-20211228134525809" style="zoom: 25%;" />

<p>&nbsp;</p>
<h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><p>&emsp;&emsp;时序差分和蒙特卡罗法比它更加灵活，学习能力更强，因此是目前主流的强化学习求解问题的方法，现在绝大部分强化学习乃至深度强化学习的求解都是以时序差分的思想为基础的。</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Lrk612
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://sharp-rookie.github.io/2021/12/28/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B1%82%E8%A7%A3/" title="【强化学习】强化学习基础模型及求解">https://sharp-rookie.github.io/2021/12/28/【强化学习】强化学习基础模型及求解/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
              <a href="/tags/Algorithm/" rel="tag"># Algorithm</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/12/27/%E3%80%90%E8%AE%BA%E6%96%87%E3%80%91Network%20Traffic%20%20Characterization%20%20Using%20Token%20%20Bucket%20%20Model/" rel="prev" title="Token Bucket最优参数的数学推导">
      <i class="fa fa-chevron-left"></i> Token Bucket最优参数的数学推导
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/12/29/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%EF%BC%9ASarsa&Q-Lreaning/" rel="next" title="【强化学习】时序差分模型：SARSA & Q-Learning">
      【强化学习】时序差分模型：SARSA & Q-Learning <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-text">基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE"><span class="nav-text">在机器学习中的位置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-text">监督学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-text">非监督学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-text">强化学习</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%BB%BA%E6%A8%A1"><span class="nav-text">强化学习建模</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BB%BA%E6%A8%A1"><span class="nav-text">建模</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-text">模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Model-Based"><span class="nav-text">Model-Based</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Model-Free"><span class="nav-text">Model-Free</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%B1%82%E8%A7%A3"><span class="nav-text">模型求解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B4%E6%8E%A5%E6%B3%95"><span class="nav-text">直接法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%B4%E6%8E%A5%E6%B3%95"><span class="nav-text">间接法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#On-Line-%E5%92%8C-Off-Line-%E5%AD%A6%E4%B9%A0"><span class="nav-text">On-Line 和 Off-Line 学习</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92DP%EF%BC%88Model-Based%EF%BC%89"><span class="nav-text">动态规划DP（Model-Based）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8BMDP"><span class="nav-text">马尔可夫决策过程MDP</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="nav-text">定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5"><span class="nav-text">重要概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-text">最优价值函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3-%E6%B1%82%E8%A7%A3%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-text">策略迭代 求解强化学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3-%E6%B1%82%E8%A7%A3%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-text">价值迭代 求解强化学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%82%E6%AD%A5%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92"><span class="nav-text">异步动态规划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%B3%95MC%EF%BC%88Model-free%EF%BC%89"><span class="nav-text">蒙特卡罗法MC（Model-free）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98"><span class="nav-text">不基于模型的强化学习问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%80%9D%E6%83%B3"><span class="nav-text">蒙特卡罗思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="nav-text">价值迭代</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93-1"><span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%B3%95TD%EF%BC%88Model-free%EF%BC%89"><span class="nav-text">时序差分法TD（Model-free）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%B3%95"><span class="nav-text">时序差分法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#n%E6%AD%A5%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%EF%BC%9A"><span class="nav-text">n步时序差分：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TD-%CE%BB"><span class="nav-text">TD(λ)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93-2"><span class="nav-text">小结</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Lrk612"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Lrk612</p>
  <div class="site-description" itemprop="description">Lrk's blog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.csdn.net/" title="https:&#x2F;&#x2F;www.csdn.net&#x2F;" rel="noopener" target="_blank">CSDN</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.google.com/" title="https:&#x2F;&#x2F;www.google.com" rel="noopener" target="_blank">Google</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <!-- 访问量 -->

    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


<!-- 版权 -->
<div class="copyright">
  
  &copy; 2021-12-1 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lrk612</span>
</div>

<!-- ICP备案 -->
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP备案 </a>
      <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211209170739.png" style="display: inline-block;"><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=2021035798" rel="noopener" target="_blank">豫ICP备2021035798号 </a>
  </div>

<!-- 不知道是什么 -->
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'fH6OdGBcCG8D6jC8NeeJlX9b-gzGzoHsz',
      appKey     : '1UoIb6vszMw6wl0n7kreb4Xz',
      placeholder: "Just go go ^_^",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
    var infoEle = document.querySelector('#comments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
      infoEle.childNodes.forEach(function(item) {
        item.parentNode.removeChild(item);
      });
    }
  }, window.Valine);
});
</script>

</body>
</html>
