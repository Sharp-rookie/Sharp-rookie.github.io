<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sharp-rookie.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

<script> 
   (function(){
          if(''){
              if (prompt('请输入文章密码') !== ''){
                  alert('密码错误！');
                  history.back();
              }
          }
      })();
  </script>
  <meta name="description" content="&amp;emsp;&amp;emsp;Q-Learning、Sarsa、DQN等强化学习算法都是基于价值函数的学习方式，但在强化学习的领域中还有另一种优化流派：基于策略学习（策略梯度） &amp;emsp;&amp;emsp;基于策略学习的问题既可以使用蒙特卡洛法求解，也可以使用其他方法。使用蒙特卡洛法求解基于策略的强化学习方法需要完整的序列，并且只单独对策略函数进行更新，不容易收敛，因此可以把基于策略和基于价值的思路相结合，">
<meta property="og:type" content="article">
<meta property="og:title" content="【强化学习】策略梯度：PG算法族 &amp; A3C">
<meta property="og:url" content="https://sharp-rookie.github.io/2022/01/16/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%EF%BC%9APG%E7%AE%97%E6%B3%95%E6%97%8F%20&%20A3C/index.html">
<meta property="og:site_name" content="最忆是江南">
<meta property="og:description" content="&amp;emsp;&amp;emsp;Q-Learning、Sarsa、DQN等强化学习算法都是基于价值函数的学习方式，但在强化学习的领域中还有另一种优化流派：基于策略学习（策略梯度） &amp;emsp;&amp;emsp;基于策略学习的问题既可以使用蒙特卡洛法求解，也可以使用其他方法。使用蒙特卡洛法求解基于策略的强化学习方法需要完整的序列，并且只单独对策略函数进行更新，不容易收敛，因此可以把基于策略和基于价值的思路相结合，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112085053.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220111162523.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220111162846.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220111163508.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220117182507.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220111164026.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220111164440.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220111164711.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220111165453.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220111165244.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220111165543.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112085002.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112084515.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220118104115.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112090509.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112090441.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112090613.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112090936.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112091001.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112091050.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112091237.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112091536.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112091703.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112092148.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112092450.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112091050.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112103230.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220118105019.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220116163324.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112094415.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112095035.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112095453.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220113125452.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112101419.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112102319.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112102707.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220118130134.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220116163325.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220113101331.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220113101501.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220113122829.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220113123143.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220113123425.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220113123623.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220113125412.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220113125013.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220113124619.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220113123425.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220113123623.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220113122829.png">
<meta property="article:published_time" content="2022-01-16T08:37:43.185Z">
<meta property="article:modified_time" content="2022-01-25T04:09:56.512Z">
<meta property="article:author" content="Lrk612">
<meta property="article:tag" content="Reinforcement Learning">
<meta property="article:tag" content="Algorithm">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220112085053.png">

<link rel="canonical" href="https://sharp-rookie.github.io/2022/01/16/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%EF%BC%9APG%E7%AE%97%E6%B3%95%E6%97%8F%20&%20A3C/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>【强化学习】策略梯度：PG算法族 & A3C | 最忆是江南</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
        <a target="_blank" rel="noopener" href="https://github.com/Sharp-rookie" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">最忆是江南</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Love actually is all around</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th-list fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-friendlink">

    <a href="/friendlink/" rel="section"><i class="fa fa-link fa-fw"></i>友链</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-download fa-fw"></i>资源</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sharp-rookie.github.io/2022/01/16/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%EF%BC%9APG%E7%AE%97%E6%B3%95%E6%97%8F%20&%20A3C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Lrk612">
      <meta itemprop="description" content="Lrk's blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="最忆是江南">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【强化学习】策略梯度：PG算法族 & A3C
        </h1>

        <div class="post-meta">
         
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-16 16:37:43" itemprop="dateCreated datePublished" datetime="2022-01-16T16:37:43+08:00">2022-01-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/" itemprop="url" rel="index"><span itemprop="name">技术分享</span></a>
                </span>
            </span>

          
            <span id="/2022/01/16/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%EF%BC%9APG%E7%AE%97%E6%B3%95%E6%97%8F%20&%20A3C/" class="post-meta-item leancloud_visitors" data-flag-title="【强化学习】策略梯度：PG算法族 & A3C" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/01/16/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%EF%BC%9APG%E7%AE%97%E6%B3%95%E6%97%8F%20&%20A3C/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/01/16/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%EF%BC%9APG%E7%AE%97%E6%B3%95%E6%97%8F%20&%20A3C/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>&emsp;&emsp;Q-Learning、Sarsa、DQN等强化学习算法都是基于价值函数的学习方式，但在强化学习的领域中还有另一种优化流派：基于策略学习（策略梯度）</p>
<p>&emsp;&emsp;基于策略学习的问题既可以使用蒙特卡洛法求解，也可以使用其他方法。使用蒙特卡洛法求解基于策略的强化学习方法需要完整的序列，并且只单独对策略函数进行更新，不容易收敛，因此可以把基于策略和基于价值的思路相结合，由此得到：Actor-Critic算法。</p>
<p>&emsp;&emsp;而为了更快地收敛，A3C算法使用了异步并发的强化学习框架，DDPG算法则引用了Nature DQN，DDQN的思想。</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112085053.png" alt="image-20220112085053424" style="zoom: 33%;" />

<span id="more"></span>

<p>&nbsp;</p>
<h2 id="策略梯度（Policy-Gradient）"><a href="#策略梯度（Policy-Gradient）" class="headerlink" title="策略梯度（Policy-Gradient）"></a>策略梯度（Policy-Gradient）</h2><h3 id="Value-based-强化学习方法的不足"><a href="#Value-based-强化学习方法的不足" class="headerlink" title="Value-based 强化学习方法的不足"></a>Value-based 强化学习方法的不足</h3><ol>
<li><p>对连续动作的处理能力不足</p>
<p>DQN等算法只处理离散动作，无法处理连续动作，例如：力的方向和大小</p>
</li>
<li><p>对受限状态下的问题处理能力不足</p>
<p>使用特征来描述状态空间中某一个状态时，有可能因为个体观测的限制或者建模的限制导致真实环境下不同的状态具有相同的特征描述</p>
</li>
<li><p>无法解决随机策略问题</p>
<p>Value Based强化学习方法对应的最优策略通常是确定性策略，因为其是从众多行为价值中选择一个最大价值的行为，而有些问题的最优策略却是随机策略，这种情况下同样是无法通过基于价值的学习来求解的</p>
</li>
</ol>
<h3 id="Policy-based-强化学习方法的引入"><a href="#Policy-based-强化学习方法的引入" class="headerlink" title="Policy-based 强化学习方法的引入"></a>Policy-based 强化学习方法的引入</h3><p>&emsp;&emsp;在Value-based强化学习方法里，对价值函数进行了近似表示，引入了一个动作价值函数q，这个函数由参数w描述（例如DQN的Q网络），并接受状态S与动作A作为输入，计算后得到近似的动作价值，即：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220111162523.png" alt="image-20220111162523170" style="zoom: 45%;" />

<p>&emsp;&emsp;在Policy-based强化学习方法下，采样类似的思路，只不过这时对策略进行近似表示。此时策略π可以被被描述为一个包含参数θ的函数，即：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220111162846.png" alt="image-20220111162846573" style="zoom: 45%;" />

<p>&emsp;&emsp;将策略表示成一个连续的函数后就可以用连续函数的优化方法来寻找最优的策略了，最常用的方法就是梯度上升法。</p>
<h3 id="策略梯度的优化目标"><a href="#策略梯度的优化目标" class="headerlink" title="策略梯度的优化目标"></a>策略梯度的优化目标</h3><table>
<thead>
<tr>
<th>优化目标</th>
<th>定义式</th>
</tr>
</thead>
<tbody><tr>
<td>初始状态收获的期望</td>
<td><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220111163508.png" alt="image-20220111163508366" style="zoom: 33%;" /></td>
</tr>
<tr>
<td>平均价值</td>
<td><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220117182507.png" alt="image-20220111163740877" style="zoom:33%;" /></td>
</tr>
<tr>
<td>每一时间步的平均奖励</td>
<td><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220111164026.png" alt="image-20220111164026634" style="zoom:33%;" /></td>
</tr>
<tr>
<td>……</td>
<td>……</td>
</tr>
</tbody></table>
<p>无论哪种优化目标，其梯度都可以表示为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220111164440.png" alt="image-20220111164439961" style="zoom: 45%;" />

<p>不同优化目标只有 <code>Qπ(s,a)</code> 部分不同，策略部分不改变，称为<strong>分值函数</strong>：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220111164711.png" alt="image-20220111164710941" style="zoom: 45%;" />

<h3 id="策略函数的设计"><a href="#策略函数的设计" class="headerlink" title="策略函数的设计"></a>策略函数的设计</h3><h4 id="Softmax策略函数"><a href="#Softmax策略函数" class="headerlink" title="Softmax策略函数"></a>Softmax策略函数</h4><p>&emsp;&emsp;主要应用于离散空间中，使用描述状态和行为的特征 <code>ϕ(s,a)</code> 与参数 <code>θ </code>的线性组合来权衡一个行为发生的几率，即：   </p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220111165453.png" alt="image-20220111165453539" style="zoom:45%;" />

<p>则通过求导求出对应的分值函数为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220111165244.png" alt="image-20220111165244127" style="zoom: 45%;" />

<h4 id="Gauss策略函数"><a href="#Gauss策略函数" class="headerlink" title="Gauss策略函数"></a>Gauss策略函数</h4><p>&emsp;&emsp;主要应用于连续行为空间，对应的行为从高斯分布 <code>N(ϕ(s)^T θ,σ^2)</code> 中产生。对应的分值函数通过对策略求导可以得到：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220111165543.png" alt="image-20220111165543029" style="zoom:45%;" />

<p>&nbsp;</p>
<h2 id="蒙特卡罗策略梯度算法"><a href="#蒙特卡罗策略梯度算法" class="headerlink" title="蒙特卡罗策略梯度算法"></a>蒙特卡罗策略梯度算法</h2><p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112085002.png" alt="image-20220112085002544"></p>
<p>&emsp;&emsp;蒙特卡罗策略梯度是最简单的策略梯度算法，使用价值函数 v(s) 来近似代替策略梯度公式里面的 Qπ(s,a)。</p>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>输入：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">N个蒙特卡罗完整序列，训练步长α</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">策略函数的参数θ</span><br></pre></td></tr></table></figure>

<p>流程：</p>
<ol>
<li><p>for 每个蒙特卡罗序列：</p>
<ul>
<li><p>用蒙特卡罗法计算序列每个时间位置t的状态价值 vt</p>
</li>
<li><p>对序列每个时间位置t，使用梯度上升法更新策略函数的参数θ：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112084515.png" alt="image-20220112084515658" style="zoom:50%;" /></li>
</ul>
</li>
<li><p>返回策略函数的参数θ（策略函数可以是Softmax策略，高斯策略或者其他策略 ）</p>
</li>
</ol>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>&emsp;&emsp;以OpenAI Gym中的CartPole-v0游戏为例，介绍参见<a target="_blank" rel="noopener" href="https://github.com/openai/gym/wiki/CartPole-v0">这里</a>。它的基本要求就是控制下面的cart移动使连接在上面的pole保持垂直不倒。这个任务只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个cart的位置和速度， pole的角度和角速度，4维的特征。坚持到200分的奖励则为过关。</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><p>&emsp;&emsp;这里采用somftmax策略函数，用三层softmax神经网络表示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="comment"># import tensorflow as tf</span></span><br><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line">tf.disable_v2_behavior()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyper Parameters</span></span><br><span class="line">GAMMA = <span class="number">0.95</span>  <span class="comment"># discount factor</span></span><br><span class="line">LEARNING_RATE = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Policy_Gradient</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, env</span>):</span></span><br><span class="line">        <span class="comment"># init some parameters</span></span><br><span class="line">        self.time_step = <span class="number">0</span></span><br><span class="line">        self.state_dim = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line">        self.action_dim = env.action_space.n</span><br><span class="line">        self.ep_obs, self.ep_as, self.ep_rs = [], [], []</span><br><span class="line">        self.create_softmax_network()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Init session</span></span><br><span class="line">        self.session = tf.InteractiveSession()</span><br><span class="line">        self.session.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_softmax_network</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># network weights</span></span><br><span class="line">        W1 = self.weight_variable([self.state_dim, <span class="number">20</span>])</span><br><span class="line">        b1 = self.bias_variable([<span class="number">20</span>])</span><br><span class="line">        W2 = self.weight_variable([<span class="number">20</span>, self.action_dim])</span><br><span class="line">        b2 = self.bias_variable([self.action_dim])</span><br><span class="line">        <span class="comment"># input layer</span></span><br><span class="line">        self.state_input = tf.placeholder(<span class="string">&quot;float&quot;</span>, [<span class="literal">None</span>, self.state_dim])</span><br><span class="line">        self.tf_acts = tf.placeholder(tf.int32, [<span class="literal">None</span>, ], name=<span class="string">&quot;actions_num&quot;</span>)</span><br><span class="line">        self.tf_vt = tf.placeholder(tf.float32, [<span class="literal">None</span>, ], name=<span class="string">&quot;actions_value&quot;</span>)</span><br><span class="line">        <span class="comment"># hidden layers</span></span><br><span class="line">        h_layer = tf.nn.relu(tf.matmul(self.state_input, W1) + b1)</span><br><span class="line">        <span class="comment"># softmax layer</span></span><br><span class="line">        self.softmax_input = tf.matmul(h_layer, W2) + b2</span><br><span class="line">        <span class="comment"># softmax output</span></span><br><span class="line">        self.all_act_prob = tf.nn.softmax(self.softmax_input, name=<span class="string">&#x27;act_prob&#x27;</span>)</span><br><span class="line">        self.neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.softmax_input,</span><br><span class="line">                                                                           labels=self.tf_acts)</span><br><span class="line">        self.loss = tf.reduce_mean(</span><br><span class="line">            self.neg_log_prob * self.tf_vt)  <span class="comment"># reward guided loss</span></span><br><span class="line"></span><br><span class="line">        self.train_op = tf.train.AdamOptimizer(</span><br><span class="line">            LEARNING_RATE).minimize(self.loss)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span>(<span class="params">self, shape</span>):</span></span><br><span class="line">        initial = tf.truncated_normal(shape)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span>(<span class="params">self, shape</span>):</span></span><br><span class="line">        initial = tf.constant(<span class="number">0.01</span>, shape=shape)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        prob_weights = self.session.run(self.all_act_prob, feed_dict=&#123;</span><br><span class="line">                                        self.state_input: observation[np.newaxis, :]&#125;)</span><br><span class="line">        <span class="comment"># select action w.r.t the actions prob</span></span><br><span class="line">        action = np.random.choice(</span><br><span class="line">            <span class="built_in">range</span>(prob_weights.shape[<span class="number">1</span>]), p=prob_weights.ravel())</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r</span>):</span></span><br><span class="line">        self.ep_obs.append(s)</span><br><span class="line">        self.ep_as.append(a)</span><br><span class="line">        self.ep_rs.append(r)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">        discounted_ep_rs = np.zeros_like(self.ep_rs)</span><br><span class="line">        running_add = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(self.ep_rs))):</span><br><span class="line">            running_add = running_add * GAMMA + self.ep_rs[t]</span><br><span class="line">            discounted_ep_rs[t] = running_add</span><br><span class="line"></span><br><span class="line">        discounted_ep_rs -= np.mean(discounted_ep_rs)</span><br><span class="line">        discounted_ep_rs /= np.std(discounted_ep_rs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># train on episode</span></span><br><span class="line">        self.session.run(self.train_op, feed_dict=&#123;</span><br><span class="line">            self.state_input: np.vstack(self.ep_obs),</span><br><span class="line">            self.tf_acts: np.array(self.ep_as),</span><br><span class="line">            self.tf_vt: discounted_ep_rs,</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        self.ep_obs, self.ep_as, self.ep_rs = [], [], []    <span class="comment"># empty episode data</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyper Parameters</span></span><br><span class="line">ENV_NAME = <span class="string">&#x27;CartPole-v0&#x27;</span></span><br><span class="line">EPISODE = <span class="number">3000</span>  <span class="comment"># Episode limitation</span></span><br><span class="line">STEP = <span class="number">3000</span>  <span class="comment"># Step limitation in an episode</span></span><br><span class="line">TEST = <span class="number">10</span>  <span class="comment"># The number of experiment test every 100 episode</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="comment"># initialize OpenAI Gym env and dqn agent</span></span><br><span class="line">    env = gym.make(ENV_NAME)</span><br><span class="line">    agent = Policy_Gradient(env)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(EPISODE):</span><br><span class="line">        <span class="comment"># initialize task</span></span><br><span class="line">        state = env.reset()</span><br><span class="line">        <span class="comment"># Train</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(STEP):</span><br><span class="line">            action = agent.choose_action(state)  <span class="comment"># e-greedy action for train</span></span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">            agent.store_transition(state, action, reward)</span><br><span class="line">            state = next_state</span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="comment">#print(&quot;stick for &quot;,step, &quot; steps&quot;)</span></span><br><span class="line">                agent.learn()</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Test every 100 episodes</span></span><br><span class="line">        <span class="keyword">if</span> episode % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            total_reward = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(TEST):</span><br><span class="line">                state = env.reset()</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(STEP):</span><br><span class="line">                    env.render()</span><br><span class="line">                    action = agent.choose_action(</span><br><span class="line">                        state)  <span class="comment"># direct action for test</span></span><br><span class="line">                    state, reward, done, _ = env.step(action)</span><br><span class="line">                    total_reward += reward</span><br><span class="line">                    <span class="keyword">if</span> done:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">            ave_reward = total_reward/TEST</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;episode: &#x27;</span>, episode, <span class="string">&#x27;Evaluation Average Reward:&#x27;</span>, ave_reward)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<h4 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h4><p>&emsp;&emsp;400轮左右收敛，但是并不稳定，平均Reward在200附近浮动</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220118104115.png" alt="image-20220118104115127" style="zoom: 40%;" />

<p>&nbsp;</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>&emsp;&emsp;策略梯度提供了和 DQN 之类的方法不同的新思路，但是简单的蒙特卡罗策略梯度reinforce算法却并不完美。蒙特卡罗法需要完全的序列样本才能做算法迭代，同时蒙特卡罗法使用收获的期望来计算状态价值，会导致行为有较多的变异性，参数更新的方向很可能不是策略梯度的最优方向。因此，Policy-based的强化学习方法还需要改进，例如 Policy-based与Value-based结合的策略梯度方法Actor-Critic。</p>
<p>&nbsp;</p>
<h2 id="Actor-Critic算法"><a href="#Actor-Critic算法" class="headerlink" title="Actor-Critic算法"></a>Actor-Critic算法</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>&emsp;&emsp;Actor-Critic包括两部分：演员(Actor)、评价者(Critic)。其中Actor使用策略函数，负责生成动作(Action)并和环境交互，而Critic使用价值函数，负责评估Actor的表现，并指导Actor下一阶段的动作。在Policy-Gradient中，策略函数就是Actor，但是那里没有Critic，而是使用了蒙特卡罗法来计算每一步的价值部分从而替代了Critic的功能，但是场景比较受限。因此现在使用类似DQN中用的价值近似来替代蒙特卡罗法，作为一个比较通用的Critic。</p>
<p>&emsp;&emsp;Actor-Critic算法中做了两组近似：</p>
<h4 id="策略函数的近似"><a href="#策略函数的近似" class="headerlink" title="策略函数的近似"></a>策略函数的近似</h4><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112090509.png" alt="image-20220112090509167" style="zoom:45%;" />

<h4 id="价值函数的近似"><a href="#价值函数的近似" class="headerlink" title="价值函数的近似"></a>价值函数的近似</h4><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112090441.png" alt="image-20220112090441163" style="zoom: 45%;" />

<p>&emsp;&emsp;也就是说，蒙特卡罗reinforce算法参数的参数更新公式（如下）中，vt的求取不再用蒙特卡罗法那样采样平均而是用DQN的近似价值函数求取，而分值函数部分∇θlogπθ(s,a)不变。</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112090613.png" alt="image-20220112090613113" style="zoom:45%;" />

<p>&emsp;&emsp;通常使用Q网络来近似价值函数，所以汇总来说，就是Critic通过Q网络计算状态的最优价值vt，而Actor利用vt这个最优价值迭代更新策略函数的参数θ，进而选择动作，并得到反馈和新的状态，Critic使用反馈和新的状态更新Q网络参数w，在后面Critic会使用新的网络参数w来帮Actor计算状态的最优价值vt。</p>
<p>&nbsp;</p>
<h4 id="可选形式"><a href="#可选形式" class="headerlink" title="可选形式"></a>可选形式</h4><p>&emsp;&emsp;Critic有多种选择，其网络输出作为Actor的优化目标，对应Actor多种策略梯度更新方法：</p>
<table>
<thead>
<tr>
<th>Critic</th>
<th>对应Actor更新方法</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>状态价值</td>
<td><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112090936.png" alt="image-20220112090936577" style="zoom:33%;" /></td>
<td></td>
</tr>
<tr>
<td>动作价值</td>
<td><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112091001.png" alt="image-20220112091001498" style="zoom:33%;" /></td>
<td></td>
</tr>
<tr>
<td>TD误差</td>
<td><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112091050.png" alt="image-20220112091050308" style="zoom:33%;" /></td>
<td><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112091237.png" alt="image-20220112091237744" style="zoom:33%;" /></td>
</tr>
<tr>
<td>优势函数</td>
<td><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112091536.png" alt="image-20220112091536610" style="zoom: 33%;" /></td>
<td><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112091703.png" alt="image-20220112091703913" style="zoom:33%;" /></td>
</tr>
<tr>
<td>TD(λ)误差</td>
<td><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112092148.png" alt="image-20220112092148003" style="zoom:33%;" /></td>
<td>基于后向TD(λ)误差，是TD误差和效用迹的乘积</td>
</tr>
</tbody></table>
<p>&emsp;&emsp;对于Critic本身的网络参数w，一般都是使用 <code>均方误差损失函数</code> 来做迭代更新，类似DQN系列中的迭代方法。</p>
<p>&nbsp;</p>
<h3 id="算法流程（基于TD误差）"><a href="#算法流程（基于TD误差）" class="headerlink" title="算法流程（基于TD误差）"></a>算法流程（基于TD误差）</h3><p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112092450.png" alt="image-20220112092450135"></p>
<p>（评估点基于TD误差，Critic使用神经网络来计算TD误差并更新网络参数，Actor也使用Critic神经网络的结果来更新网络参数）</p>
<p>输入：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">迭代轮数T、状态特征维度n、动作集A、步长α、β、衰减因子γ、探索率ϵ、Critic网络结构、Actor网络结构</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Actor网络参数θ、Critic网络参数w</span><br></pre></td></tr></table></figure>

<p>流程：</p>
<ol>
<li><p>随机初始化所有的状态和动作对应的价值Q</p>
</li>
<li><p>for i in [ 1, T ]：</p>
<ol>
<li>初始化S为当前状态序列的第一个状态，拿到其特征向量Φ(S)</li>
<li>把Φ(S)输入Actor网络，得到输出动作A，并由环境获得新状态S’、奖励R</li>
<li>把Φ(S)、Φ(S‘ )分别输入Critic网络，分别得到输出的Q值V(S)、V(S’ )</li>
<li>计算TD误差 δ = R + γV(S′ ) − V(S)</li>
<li>计算均方差损失函数 ∑(R+γV(S′)−V(S,w))^2，梯度反向传播更新Critic网络参数w</li>
<li>更新Actor网络参数：</li>
</ol>
</li>
</ol>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112091050.png" alt="image-20220112091050308" style="zoom:45%;" />

<p>（这里以TD误差为评估点， 分值函数∇θlogπθ(St,A)可以选择softmax或者高斯分值函数 )</p>
<p>对应模型结构：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112103230.png" alt="image-20220112103230292" style="zoom:50%;" />

<p>&nbsp;</p>
<h3 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h3><h4 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h4><p>&emsp;&emsp;CartPole-v0游戏</p>
<h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><p>&emsp;&emsp;代码分为Actor和Critic两部分，Actor部分和上面策略梯度的代码差别不大，主要是梯度更新方法不同，策略梯度使用蒙特卡洛法计算出价值，这里Actor使用的是TD误差。Critic部分类似于之前的DQN的三层神经网络，但是输出只有一维，并且没有经验回放。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="comment"># import tensorflow as tf</span></span><br><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line">tf.disable_v2_behavior()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyper Parameters</span></span><br><span class="line">GAMMA = <span class="number">0.95</span>  <span class="comment"># discount factor</span></span><br><span class="line">LEARNING_RATE = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, env, sess</span>):</span></span><br><span class="line">        <span class="comment"># init some parameters</span></span><br><span class="line">        self.time_step = <span class="number">0</span></span><br><span class="line">        self.state_dim = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line">        self.action_dim = env.action_space.n</span><br><span class="line">        self.create_softmax_network()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Init session</span></span><br><span class="line">        self.session = sess</span><br><span class="line">        self.session.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_softmax_network</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># network weights</span></span><br><span class="line">        W1 = self.weight_variable([self.state_dim, <span class="number">20</span>])</span><br><span class="line">        b1 = self.bias_variable([<span class="number">20</span>])</span><br><span class="line">        W2 = self.weight_variable([<span class="number">20</span>, self.action_dim])</span><br><span class="line">        b2 = self.bias_variable([self.action_dim])</span><br><span class="line">        <span class="comment"># input layer</span></span><br><span class="line">        self.state_input = tf.placeholder(<span class="string">&quot;float&quot;</span>, [<span class="literal">None</span>, self.state_dim])</span><br><span class="line">        self.tf_acts = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="number">2</span>], name=<span class="string">&quot;actions_num&quot;</span>)</span><br><span class="line">        self.td_error = tf.placeholder(</span><br><span class="line">            tf.float32, <span class="literal">None</span>, <span class="string">&quot;td_error&quot;</span>)  <span class="comment"># TD_error</span></span><br><span class="line">        <span class="comment"># hidden layers</span></span><br><span class="line">        h_layer = tf.nn.relu(tf.matmul(self.state_input, W1) + b1)</span><br><span class="line">        <span class="comment"># softmax layer</span></span><br><span class="line">        self.softmax_input = tf.matmul(h_layer, W2) + b2</span><br><span class="line">        <span class="comment"># softmax output</span></span><br><span class="line">        self.all_act_prob = tf.nn.softmax(self.softmax_input, name=<span class="string">&#x27;act_prob&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        self.neg_log_prob = tf.nn.softmax_cross_entropy_with_logits(logits=self.softmax_input,</span><br><span class="line">                                                                    labels=self.tf_acts)</span><br><span class="line">        self.exp = tf.reduce_mean(self.neg_log_prob * self.td_error)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里需要最大化当前策略的价值，因此需要最大化self.exp,即最小化-self.exp</span></span><br><span class="line">        self.train_op = tf.train.AdamOptimizer(</span><br><span class="line">            LEARNING_RATE).minimize(-self.exp)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span>(<span class="params">self, shape</span>):</span></span><br><span class="line">        initial = tf.truncated_normal(shape)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span>(<span class="params">self, shape</span>):</span></span><br><span class="line">        initial = tf.constant(<span class="number">0.01</span>, shape=shape)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        prob_weights = self.session.run(self.all_act_prob, feed_dict=&#123;</span><br><span class="line">                                        self.state_input: observation[np.newaxis, :]&#125;)</span><br><span class="line">        <span class="comment"># select action w.r.t the actions prob</span></span><br><span class="line">        action = np.random.choice(</span><br><span class="line">            <span class="built_in">range</span>(prob_weights.shape[<span class="number">1</span>]), p=prob_weights.ravel())</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, state, action, td_error</span>):</span></span><br><span class="line">        s = state[np.newaxis, :]</span><br><span class="line">        one_hot_action = np.zeros(self.action_dim)</span><br><span class="line">        one_hot_action[action] = <span class="number">1</span></span><br><span class="line">        a = one_hot_action[np.newaxis, :]</span><br><span class="line">        <span class="comment"># train on episode</span></span><br><span class="line">        self.session.run(self.train_op, feed_dict=&#123;</span><br><span class="line">            self.state_input: s,</span><br><span class="line">            self.tf_acts: a,</span><br><span class="line">            self.td_error: td_error,</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">EPSILON = <span class="number">0.01</span>  <span class="comment"># final value of epsilon</span></span><br><span class="line">REPLAY_SIZE = <span class="number">10000</span>  <span class="comment"># experience replay buffer size</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span>  <span class="comment"># size of minibatch</span></span><br><span class="line">REPLACE_TARGET_FREQ = <span class="number">10</span>  <span class="comment"># frequency to update target Q network</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, env, sess</span>):</span></span><br><span class="line">        <span class="comment"># init some parameters</span></span><br><span class="line">        self.time_step = <span class="number">0</span></span><br><span class="line">        self.epsilon = EPSILON</span><br><span class="line">        self.state_dim = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line">        self.action_dim = env.action_space.n</span><br><span class="line"></span><br><span class="line">        self.create_Q_network()</span><br><span class="line">        self.create_training_method()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Init session</span></span><br><span class="line">        self.session = sess</span><br><span class="line">        self.session.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_Q_network</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># network weights</span></span><br><span class="line">        W1q = self.weight_variable([self.state_dim, <span class="number">20</span>])</span><br><span class="line">        b1q = self.bias_variable([<span class="number">20</span>])</span><br><span class="line">        W2q = self.weight_variable([<span class="number">20</span>, <span class="number">1</span>])</span><br><span class="line">        b2q = self.bias_variable([<span class="number">1</span>])</span><br><span class="line">        self.state_input = tf.placeholder(</span><br><span class="line">            tf.float32, [<span class="number">1</span>, self.state_dim], <span class="string">&quot;state&quot;</span>)</span><br><span class="line">        <span class="comment"># hidden layers</span></span><br><span class="line">        h_layerq = tf.nn.relu(tf.matmul(self.state_input, W1q) + b1q)</span><br><span class="line">        <span class="comment"># Q Value layer</span></span><br><span class="line">        self.Q_value = tf.matmul(h_layerq, W2q) + b2q</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_training_method</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.next_value = tf.placeholder(tf.float32, [<span class="number">1</span>, <span class="number">1</span>], <span class="string">&quot;v_next&quot;</span>)</span><br><span class="line">        self.reward = tf.placeholder(tf.float32, <span class="literal">None</span>, <span class="string">&#x27;reward&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;squared_TD_error&#x27;</span>):</span><br><span class="line">            self.td_error = self.reward + GAMMA * self.next_value - self.Q_value</span><br><span class="line">            self.loss = tf.square(self.td_error)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;train&#x27;</span>):</span><br><span class="line">            self.train_op = tf.train.AdamOptimizer(</span><br><span class="line">                self.epsilon).minimize(self.loss)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_Q_network</span>(<span class="params">self, state, reward, next_state</span>):</span></span><br><span class="line">        s, s_ = state[np.newaxis, :], next_state[np.newaxis, :]</span><br><span class="line">        v_ = self.session.run(self.Q_value, &#123;self.state_input: s_&#125;)</span><br><span class="line">        td_error, _ = self.session.run([self.td_error, self.train_op],</span><br><span class="line">                                       &#123;self.state_input: s, self.next_value: v_, self.reward: reward&#125;)</span><br><span class="line">        <span class="keyword">return</span> td_error</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span>(<span class="params">self, shape</span>):</span></span><br><span class="line">        initial = tf.truncated_normal(shape)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span>(<span class="params">self, shape</span>):</span></span><br><span class="line">        initial = tf.constant(<span class="number">0.01</span>, shape=shape)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyper Parameters</span></span><br><span class="line">ENV_NAME = <span class="string">&#x27;CartPole-v0&#x27;</span></span><br><span class="line">EPISODE = <span class="number">3000</span>  <span class="comment"># Episode limitation</span></span><br><span class="line">STEP = <span class="number">3000</span>  <span class="comment"># Step limitation in an episode</span></span><br><span class="line">TEST = <span class="number">10</span>  <span class="comment"># The number of experiment test every 100 episode</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="comment"># initialize OpenAI Gym env and dqn agent</span></span><br><span class="line">    sess = tf.InteractiveSession()</span><br><span class="line">    env = gym.make(ENV_NAME)</span><br><span class="line">    actor = Actor(env, sess)</span><br><span class="line">    critic = Critic(env, sess)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(EPISODE):</span><br><span class="line">        <span class="comment"># initialize task</span></span><br><span class="line">        state = env.reset()</span><br><span class="line">        <span class="comment"># Train</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(STEP):</span><br><span class="line">            action = actor.choose_action(state)  <span class="comment"># e-greedy action for train</span></span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">            <span class="comment"># gradient = grad[r + gamma * V(s_) - V(s)]</span></span><br><span class="line">            td_error = critic.train_Q_network(state, reward, next_state)</span><br><span class="line">            <span class="comment"># true_gradient = grad[logPi(s,a) * td_error]</span></span><br><span class="line">            actor.learn(state, action, td_error)</span><br><span class="line">            state = next_state</span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Test every 100 episodes</span></span><br><span class="line">        <span class="keyword">if</span> episode % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            total_reward = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(TEST):</span><br><span class="line">                state = env.reset()</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(STEP):</span><br><span class="line">                    env.render()</span><br><span class="line">                    action = actor.choose_action(</span><br><span class="line">                        state)  <span class="comment"># direct action for test</span></span><br><span class="line">                    state, reward, done, _ = env.step(action)</span><br><span class="line">                    total_reward += reward</span><br><span class="line">                    <span class="keyword">if</span> done:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">            ave_reward = total_reward/TEST</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;episode: &#x27;</span>, episode, <span class="string">&#x27;Evaluation Average Reward:&#x27;</span>, ave_reward)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<h4 id="效果-1"><a href="#效果-1" class="headerlink" title="效果"></a>效果</h4><p>&emsp;&emsp;很明显，这个算法很难收敛，平均Reward总是很低</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220118105019.png" alt="image-20220118105019197" style="zoom:50%;" />

<p>&nbsp;</p>
<h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>&emsp;&emsp;基本版的Actor-Critic算法已经是一个很好的算法框架，但是离实际应用还比较远。主要原因是这里有两个神经网络，都需要梯度更新，而且互相依赖，所以难以收敛。</p>
<p>&emsp;&emsp;目前改进的比较好的有两个经典算法，一个是DDPG算法，使用了双Actor神经网络和双Critic神经网络的方法来改善收敛性。这个方法在从DQN到Nature DQN的过程中用过。另一个是A3C算法，使用了多线程的方式，一个主线程负责更新Actor和Critic的参数，多个辅线程负责分别和环境交互，得到梯度更新值，汇总更新主线程的参数。而所有的辅线程会定期从主线程更新网络参数。这些辅线程起到了类似DQN中经验回放的作用，但是效果更好。</p>
<p>&nbsp;</p>
<h2 id="A3C"><a href="#A3C" class="headerlink" title="A3C"></a>A3C</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><h4 id="针对问题"><a href="#针对问题" class="headerlink" title="针对问题"></a>针对问题</h4><p>&emsp;&emsp;单纯的Actor-Critic算法难以收敛</p>
<h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><p>&emsp;&emsp;DQN算法在解决难收敛问题中使用了经验回放的技巧，所以A3C其实也可以使用类似的手段，但是实际上A3C更进一步，使用多线程，多个相同的网络结构在多个线程里同时与环境交互进行学习，每个线程训练时经历的的环境状态、动作、奖励都供同一个公用的网络进行训练。A3C避免了经验回放相关性过强的问题，由此实现了异步并发的学习模型。</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220116163324.png" alt="image-20220112093804938" style="zoom: 25%;" />

<h5 id="异步训练框架"><a href="#异步训练框架" class="headerlink" title="异步训练框架"></a>异步训练框架</h5><p>&emsp;&emsp;图中上部的Global Network就是共享的公共部分，主要是一个公共的神经网络模型，这个神经网络包括Actor网络和Critic网络两部分的功能。下面有n个worker线程，每个线程里有和公共的神经网络一样的网络结构，每个线程会独立的和环境进行交互得到经验数据，这些线程之间互不干扰，独立运行。</p>
<p>&emsp;&emsp;每个线程和环境交互到一定量的数据后，就计算在自己线程里的神经网络损失函数的梯度，但是这些梯度却并不更新自己线程里的神经网络，而是去更新公共的神经网络。也就是n个线程会独立的使用累积的梯度分别更新公共部分的神经网络模型参数。每隔一段时间，线程会将自己的神经网络的参数更新为公共神经网络的参数，进而指导后面的环境交互。</p>
<p>&emsp;&emsp;可见，公共部分的网络模型就是待训练的模型，而线程里的网络模型主要是用于和环境交互使用的，这些线程里的模型可以帮助线程更好的和环境交互，拿到高质量的数据帮助模型更快收敛。</p>
<h5 id="网络结构优化"><a href="#网络结构优化" class="headerlink" title="网络结构优化"></a>网络结构优化</h5><p>&emsp;&emsp;A3C把两个网络放到了一起，即输入状态S，可以输出状态价值V和对应的策略π。不过仍然可以把Actor和Critic看做独立的两块，分别处理：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112094415.png" alt="image-20220112094415642" style="zoom: 25%;" />

<h5 id="Critic选择的优化"><a href="#Critic选择的优化" class="headerlink" title="Critic选择的优化"></a>Critic选择的优化</h5><p>&emsp;&emsp;在A3C中Critic选择为优势函数的近似，省略<a href="extension://bfdogplmndidlpjfhoijckpakkdjkkil/pdf/viewer.html?file=http%3A%2F%2Fproceedings.mlr.press%2Fv48%2Fmniha16.pdf">A3C论文</a>中的推导，直接给出其计算公式如下：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112095035.png" alt="image-20220112095035246" style="zoom:45%;" />

<h5 id="损失函数优化"><a href="#损失函数优化" class="headerlink" title="损失函数优化"></a>损失函数优化</h5><p>&emsp;&emsp;在Actor的损失函数中加入了策略π的熵项，系数为c：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112095453.png" alt="image-20220112095453116" style="zoom:45%;" />

<p>&nbsp;</p>
<h3 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a>算法流程</h3><p>A3C是多线程的，这里给出任意一个线程的算法流程：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220113125452.png" alt="image-20220113125452328" style="zoom: 50%;" />

<p>输入：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">公共部分的A3C神经网络结构，对应参数位θ、w，本线程的A3C神经网络结构，对应参数θ′、w′，全局共享的迭代轮数T，全局最大迭代次数Tmax，线程内单次迭代时间序列最大长度Tmax，状态特征维度n, 动作集A，步长α、β，熵系数c，衰减因子γ</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">公共部分的A3C神经网络参数θ、w</span><br></pre></td></tr></table></figure>

<p>流程：</p>
<ol>
<li><p>更新时间序列 t=1</p>
</li>
<li><p>重置Actor和Critic的梯度更新量：<code>dθ=0、dw=0</code></p>
</li>
<li><p>从公共部分的A3C神经网络同步参数到本线程的神经网络：<code>θ’=θ、w’=w</code></p>
</li>
<li><p><code>tstart = t</code>，初始化状态st</p>
</li>
<li><p>if ( st不是终止状态 且 t - tstart != tmax )：</p>
<ul>
<li>基于策略 <code>π(at|st;θ)</code> 选择出动作at</li>
<li>执行动作at得到奖励rt和新状态st+1</li>
<li>t = t+1、T = T+1</li>
</ul>
</li>
<li><p>else：</p>
<ul>
<li><p>计算最后一个时间序列位置st的Q(s,t)：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112101419.png" alt="image-20220112101419222" style="zoom:45%;" /></li>
<li><p>for i in [t-1, tstart]：</p>
<ol>
<li><p>计算每个时刻的Q(s, i)：Q(s, i) = ri + γQ(s, i+1)</p>
</li>
<li><p>累计Actor的本地梯度更新：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112102319.png" alt="image-20220112102319176" style="zoom:45%;" /></li>
<li><p>累计Critic的本地梯度更新：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220112102707.png" alt="image-20220112102707754" style="zoom:45%;" /></li>
</ol>
</li>
<li><p>更新全局神经网络的参数：<code>θ=θ-αdθ、w=w-βdw</code></p>
</li>
<li><p>if ( T&gt;Tmax )：算法结束，输出公共部分的A3C神经网络参数 θ、w；else：进入步骤3</p>
</li>
</ul>
</li>
</ol>
<p>&nbsp;</p>
<h3 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h3><h4 id="问题-2"><a href="#问题-2" class="headerlink" title="问题"></a>问题</h4><p>&emsp;&emsp;CartPole-v0游戏</p>
<h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="comment"># import tensorflow as tf</span></span><br><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line">tf.disable_v2_behavior()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">GAME = <span class="string">&#x27;CartPole-v0&#x27;</span></span><br><span class="line">OUTPUT_GRAPH = <span class="literal">True</span></span><br><span class="line">LOG_DIR = <span class="string">&#x27;./log&#x27;</span></span><br><span class="line">N_WORKERS = <span class="number">3</span></span><br><span class="line">MAX_GLOBAL_EP = <span class="number">3000</span></span><br><span class="line">GLOBAL_NET_SCOPE = <span class="string">&#x27;Global_Net&#x27;</span></span><br><span class="line">UPDATE_GLOBAL_ITER = <span class="number">100</span></span><br><span class="line">GAMMA = <span class="number">0.9</span></span><br><span class="line">ENTROPY_BETA = <span class="number">0.001</span></span><br><span class="line">LR_A = <span class="number">0.001</span>    <span class="comment"># learning rate for actor</span></span><br><span class="line">LR_C = <span class="number">0.001</span>    <span class="comment"># learning rate for critic</span></span><br><span class="line">GLOBAL_RUNNING_R = []</span><br><span class="line">GLOBAL_EP = <span class="number">0</span></span><br><span class="line">STEP = <span class="number">3000</span>  <span class="comment"># Step limitation in an episode</span></span><br><span class="line">TEST = <span class="number">10</span>  <span class="comment"># The number of experiment test every 100 episode</span></span><br><span class="line"></span><br><span class="line">env = gym.make(GAME)</span><br><span class="line">N_S = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line">N_A = env.action_space.n</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ACNet</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, scope, globalAC=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> scope == GLOBAL_NET_SCOPE:   <span class="comment"># get global network</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">                self.s = tf.placeholder(tf.float32, [<span class="literal">None</span>, N_S], <span class="string">&#x27;S&#x27;</span>)</span><br><span class="line">                self.a_params, self.c_params = self._build_net(scope)[-<span class="number">2</span>:]</span><br><span class="line">        <span class="keyword">else</span>:   <span class="comment"># local net, calculate losses</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">                self.s = tf.placeholder(tf.float32, [<span class="literal">None</span>, N_S], <span class="string">&#x27;S&#x27;</span>)</span><br><span class="line">                self.a_his = tf.placeholder(tf.int32, [<span class="literal">None</span>, ], <span class="string">&#x27;A&#x27;</span>)</span><br><span class="line">                self.v_target = tf.placeholder(</span><br><span class="line">                    tf.float32, [<span class="literal">None</span>, <span class="number">1</span>], <span class="string">&#x27;Vtarget&#x27;</span>)</span><br><span class="line"></span><br><span class="line">                self.a_prob, self.v, self.a_params, self.c_params = self._build_net(</span><br><span class="line">                    scope)</span><br><span class="line"></span><br><span class="line">                td = tf.subtract(self.v_target, self.v, name=<span class="string">&#x27;TD_error&#x27;</span>)</span><br><span class="line">                <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;c_loss&#x27;</span>):</span><br><span class="line">                    self.c_loss = tf.reduce_mean(tf.square(td))</span><br><span class="line"></span><br><span class="line">                <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;a_loss&#x27;</span>):</span><br><span class="line">                    log_prob = tf.reduce_sum(tf.log(self.a_prob + <span class="number">1e-5</span>) * tf.one_hot(</span><br><span class="line">                        self.a_his, N_A, dtype=tf.float32), axis=<span class="number">1</span>, keep_dims=<span class="literal">True</span>)</span><br><span class="line">                    exp_v = log_prob * tf.stop_gradient(td)</span><br><span class="line">                    entropy = -tf.reduce_sum(self.a_prob * tf.log(self.a_prob + <span class="number">1e-5</span>),</span><br><span class="line">                                             axis=<span class="number">1</span>, keep_dims=<span class="literal">True</span>)  <span class="comment"># encourage exploration</span></span><br><span class="line">                    self.exp_v = ENTROPY_BETA * entropy + exp_v</span><br><span class="line">                    self.a_loss = tf.reduce_mean(-self.exp_v)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;local_grad&#x27;</span>):</span><br><span class="line">                    self.a_grads = tf.gradients(self.a_loss, self.a_params)</span><br><span class="line">                    self.c_grads = tf.gradients(self.c_loss, self.c_params)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;sync&#x27;</span>):</span><br><span class="line">                <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;pull&#x27;</span>):</span><br><span class="line">                    self.pull_a_params_op = [l_p.assign(g_p) <span class="keyword">for</span> l_p, g_p <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">                        self.a_params, globalAC.a_params)]</span><br><span class="line">                    self.pull_c_params_op = [l_p.assign(g_p) <span class="keyword">for</span> l_p, g_p <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">                        self.c_params, globalAC.c_params)]</span><br><span class="line">                <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;push&#x27;</span>):</span><br><span class="line">                    self.update_a_op = OPT_A.apply_gradients(</span><br><span class="line">                        <span class="built_in">zip</span>(self.a_grads, globalAC.a_params))</span><br><span class="line">                    self.update_c_op = OPT_C.apply_gradients(</span><br><span class="line">                        <span class="built_in">zip</span>(self.c_grads, globalAC.c_params))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_net</span>(<span class="params">self, scope</span>):</span></span><br><span class="line">        w_init = tf.random_normal_initializer(<span class="number">0.</span>, <span class="number">.1</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;actor&#x27;</span>):</span><br><span class="line">            l_a = tf.layers.dense(self.s, <span class="number">200</span>, tf.nn.relu6,</span><br><span class="line">                                  kernel_initializer=w_init, name=<span class="string">&#x27;la&#x27;</span>)</span><br><span class="line">            a_prob = tf.layers.dense(</span><br><span class="line">                l_a, N_A, tf.nn.softmax, kernel_initializer=w_init, name=<span class="string">&#x27;ap&#x27;</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;critic&#x27;</span>):</span><br><span class="line">            l_c = tf.layers.dense(self.s, <span class="number">100</span>, tf.nn.relu6,</span><br><span class="line">                                  kernel_initializer=w_init, name=<span class="string">&#x27;lc&#x27;</span>)</span><br><span class="line">            v = tf.layers.dense(</span><br><span class="line">                l_c, <span class="number">1</span>, kernel_initializer=w_init, name=<span class="string">&#x27;v&#x27;</span>)  <span class="comment"># state value</span></span><br><span class="line">        a_params = tf.get_collection(</span><br><span class="line">            tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + <span class="string">&#x27;/actor&#x27;</span>)</span><br><span class="line">        c_params = tf.get_collection(</span><br><span class="line">            tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + <span class="string">&#x27;/critic&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> a_prob, v, a_params, c_params</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_global</span>(<span class="params">self, feed_dict</span>):</span>  <span class="comment"># run by a local</span></span><br><span class="line">        <span class="comment"># local grads applies to global net</span></span><br><span class="line">        SESS.run([self.update_a_op, self.update_c_op], feed_dict)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pull_global</span>(<span class="params">self</span>):</span>  <span class="comment"># run by a local</span></span><br><span class="line">        SESS.run([self.pull_a_params_op, self.pull_c_params_op])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, s</span>):</span>  <span class="comment"># run by a local</span></span><br><span class="line">        prob_weights = SESS.run(self.a_prob, feed_dict=&#123;</span><br><span class="line">                                self.s: s[np.newaxis, :]&#125;)</span><br><span class="line">        action = np.random.choice(<span class="built_in">range</span>(prob_weights.shape[<span class="number">1</span>]),</span><br><span class="line">                                  p=prob_weights.ravel())  <span class="comment"># select action w.r.t the actions prob</span></span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Worker</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, globalAC</span>):</span></span><br><span class="line">        self.env = gym.make(GAME).unwrapped</span><br><span class="line">        self.name = name</span><br><span class="line">        self.AC = ACNet(name, globalAC)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">work</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">global</span> GLOBAL_RUNNING_R, GLOBAL_EP</span><br><span class="line">        total_step = <span class="number">1</span></span><br><span class="line">        buffer_s, buffer_a, buffer_r = [], [], []</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> COORD.should_stop() <span class="keyword">and</span> GLOBAL_EP &lt; MAX_GLOBAL_EP:</span><br><span class="line">            s = self.env.reset()</span><br><span class="line">            ep_r = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                <span class="comment"># if self.name == &#x27;W_0&#x27;:</span></span><br><span class="line">                <span class="comment">#     self.env.render()</span></span><br><span class="line">                a = self.AC.choose_action(s)</span><br><span class="line">                s_, r, done, info = self.env.step(a)</span><br><span class="line">                <span class="keyword">if</span> done:</span><br><span class="line">                    r = -<span class="number">5</span></span><br><span class="line">                ep_r += r</span><br><span class="line">                buffer_s.append(s)</span><br><span class="line">                buffer_a.append(a)</span><br><span class="line">                buffer_r.append(r)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> total_step % UPDATE_GLOBAL_ITER == <span class="number">0</span> <span class="keyword">or</span> done:   <span class="comment"># update global and assign to local net</span></span><br><span class="line">                    <span class="keyword">if</span> done:</span><br><span class="line">                        v_s_ = <span class="number">0</span>   <span class="comment"># terminal</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        v_s_ = SESS.run(</span><br><span class="line">                            self.AC.v, &#123;self.AC.s: s_[np.newaxis, :]&#125;)[<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">                    buffer_v_target = []</span><br><span class="line">                    <span class="keyword">for</span> r <span class="keyword">in</span> buffer_r[::-<span class="number">1</span>]:    <span class="comment"># reverse buffer r</span></span><br><span class="line">                        v_s_ = r + GAMMA * v_s_</span><br><span class="line">                        buffer_v_target.append(v_s_)</span><br><span class="line">                    buffer_v_target.reverse()</span><br><span class="line"></span><br><span class="line">                    buffer_s, buffer_a, buffer_v_target = np.vstack(</span><br><span class="line">                        buffer_s), np.array(buffer_a), np.vstack(buffer_v_target)</span><br><span class="line">                    feed_dict = &#123;</span><br><span class="line">                        self.AC.s: buffer_s,</span><br><span class="line">                        self.AC.a_his: buffer_a,</span><br><span class="line">                        self.AC.v_target: buffer_v_target,</span><br><span class="line">                    &#125;</span><br><span class="line">                    self.AC.update_global(feed_dict)</span><br><span class="line"></span><br><span class="line">                    buffer_s, buffer_a, buffer_r = [], [], []</span><br><span class="line">                    self.AC.pull_global()</span><br><span class="line"></span><br><span class="line">                s = s_</span><br><span class="line">                total_step += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> done:</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">len</span>(GLOBAL_RUNNING_R) == <span class="number">0</span>:  <span class="comment"># record running episode reward</span></span><br><span class="line">                        GLOBAL_RUNNING_R.append(ep_r)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        GLOBAL_RUNNING_R.append(</span><br><span class="line">                            <span class="number">0.99</span> * GLOBAL_RUNNING_R[-<span class="number">1</span>] + <span class="number">0.01</span> * ep_r)</span><br><span class="line">                    <span class="built_in">print</span>(</span><br><span class="line">                        self.name,</span><br><span class="line">                        <span class="string">&quot;Ep:&quot;</span>, GLOBAL_EP,</span><br><span class="line">                        <span class="string">&quot;| Ep_r: %i&quot;</span> % GLOBAL_RUNNING_R[-<span class="number">1</span>],</span><br><span class="line">                    )</span><br><span class="line">                    GLOBAL_EP += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    SESS = tf.Session()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.device(<span class="string">&quot;/cpu:0&quot;</span>):</span><br><span class="line">        OPT_A = tf.train.RMSPropOptimizer(LR_A, name=<span class="string">&#x27;RMSPropA&#x27;</span>)</span><br><span class="line">        OPT_C = tf.train.RMSPropOptimizer(LR_C, name=<span class="string">&#x27;RMSPropC&#x27;</span>)</span><br><span class="line">        GLOBAL_AC = ACNet(GLOBAL_NET_SCOPE)  <span class="comment"># we only need its params</span></span><br><span class="line">        workers = []</span><br><span class="line">        <span class="comment"># Create worker</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N_WORKERS):</span><br><span class="line">            i_name = <span class="string">&#x27;W_%i&#x27;</span> % i   <span class="comment"># worker name</span></span><br><span class="line">            workers.append(Worker(i_name, GLOBAL_AC))</span><br><span class="line"></span><br><span class="line">    COORD = tf.train.Coordinator()</span><br><span class="line">    SESS.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> OUTPUT_GRAPH:</span><br><span class="line">        <span class="keyword">if</span> os.path.exists(LOG_DIR):</span><br><span class="line">            shutil.rmtree(LOG_DIR)</span><br><span class="line">        tf.summary.FileWriter(LOG_DIR, SESS.graph)</span><br><span class="line"></span><br><span class="line">    worker_threads = []</span><br><span class="line">    <span class="keyword">for</span> worker <span class="keyword">in</span> workers:</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">job</span>():</span> <span class="keyword">return</span> worker.work()</span><br><span class="line">        t = threading.Thread(target=job)</span><br><span class="line">        t.start()</span><br><span class="line">        worker_threads.append(t)</span><br><span class="line">    COORD.join(worker_threads)</span><br><span class="line"></span><br><span class="line">    testWorker = Worker(<span class="string">&quot;test&quot;</span>, GLOBAL_AC)</span><br><span class="line">    testWorker.AC.pull_global()</span><br><span class="line"></span><br><span class="line">    total_reward = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(TEST):</span><br><span class="line">        state = env.reset()</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(STEP):</span><br><span class="line">            env.render()</span><br><span class="line">            action = testWorker.AC.choose_action(</span><br><span class="line">                state)  <span class="comment"># direct action for test</span></span><br><span class="line">            state, reward, done, _ = env.step(action)</span><br><span class="line">            total_reward += reward</span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    ave_reward = total_reward / TEST</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;episode: &#x27;</span>, GLOBAL_EP, <span class="string">&#x27;Evaluation Average Reward:&#x27;</span>, ave_reward)</span><br><span class="line"></span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(GLOBAL_RUNNING_R)), GLOBAL_RUNNING_R)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;step&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Total moving reward&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<h4 id="效果-2"><a href="#效果-2" class="headerlink" title="效果"></a>效果</h4><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220118130134.png" alt="image-20220118105654323" style="zoom: 37%;" />

<p>&nbsp;</p>
<h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><p>&emsp;&emsp;A3C解决了Actor-Critic难以收敛的问题，同时更重要的是，提供了一种通用的异步的并发的强化学习框架，也就是说，这个并发框架不光可以用于A3C，还可以用于其他的强化学习算法，是A3C最大的贡献。目前，已经有基于GPU的A3C框架，这样A3C的框架训练速度就更快了。</p>
<p>&emsp;&emsp;除了A3C，DDPG算法也可以改善Actor-Critic难收敛的问题。它使用了Nature DQN，DDQN类似的思想，用两个Actor网络，两个Critic网络，一共4个神经网络来迭代更新模型参数。</p>
<p>&nbsp;</p>
<h2 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h2><h3 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h3><h4 id="针对问题-1"><a href="#针对问题-1" class="headerlink" title="针对问题"></a>针对问题</h4><p>&emsp;&emsp;对于某一些动作集合来说，它可能是连续值，或者非常高维的离散值，这样动作的空间维度极大。如果使用随机策略，即像DQN一样研究它所有的可能动作的概率，并计算各个可能的动作的价值的话，那需要的样本量是非常大才可行的</p>
<h4 id="优化：确定性策略（DPG）"><a href="#优化：确定性策略（DPG）" class="headerlink" title="优化：确定性策略（DPG）"></a>优化：确定性策略（DPG）</h4><p>&emsp;&emsp;同一个随机策略在同一个状态处采用的动作基于一个概率分布，依概率取动作值，即不确定。而确定性策略则在不同概率的动作中取概率最大者，同一个状态的动作是唯一确定的，此时策略为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220116163325.png" alt="image-20220113100808659" style="zoom:45%;" />

<p>基于Q值的随机性策略梯度的梯度计算公式为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220113101331.png" alt="image-20220113101331271" style="zoom:45%;" />

<p>其中状态的采样空间为ρπ，可见随机性策略梯度需要在整个动作的空间πθπθ进行采样。</p>
<p>而基于Q值的确定性策略梯度的梯度计算公式是：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220113101501.png" alt="image-20220113101501314" style="zoom:45%;" />

<p>跟随机策略梯度的式子相比，少了对动作的积分，多了回报Q函数对动作的导数。</p>
<h4 id="DDPG原理"><a href="#DDPG原理" class="headerlink" title="DDPG原理"></a>DDPG原理</h4><p>&emsp;&emsp;从DPG到DDPG的过程，完全可以类比DQN到DDQN的过程。除了经验回放外还有双网络，即当前网络和目标网络。Actor双网络和Critic双网络总共四个网络结构。</p>
<h5 id="回顾DDQN的双网络模型"><a href="#回顾DDQN的双网络模型" class="headerlink" title="回顾DDQN的双网络模型"></a>回顾DDQN的双网络模型</h5><p>&emsp;&emsp;DDQN的当前网络负责对当前状态<code>S</code>使用<code>ε-greedy</code>法选择动作<code>A</code>，执行动作<code>A</code>获得新状态<code>S‘</code>和奖励<code>R</code>，将此样本放入经验池。对经验池中采样的下一状态<code>S’</code>使用贪婪<code>greedy</code>法选择动作<code>A’</code>，供目标网络计算目标Q值，然后当前网络反向传播更新参数，并定期把最新的网络参数复制到目标网络。</p>
<p>&emsp;&emsp;DDQN的目标网络基于经验回放计算目标Q值，供当前网络计算损失函数然后反向传播。</p>
<h5 id="DDPG工作原理"><a href="#DDPG工作原理" class="headerlink" title="DDPG工作原理"></a>DDPG工作原理</h5><p>&emsp;&emsp;与DDQN相比较，DDPG的Critic的当前网络/目标网络同DDQN相似，但是因为DDPG还有Actor双网络，因此DDPG用Actor当前网络代替DDQN中的<code>ε-greedy</code>法选择动作<code>A</code>、用Actor目标网络代替DDQN中的贪婪<code>greedy</code>法选择动作<code>A’</code>。</p>
<p>&emsp;&emsp;基于经验回放池和目标Actor网络提供的<code>S′</code>、<code>A′</code>计算目标Q值的一部分，这部分由于是评估，因此还是放到Critic目标网络完成。而Critic目标网络计算出目标Q值一部分后，Critic当前网络会计算目标Q值，进行网络参数的反向传播更新，并定期将当前网络参数复制到Critic目标网络。</p>
<h5 id="DDPG-4个网络的功能"><a href="#DDPG-4个网络的功能" class="headerlink" title="DDPG 4个网络的功能"></a>DDPG 4个网络的功能</h5><table>
<thead>
<tr>
<th>网络</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>Critic当前网络</td>
<td>负责价值网络参数w的更新，计算当前Q(S,A,w)，目标Q值yi=R+γQ’(S’,A’,w’)</td>
</tr>
<tr>
<td>Critic目标网络</td>
<td>计算目标Q值中的Q’(S’,A’,w’)部分，网络参数w’定期从w复制</td>
</tr>
<tr>
<td>Actor当前网络</td>
<td>负责策略网络参数θ的迭代更新，根据当前状态S选择当前动作A，与环境交互生成S‘、R</td>
</tr>
<tr>
<td>Actor目标网络</td>
<td>根据经验池中采样的下一状态S’选择最优下一动作A‘，网络参数θ’定期从θ复制</td>
</tr>
</tbody></table>
<h5 id="网络参数的复制"><a href="#网络参数的复制" class="headerlink" title="网络参数的复制"></a>网络参数的复制</h5><p>&emsp;&emsp;DDPG中网络参数的赋值并不是和DQN一样直接全部复制（硬更新），而是每次都只更新一点点（软更新）：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220113122829.png" alt="image-20220113122829564" style="zoom:45%;" />

<p>（其中，τ是更新系数，典型值为0.01~0.1）</p>
<h5 id="动作噪声"><a href="#动作噪声" class="headerlink" title="动作噪声"></a>动作噪声</h5><p>&emsp;&emsp;为了学习过程增加一些随机性，从而增加学习的覆盖，DDPG对选择出来的动作<code>A</code>会增加一定的噪声<code>N</code>，即最终和环境交互的动作<code>A</code>为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220113123143.png" alt="image-20220113123143564" style="zoom:45%;" />

<h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><h6 id="Critic当前网络：均方误差"><a href="#Critic当前网络：均方误差" class="headerlink" title="Critic当前网络：均方误差"></a>Critic当前网络：均方误差</h6><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220113123425.png" alt="image-20220113123425768" style="zoom:45%;" />

<h6 id="Actor当前网络"><a href="#Actor当前网络" class="headerlink" title="Actor当前网络"></a>Actor当前网络</h6><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220113123623.png" alt="image-20220113123623061" style="zoom:45%;" />

<p>&nbsp;</p>
<h3 id="算法流程-2"><a href="#算法流程-2" class="headerlink" title="算法流程"></a>算法流程</h3><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220113125412.png" alt="image-20220113125412297" style="zoom:50%;" />

<p>输入：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Actor当前/目标网络，Critic当前/目标网络，参数分别为θ、θ′、w、w′，衰减因子γ、软更新系数τ、批量梯度下降的样本数m、目标Q网络参数更新频率C、最大迭代次数T、随机噪音函数N</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最优Actor当前网络参数θ、Critic当前网络参数w</span><br></pre></td></tr></table></figure>

<p>流程：</p>
<ol>
<li><p>随机初始化<code>θ</code>、<code>w</code>、<code>w′=w</code>、<code>θ′=θ</code>，清空经验池 D</p>
</li>
<li><p>for i in [ 1, T ]：</p>
<ol>
<li><p>初始化<code>S</code>为当前状态序列的第一个状态，拿到其特征向量<code>Φ(S)</code></p>
</li>
<li><p>把状态<code>S</code>输入<strong>Actor当前网络</strong>得到输出动作：</p>
 <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220113125013.png" alt="image-20220113125013515" style="zoom:45%;" /></li>
<li><p>执行动作<code>A</code>，由环境得到新状态<code>S‘</code>、奖励 <code>R</code>、终止标志 <code>is_end</code></p>
</li>
<li><p>将 <code>&#123; ϕ(S)、A、R、ϕ(S′)、is_end &#125;</code> 这个五元组存入经验池 <code>D</code></p>
</li>
<li><p>前进一步：<code>S = S’</code></p>
</li>
<li><p>从经验池<code>D</code>中采样<code>m</code>个样本 <code>&#123; ϕ(Sj)、Aj、Rj、ϕ(S′j)、is_endj &#125; ，j=1,2...m</code> ， 用<strong>目标Critic网络</strong>结合公式计算当前目标Q值<code>yj</code>：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220113124619.png" alt="image-20220113124619535" style="zoom:45%;" />

<p>（注意：<code>πθ′(ϕ(S′j))</code>是通过<strong>Actor目标网络</strong>得到 ）</p>
</li>
<li><p>用<strong>当前Critic网络</strong>计算Q估计值，梯度反向传播更新Critic当前网络参数 <code>w</code>，损失函数为均方误差：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220113123425.png" alt="image-20220113123425768" style="zoom:45%;" /></li>
<li><p>梯度反向传播更新Actor当前网络参数 <code>θ</code>，损失函数为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220113123623.png" alt="image-20220113123623061" style="zoom:45%;" /></li>
<li><p>if <code>i % C == 0</code>，则更新Critic目标网络和Actor目标网络参数：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220113122829.png" alt="image-20220113122829564" style="zoom:45%;" /></li>
<li><p>if  <code>S’ </code>是终止状态，break；else 跳回步骤 2.2</p>
</li>
</ol>
</li>
</ol>
<p>（注意，上面πθ由Actor目标网络得到，Q‘由Critic目标网络得到）</p>
<p>&nbsp;</p>
<h3 id="示例-3"><a href="#示例-3" class="headerlink" title="示例"></a>示例</h3><h4 id="问题-3"><a href="#问题-3" class="headerlink" title="问题"></a>问题</h4><p>&emsp;&emsp;这里不用之前的CartPole游戏，因为它不是连续动作，而是使用Pendulum-v1游戏为例。目的是用最小的力矩使棒子竖起来，这个游戏的详细介绍参见<a target="_blank" rel="noopener" href="https://github.com/openai/gym/wiki/Pendulum-v0">这里</a>。输入状态是角度的sin，cos值，以及角速度，一共三个值。动作是一个连续的力矩值。</p>
<h4 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import tensorflow as tf</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line">tf.disable_v2_behavior()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#####################  hyper parameters  ####################</span></span><br><span class="line"></span><br><span class="line">MAX_EPISODES = <span class="number">2000</span></span><br><span class="line">MAX_EP_STEPS = <span class="number">200</span></span><br><span class="line">LR_A = <span class="number">0.001</span>    <span class="comment"># learning rate for actor</span></span><br><span class="line">LR_C = <span class="number">0.002</span>    <span class="comment"># learning rate for critic</span></span><br><span class="line">GAMMA = <span class="number">0.9</span>     <span class="comment"># reward discount</span></span><br><span class="line">TAU = <span class="number">0.01</span>      <span class="comment"># soft replacement</span></span><br><span class="line">MEMORY_CAPACITY = <span class="number">10000</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">RENDER = <span class="literal">False</span></span><br><span class="line">ENV_NAME = <span class="string">&#x27;Pendulum-v1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###############################  DDPG  ####################################</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DDPG</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, a_dim, s_dim, a_bound,</span>):</span></span><br><span class="line">        self.memory = np.zeros(</span><br><span class="line">            (MEMORY_CAPACITY, s_dim * <span class="number">2</span> + a_dim + <span class="number">1</span>), dtype=np.float32)</span><br><span class="line">        self.pointer = <span class="number">0</span></span><br><span class="line">        self.sess = tf.Session()</span><br><span class="line"></span><br><span class="line">        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound,</span><br><span class="line">        self.S = tf.placeholder(tf.float32, [<span class="literal">None</span>, s_dim], <span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">        self.S_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, s_dim], <span class="string">&#x27;s_&#x27;</span>)</span><br><span class="line">        self.R = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>], <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Actor&#x27;</span>):</span><br><span class="line">            self.a = self._build_a(self.S, scope=<span class="string">&#x27;eval&#x27;</span>, trainable=<span class="literal">True</span>)</span><br><span class="line">            a_ = self._build_a(self.S_, scope=<span class="string">&#x27;target&#x27;</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Critic&#x27;</span>):</span><br><span class="line">            <span class="comment"># assign self.a = a in memory when calculating q for td_error,</span></span><br><span class="line">            <span class="comment"># otherwise the self.a is from Actor when updating Actor</span></span><br><span class="line">            q = self._build_c(self.S, self.a, scope=<span class="string">&#x27;eval&#x27;</span>, trainable=<span class="literal">True</span>)</span><br><span class="line">            q_ = self._build_c(self.S_, a_, scope=<span class="string">&#x27;target&#x27;</span>, trainable=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># networks parameters</span></span><br><span class="line">        self.ae_params = tf.get_collection(</span><br><span class="line">            tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="string">&#x27;Actor/eval&#x27;</span>)</span><br><span class="line">        self.at_params = tf.get_collection(</span><br><span class="line">            tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="string">&#x27;Actor/target&#x27;</span>)</span><br><span class="line">        self.ce_params = tf.get_collection(</span><br><span class="line">            tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="string">&#x27;Critic/eval&#x27;</span>)</span><br><span class="line">        self.ct_params = tf.get_collection(</span><br><span class="line">            tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="string">&#x27;Critic/target&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># target net replacement</span></span><br><span class="line">        self.soft_replace = [tf.assign(t, (<span class="number">1</span> - TAU) * t + TAU * e)</span><br><span class="line">                             <span class="keyword">for</span> t, e <span class="keyword">in</span> <span class="built_in">zip</span>(self.at_params + self.ct_params, self.ae_params + self.ce_params)]</span><br><span class="line"></span><br><span class="line">        q_target = self.R + GAMMA * q_</span><br><span class="line">        <span class="comment"># in the feed_dic for the td_error, the self.a should change to actions in memory</span></span><br><span class="line">        td_error = tf.losses.mean_squared_error(labels=q_target, predictions=q)</span><br><span class="line">        self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(</span><br><span class="line">            td_error, var_list=self.ce_params)</span><br><span class="line"></span><br><span class="line">        a_loss = - tf.reduce_mean(q)    <span class="comment"># maximize the q</span></span><br><span class="line">        self.atrain = tf.train.AdamOptimizer(</span><br><span class="line">            LR_A).minimize(a_loss, var_list=self.ae_params)</span><br><span class="line"></span><br><span class="line">        self.sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, s</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.sess.run(self.a, &#123;self.S: s[np.newaxis, :]&#125;)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># soft target replacement</span></span><br><span class="line">        self.sess.run(self.soft_replace)</span><br><span class="line"></span><br><span class="line">        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)</span><br><span class="line">        bt = self.memory[indices, :]</span><br><span class="line">        bs = bt[:, :self.s_dim]</span><br><span class="line">        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]</span><br><span class="line">        br = bt[:, -self.s_dim - <span class="number">1</span>: -self.s_dim]</span><br><span class="line">        bs_ = bt[:, -self.s_dim:]</span><br><span class="line"></span><br><span class="line">        self.sess.run(self.atrain, &#123;self.S: bs&#125;)</span><br><span class="line">        self.sess.run(self.ctrain, &#123;self.S: bs,</span><br><span class="line">                      self.a: ba, self.R: br, self.S_: bs_&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        transition = np.hstack((s, a, [r], s_))</span><br><span class="line">        <span class="comment"># replace the old memory with new memory</span></span><br><span class="line">        index = self.pointer % MEMORY_CAPACITY</span><br><span class="line">        self.memory[index, :] = transition</span><br><span class="line">        self.pointer += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_a</span>(<span class="params">self, s, scope, trainable</span>):</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            net = tf.layers.dense(</span><br><span class="line">                s, <span class="number">30</span>, activation=tf.nn.relu, name=<span class="string">&#x27;l1&#x27;</span>, trainable=trainable)</span><br><span class="line">            a = tf.layers.dense(</span><br><span class="line">                net, self.a_dim, activation=tf.nn.tanh, name=<span class="string">&#x27;a&#x27;</span>, trainable=trainable)</span><br><span class="line">            <span class="keyword">return</span> tf.multiply(a, self.a_bound, name=<span class="string">&#x27;scaled_a&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_c</span>(<span class="params">self, s, a, scope, trainable</span>):</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">            n_l1 = <span class="number">30</span></span><br><span class="line">            w1_s = tf.get_variable(</span><br><span class="line">                <span class="string">&#x27;w1_s&#x27;</span>, [self.s_dim, n_l1], trainable=trainable)</span><br><span class="line">            w1_a = tf.get_variable(</span><br><span class="line">                <span class="string">&#x27;w1_a&#x27;</span>, [self.a_dim, n_l1], trainable=trainable)</span><br><span class="line">            b1 = tf.get_variable(<span class="string">&#x27;b1&#x27;</span>, [<span class="number">1</span>, n_l1], trainable=trainable)</span><br><span class="line">            net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)</span><br><span class="line">            <span class="keyword">return</span> tf.layers.dense(net, <span class="number">1</span>, trainable=trainable)  <span class="comment"># Q(s,a)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###############################  training  ####################################</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">env = gym.make(ENV_NAME)</span><br><span class="line">env = env.unwrapped</span><br><span class="line">env.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">s_dim = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line">a_dim = env.action_space.shape[<span class="number">0</span>]</span><br><span class="line">a_bound = env.action_space.high</span><br><span class="line"></span><br><span class="line">ddpg = DDPG(a_dim, s_dim, a_bound)</span><br><span class="line"></span><br><span class="line">var = <span class="number">3</span>  <span class="comment"># control exploration</span></span><br><span class="line">t1 = time.time()</span><br><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(MAX_EPISODES):</span><br><span class="line">    s = env.reset()</span><br><span class="line">    ep_reward = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(MAX_EP_STEPS):</span><br><span class="line">        <span class="keyword">if</span> RENDER:</span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add exploration noise</span></span><br><span class="line">        a = ddpg.choose_action(s)</span><br><span class="line">        <span class="comment"># add randomness to action selection for exploration</span></span><br><span class="line">        a = np.clip(np.random.normal(a, var), -<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        s_, r, done, info = env.step(a)</span><br><span class="line"></span><br><span class="line">        ddpg.store_transition(s, a, r / <span class="number">10</span>, s_)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ddpg.pointer &gt; MEMORY_CAPACITY:</span><br><span class="line">            var *= <span class="number">.9995</span>    <span class="comment"># decay the action randomness</span></span><br><span class="line">            ddpg.learn()</span><br><span class="line"></span><br><span class="line">        s = s_</span><br><span class="line">        ep_reward += r</span><br><span class="line">        <span class="keyword">if</span> j == MAX_EP_STEPS-<span class="number">1</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Episode:&#x27;</span>, episode, <span class="string">&#x27; Reward: %i&#x27;</span> %</span><br><span class="line">                  <span class="built_in">int</span>(ep_reward), <span class="string">&#x27;Explore: %.2f&#x27;</span> % var, )</span><br><span class="line">            <span class="comment"># if ep_reward &gt; -300:RENDER = True</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> episode % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        total_reward = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            state = env.reset()</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(MAX_EP_STEPS):</span><br><span class="line">                env.render()</span><br><span class="line">                action = ddpg.choose_action(state)  <span class="comment"># direct action for test</span></span><br><span class="line">                state, reward, done, _ = env.step(action)</span><br><span class="line">                total_reward += reward</span><br><span class="line">                <span class="keyword">if</span> done:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        ave_reward = total_reward/<span class="number">300</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;episode: &#x27;</span>, episode, <span class="string">&#x27;Evaluation Average Reward:&#x27;</span>, ave_reward)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Running time: &#x27;</span>, time.time() - t1)</span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h3 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h3><p>&emsp;&emsp;DDPG参考了DDQN的算法思想，通过双网络和经验回放，加一些其他的优化，比较好的解决了Actor-Critic难收敛的问题。因此在实际产品中尤其是自动化相关的产品中用的比较多，是一个比较成熟的Actor-Critic算法。</p>
<p>&emsp;&emsp;以上就是Policy Based RL系列算法，算上前面的Value Based RL系列算法，还剩下Model Based RL系列算法没研究，后面再说。</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Lrk612
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://sharp-rookie.github.io/2022/01/16/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%EF%BC%9APG%E7%AE%97%E6%B3%95%E6%97%8F%20&%20A3C/" title="【强化学习】策略梯度：PG算法族 &amp; A3C">https://sharp-rookie.github.io/2022/01/16/【强化学习】策略梯度：PG算法族 & A3C/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
              <a href="/tags/Algorithm/" rel="tag"># Algorithm</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/01/16/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9ADQN%E7%AE%97%E6%B3%95%E6%97%8F/" rel="prev" title="【强化学习】深度强化学习：DQN算法族">
      <i class="fa fa-chevron-left"></i> 【强化学习】深度强化学习：DQN算法族
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/01/17/%E3%80%90%E6%8A%80%E6%9C%AF%E3%80%91%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/" rel="next" title="进程与线程">
      进程与线程 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%EF%BC%88Policy-Gradient%EF%BC%89"><span class="nav-text">策略梯度（Policy-Gradient）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Value-based-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%9A%84%E4%B8%8D%E8%B6%B3"><span class="nav-text">Value-based 强化学习方法的不足</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Policy-based-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%9A%84%E5%BC%95%E5%85%A5"><span class="nav-text">Policy-based 强化学习方法的引入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="nav-text">策略梯度的优化目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E5%87%BD%E6%95%B0%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="nav-text">策略函数的设计</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Softmax%E7%AD%96%E7%95%A5%E5%87%BD%E6%95%B0"><span class="nav-text">Softmax策略函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gauss%E7%AD%96%E7%95%A5%E5%87%BD%E6%95%B0"><span class="nav-text">Gauss策略函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="nav-text">蒙特卡罗策略梯度算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="nav-text">算法流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%AE%E9%A2%98"><span class="nav-text">问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-text">代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%88%E6%9E%9C"><span class="nav-text">效果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Actor-Critic%E7%AE%97%E6%B3%95"><span class="nav-text">Actor-Critic算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0"><span class="nav-text">概述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E5%87%BD%E6%95%B0%E7%9A%84%E8%BF%91%E4%BC%BC"><span class="nav-text">策略函数的近似</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E8%BF%91%E4%BC%BC"><span class="nav-text">价值函数的近似</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%AF%E9%80%89%E5%BD%A2%E5%BC%8F"><span class="nav-text">可选形式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B%EF%BC%88%E5%9F%BA%E4%BA%8ETD%E8%AF%AF%E5%B7%AE%EF%BC%89"><span class="nav-text">算法流程（基于TD误差）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B-1"><span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%AE%E9%A2%98-1"><span class="nav-text">问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-1"><span class="nav-text">代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%88%E6%9E%9C-1"><span class="nav-text">效果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93-1"><span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A3C"><span class="nav-text">A3C</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0-1"><span class="nav-text">概述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%92%88%E5%AF%B9%E9%97%AE%E9%A2%98"><span class="nav-text">针对问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96"><span class="nav-text">优化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BC%82%E6%AD%A5%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6"><span class="nav-text">异步训练框架</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E4%BC%98%E5%8C%96"><span class="nav-text">网络结构优化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Critic%E9%80%89%E6%8B%A9%E7%9A%84%E4%BC%98%E5%8C%96"><span class="nav-text">Critic选择的优化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%BC%98%E5%8C%96"><span class="nav-text">损失函数优化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B-1"><span class="nav-text">算法流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B-2"><span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%AE%E9%A2%98-2"><span class="nav-text">问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-2"><span class="nav-text">代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%88%E6%9E%9C-2"><span class="nav-text">效果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93-2"><span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DDPG"><span class="nav-text">DDPG</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0-2"><span class="nav-text">概述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%92%88%E5%AF%B9%E9%97%AE%E9%A2%98-1"><span class="nav-text">针对问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%EF%BC%9A%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%EF%BC%88DPG%EF%BC%89"><span class="nav-text">优化：确定性策略（DPG）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DDPG%E5%8E%9F%E7%90%86"><span class="nav-text">DDPG原理</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%9E%E9%A1%BEDDQN%E7%9A%84%E5%8F%8C%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="nav-text">回顾DDQN的双网络模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DDPG%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="nav-text">DDPG工作原理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DDPG-4%E4%B8%AA%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8A%9F%E8%83%BD"><span class="nav-text">DDPG 4个网络的功能</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E7%9A%84%E5%A4%8D%E5%88%B6"><span class="nav-text">网络参数的复制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8A%A8%E4%BD%9C%E5%99%AA%E5%A3%B0"><span class="nav-text">动作噪声</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Critic%E5%BD%93%E5%89%8D%E7%BD%91%E7%BB%9C%EF%BC%9A%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE"><span class="nav-text">Critic当前网络：均方误差</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Actor%E5%BD%93%E5%89%8D%E7%BD%91%E7%BB%9C"><span class="nav-text">Actor当前网络</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B-2"><span class="nav-text">算法流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B-3"><span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%AE%E9%A2%98-3"><span class="nav-text">问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-3"><span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93-3"><span class="nav-text">小结</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Lrk612"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Lrk612</p>
  <div class="site-description" itemprop="description">Lrk's blog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.csdn.net/" title="https:&#x2F;&#x2F;www.csdn.net&#x2F;" rel="noopener" target="_blank">CSDN</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.google.com/" title="https:&#x2F;&#x2F;www.google.com" rel="noopener" target="_blank">Google</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <!-- 访问量 -->

    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


<!-- 版权 -->
<div class="copyright">
  
  &copy; 2021-12-1 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lrk612</span>
</div>

<!-- ICP备案 -->
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP备案 </a>
      <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211209170739.png" style="display: inline-block;"><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=2021035798" rel="noopener" target="_blank">豫ICP备2021035798号 </a>
  </div>

<!-- 不知道是什么 -->
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'fH6OdGBcCG8D6jC8NeeJlX9b-gzGzoHsz',
      appKey     : '1UoIb6vszMw6wl0n7kreb4Xz',
      placeholder: "Just go go ^_^",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
    var infoEle = document.querySelector('#comments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
      infoEle.childNodes.forEach(function(item) {
        item.parentNode.removeChild(item);
      });
    }
  }, window.Valine);
});
</script>

</body>
</html>
