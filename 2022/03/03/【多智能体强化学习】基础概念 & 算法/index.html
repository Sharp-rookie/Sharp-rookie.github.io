<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sharp-rookie.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

<script> 
   (function(){
          if(''){
              if (prompt('请输入文章密码') !== ''){
                  alert('密码错误！');
                  history.back();
              }
          }
      })();
  </script>
  <meta name="description" content="&amp;emsp;&amp;emsp;因为项目中遇到了多个体共同决策的场景，不同个体的离散动作空间和评价指标都不同，在使用DQN时发现随着个体数量的增加，动作空间会指数式扩张，同时奖励值的设计也很难兼顾所有需求。所以一方面尝试使用策略优化的DRL算法解决离散动作空间维度爆炸问题，另一方面考虑使用多智能体强化学习的算法。 &amp;emsp;&amp;emsp;单智能体强化学习算法有Sutton的神作《Reinforcement">
<meta property="og:type" content="article">
<meta property="og:title" content="【多智能体强化学习】基础概念 &amp; 算法">
<meta property="og:url" content="https://sharp-rookie.github.io/2022/03/03/%E3%80%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%20&%20%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="最忆是江南">
<meta property="og:description" content="&amp;emsp;&amp;emsp;因为项目中遇到了多个体共同决策的场景，不同个体的离散动作空间和评价指标都不同，在使用DQN时发现随着个体数量的增加，动作空间会指数式扩张，同时奖励值的设计也很难兼顾所有需求。所以一方面尝试使用策略优化的DRL算法解决离散动作空间维度爆炸问题，另一方面考虑使用多智能体强化学习的算法。 &amp;emsp;&amp;emsp;单智能体强化学习算法有Sutton的神作《Reinforcement">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303221852.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303193953.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303195023.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303200139.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303205335.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303210048.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303210148.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211027.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211303.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211337.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211551.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211615.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220110.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220150.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220311.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220355.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220829.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303213014.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303213350.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303213453.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303214618.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303215135.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303215312.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220304104127.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220304142823.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220304142844.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220304140123.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220304151241.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220304151327.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220304154558.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306162943.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306162849.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306163818.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306164128.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306165743.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306170459.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306170816.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306171046.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306171521.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306205629.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306205844.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306205936.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306210928.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306211805.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306211823.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306211941.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306212139.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306212411.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306213205.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306213321.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306213930.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220306214020.png">
<meta property="article:published_time" content="2022-03-03T14:23:42.444Z">
<meta property="article:modified_time" content="2022-03-07T03:13:14.181Z">
<meta property="article:author" content="Lrk612">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Algorithm">
<meta property="article:tag" content="Reinforcement Learning">
<meta property="article:tag" content="Mutli-Agent RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303221852.png">

<link rel="canonical" href="https://sharp-rookie.github.io/2022/03/03/%E3%80%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%20&%20%E7%AE%97%E6%B3%95/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>【多智能体强化学习】基础概念 & 算法 | 最忆是江南</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
        <a target="_blank" rel="noopener" href="https://github.com/Sharp-rookie" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">最忆是江南</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Love actually is all around</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th-list fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-friendlink">

    <a href="/friendlink/" rel="section"><i class="fa fa-link fa-fw"></i>友链</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-download fa-fw"></i>资源</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sharp-rookie.github.io/2022/03/03/%E3%80%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%20&%20%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Lrk612">
      <meta itemprop="description" content="Lrk's blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="最忆是江南">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【多智能体强化学习】基础概念 & 算法
        </h1>

        <div class="post-meta">
         
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-03 22:23:42" itemprop="dateCreated datePublished" datetime="2022-03-03T22:23:42+08:00">2022-03-03</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/" itemprop="url" rel="index"><span itemprop="name">知识分享</span></a>
                </span>
            </span>

          
            <span id="/2022/03/03/%E3%80%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%20&%20%E7%AE%97%E6%B3%95/" class="post-meta-item leancloud_visitors" data-flag-title="【多智能体强化学习】基础概念 & 算法" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/03/03/%E3%80%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%20&%20%E7%AE%97%E6%B3%95/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/03/03/%E3%80%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%20&%20%E7%AE%97%E6%B3%95/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>&emsp;&emsp;因为项目中遇到了多个体共同决策的场景，不同个体的离散动作空间和评价指标都不同，在使用DQN时发现随着个体数量的增加，动作空间会指数式扩张，同时奖励值的设计也很难兼顾所有需求。所以一方面尝试使用策略优化的DRL算法解决离散动作空间维度爆炸问题，另一方面考虑使用多智能体强化学习的算法。</p>
<p>&emsp;&emsp;单智能体强化学习算法有Sutton的神作《Reinforcement Learning: An introduction》，但是多智能体强化学习方面就没什么系统的网课或者书籍，因此算是比较前沿的研究领域。多智能体强化学习专注于实现具有多个智能体的自主、自学习系统的领域，其中涉及大量博弈论概念，值得一学。</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303221852.png" alt="image-20220303221852425" style="zoom:28%;" />

<span id="more"></span>

<p>&nbsp;</p>
<h1 id="基础知识与博弈论"><a href="#基础知识与博弈论" class="headerlink" title="基础知识与博弈论"></a>基础知识与博弈论</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在多智能体（Mutli-Agent）系统中，每个智能体通过与环境进行交互获得奖励值来学习改善自己的策略，从而获得该环境下最优策略的过程，就是多智能体强化学习</p>
<p>单智能体强化学习中，智能体所在的环境是稳定不变的，但是如果用单智能体来学习多个体的策略，将会产生许多困难：</p>
<ul>
<li><p>维度爆炸</p>
<p>  如果用单智能体强化学习来解决多个体问题，那么多个个体的联结动作空间（A=[a1, a2, a3,……,an]）会随着个体数量指数式增长，系统维度爆炸（但这一点可以用策略优化的算法来解决）</p>
</li>
<li><p>目标奖励难确定</p>
<p>  多个体系统中，因为每个个体评价好坏的指标不同，并且彼此之间又相互耦合影响，奖励设计的优劣会直接影响学习到的策略的收敛性和效果，通常难以设计</p>
</li>
<li><p>不稳定性</p>
<p>  多个体系统中，多个个体的策略互相影响。当同伴的策略改变时，每个个体自身的最优策略也可能会变化，这将使得单智能体难以兼顾所有个体的性能，算法的收敛性难以保证</p>
</li>
<li><p>收敛慢</p>
<p>  不光要考虑单个个体对环境的探索，所有个体的策略变化都要进行探索。每个个体策略的变化都可能打破其他个体策略的平衡状态，这将使算法很难稳定，学习速度慢</p>
</li>
</ul>
<p>因此，用单智能体解决多个体决策问题是一个很大的挑战，由此引入多智能体强化学习，为每个个体维护一个做决策的智能体。智能体之间存在合作与竞争，最终实现最大共同利益或者规定的利益，基于此，引入博弈的概念，将博弈论与强化学习相结合从而更好地处理这些问题</p>
<p>&nbsp;</p>
<h2 id="博弈论基础"><a href="#博弈论基础" class="headerlink" title="博弈论基础"></a>博弈论基础</h2><h3 id="矩阵博弈"><a href="#矩阵博弈" class="headerlink" title="矩阵博弈"></a>矩阵博弈</h3><p>矩阵博弈可以表示为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303193953.png" alt="image-20220303193953815" style="zoom:40%;" />

<p>其中 <code>n</code> 表示智能体数量，<code>A </code>为动作，<code>Ri：A1×A2×……×An——&gt;R</code> 表示第i个智能体的奖励函数，可见奖励函数与每个智能体的联结动作有关，联结动作空间为：<code>A1×A2×……×An</code>（其中每个A都是动作空间而不是单个动作）</p>
<p>每个智能体的策略是一个关于其动作空间的概率分布，每个智能体的目标是最大化其在联结策略 <code>(Π1, Π2,……,Πn)</code>下的奖励值：<code>Vi(Π1, Π2,……,Πn)</code></p>
<h4 id="纳什均衡"><a href="#纳什均衡" class="headerlink" title="纳什均衡"></a>纳什均衡</h4><p>在矩阵博弈中，如果联结策略 <code>(Π1*, Π2*,……,Πn*)</code> 满足：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303195023.png" alt="image-20220303195023004" style="zoom:70%;" />

<center>式 1</center>

<p>则称为一个纳什均衡。也就是说，纳什均衡是一个所有智能体的联结策略，在纳什均衡处，对于所有智能体而言，都不能通过仅改变自身策略的情况下获得更大的奖励值</p>
<p>设联结动作 <code>(a1,a2,……,an)</code> 的期望奖励为 <code>Q(a1,a2,……,an)</code>，令 <code>Πi(ai)</code> 表示第i个智能体选取动作 <code>ai</code></p>
<p>的概率。则纳什均衡还可定义为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303200139.png" alt="image-20220303200139101" style="zoom:60%;" />

<center>式 2</center>



<h4 id="严格纳什均衡"><a href="#严格纳什均衡" class="headerlink" title="严格纳什均衡"></a>严格纳什均衡</h4><p>公式1中取严格大于时，联结策略 <code>(Π1*, Π2*,……,Πn*)</code> 即为严格纳什均衡</p>
<h4 id="完全混合策略"><a href="#完全混合策略" class="headerlink" title="完全混合策略"></a>完全混合策略</h4><p>若一个策略对于智能体动作集中的所有动作的概率都大于0，则这个策略为一个完全混合策略</p>
<h4 id="纯策略"><a href="#纯策略" class="headerlink" title="纯策略"></a>纯策略</h4><p>若智能体的策略对一个动作的概率分布为1，对其余的动作的概率分布为0，则这个策略为一个纯策略</p>
<h3 id="两个智能体的纳什均衡"><a href="#两个智能体的纳什均衡" class="headerlink" title="两个智能体的纳什均衡"></a>两个智能体的纳什均衡</h3><p>两智能体的博弈问题是大部分多智能体强化学习算法的基础。双智能体矩阵博弈之于多智能体强化学习就像感知机之于神经网络</p>
<p>在双智能体矩阵博弈中，我们可以设计一个矩阵，矩阵每一个元素的索引坐标表示一个联结动作 <code>[A1=x, A2=y]</code>  ，第 <code>i</code> 个智能体的奖励矩阵 <code>Ri</code> 的元素 <code>rxy</code> 就表示第一个智能体采用动作 <code>x</code>，第二个智能体采用动作 <code>y</code> 时第 <code>i</code> 个智能体获得的奖励，<code>cxy</code> 也同理。通常我们将第一个智能体定义为<strong>行智能体</strong>，第二个智能体定义为<strong>列智能体</strong>，行号表示第一个智能体选取的动作，列号表示第二个智能体选取的动作。则对于只有2个动作的智能体，其奖励矩阵分别可以写为:</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303205335.png" alt="image-20220303205335878" style="zoom:70%;" />



<h4 id="零和博弈"><a href="#零和博弈" class="headerlink" title="零和博弈"></a>零和博弈</h4><p>零和博弈中，两个智能体是完全竞争对抗关系，即 <code>R1=-R2</code> 。在零和博弈中只有一个纳什均衡值，即使可能有很多纳什均衡策略，但是期望的奖励是相同的</p>
<h4 id="一般和博弈"><a href="#一般和博弈" class="headerlink" title="一般和博弈"></a>一般和博弈</h4><p>一般和博弈是指任何类型的矩阵博弈，包括完全对抗博弈、完全合作博弈以及二者的混合博弈。在一般和博弈中可能存在多个纳什均衡点</p>
<p>定义策略 <code>Πi=(Πi(a1),…,Πi(ami))</code> 为智能体 i 的动作集中每个动作的概率集合，<code>mi</code> 为可选的动作数量，则值函数 <code>Vi</code> 可表示为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303210048.png" alt="image-20220303210048275" style="zoom:72%;" />

<p>纳什均衡策略 <code>(Π1*, Π2*)</code> 可表示为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303210148.png" alt="image-20220303210148099" style="zoom:70%;" />

<p>其中 <code>PD(Ai)</code> 表示第 <code>i</code> 个智能体的策略空间，<code>-i</code> 表示另一个智能体</p>
<p>若满足：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211027.png" alt="image-20220303211027797" style="zoom:75%;" />

<p>则称 <code>(l,f)</code> 满足纯策略严格纳什均衡，<code>-l</code>、<code>-f</code> 表示除了 <code>l</code>、<code>f</code> 的另一个策略</p>
<h3 id="线性规划求解双智能体的零和博弈"><a href="#线性规划求解双智能体的零和博弈" class="headerlink" title="线性规划求解双智能体的零和博弈"></a>线性规划求解双智能体的零和博弈</h3><p>求解双智能体零和博弈的思路是最大化每个智能体在与对手博弈中最差情况下的期望奖励值：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211303.png" alt="image-20220303211303527" style="zoom:70%;" />

<p>博弈形式为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211337.png" alt="image-20220303211337168" style="zoom:70%;" />

<p>定义 <code>pj(j=1,2)</code> 为第一个智能体选择动作 <code>J</code>  的概率，<code>qj(j=1,2)</code> 为第二个智能体选择动作 <code>j</code> 的概率。则可对两个智能体列写如下线性规划：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211551.png" alt="image-20220303211551299" style="zoom:70%;" />

<center>智能体1</center>

<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211615.png" alt="image-20220303211615167" style="zoom:70%;" />

<center>智能体2</center>

<p>求解即得纳什均衡策略</p>
<h3 id="几个博弈概念"><a href="#几个博弈概念" class="headerlink" title="几个博弈概念"></a>几个博弈概念</h3><table>
<thead>
<tr>
<th>概念</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>随机博弈</td>
<td>马尔可夫决策过程包含一个智能体与多个状态，矩阵博弈包括多个智能体与一个状态。随即博弈 (stochastic game / Markov game)是马尔可夫决策过程与矩阵博弈的结合，具有多个智能体与多个状态，即多智能体强化学习</td>
</tr>
<tr>
<td>静态博弈</td>
<td>静态博弈 (static/stateless game) 是指没有状态s，不存在动力学使状态能够转移的博弈。例如一个矩阵博弈</td>
</tr>
<tr>
<td>阶段博弈</td>
<td>阶段博弈 (stage game) 是随机博弈的组成成分，状态s是固定的，相当于一个状态固定的静态博弈，随机博弈中的Q值函数就是该阶段博弈的奖励函数。若干状态的阶段博弈组成一个随机博弈</td>
</tr>
<tr>
<td>重复博弈</td>
<td>智能体重复访问同一个状态的阶段博弈，并且在访问同一个状态的阶段博弈的过程中收集其他智能体的信息与奖励值，并学习更好的Q值函数与策略</td>
</tr>
</tbody></table>
<p><strong>随机博弈示例</strong></p>
<p>定义一个2*2的网格博弈，两个智能体分别表示为 P1、P2 ，P1的初始位置在左下角，P2的初始位置在右上角，每一个智能体都想以最快的方式达到 G 标志的地方。从初始位置开始，每个智能体都有两个动作可以选择。只要有一个智能体达到 G 则游戏结束，达到 G 的智能体获得奖励10，奖励折扣率为0.9。虚线表示栏杆，智能体穿过栏杆的概率为0.5</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220110.png" alt="image-20220303220110825" style="zoom:55%;" />

<p>该随机博弈一共包含7个状态，纳什均衡策略是：每个智能体到达邻居位置而不穿过栏杆</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220150.png" alt="image-20220303220150599" style="zoom:40%;" />

<p>根据上面公式，可得状态值函数：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220311.png" alt="image-20220303220311352" style="zoom:70%;" />

<p>由此可得动作状态值函数：</p>
<p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220355.png" alt="image-20220303220355121"></p>
<p>求解下图中的矩阵博弈即可得到多智能体强化学习的策略：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220829.png" alt="image-20220303220829821" style="zoom:50%;" />

<p>&nbsp;</p>
<h2 id="多智能体强化学习"><a href="#多智能体强化学习" class="headerlink" title="多智能体强化学习"></a>多智能体强化学习</h2><blockquote>
<p>多智能体强化学习就是一个随机博弈，将每一个状态的阶段博弈的纳什策略组合起来成为一个智能体在动态环境中的策略，并不断与环境交互来更新每一个状态的阶段博弈中的Q值函数（博弈奖励）</p>
</blockquote>
<h3 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h3><p>记随机博弈为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303213014.png" alt="image-20220303213014817" style="zoom:70%;" />

<p>其中 <code>n</code> 表示智能体数量，<code>S</code> 表示状态空间，<code>Ai</code> 表示第 <code>i</code> 个智能体的动作空间（不是单个动作），<code>Tr</code> 为状态转移概率：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303213350.png" alt="image-20220303213350704" style="zoom:70%;" />

<p>奖励 <code>R</code> 定义为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303213453.png" alt="image-20220303213453284" style="zoom:70%;" />

<p>即第 <code>i</code> 个智能体在当前状态与联结动作下获得的回报值， <code>γ</code> 表示累积奖励折扣系数</p>
<p>随机博弈也具有马尔科夫性，下一个状态与奖励只与当前状态与当前的联结动作有关</p>
<h3 id="学习过程"><a href="#学习过程" class="headerlink" title="学习过程"></a>学习过程</h3><p>多智能体强化学习过程，就是<strong>找到每一个状态的纳什均衡策略</strong>，然后将这些策略联合起来。<code>Πi: S——&gt;Ai</code>  就是一个智能体i的策略，在每个状态选出最优的纳什策略。</p>
<p>多智能体强化学习最优策略（随机博弈的纳什均衡策略）可以写为  <code>(Π1*, Π2*,……,Πn*)</code>  ，且对 <code>任意s∈S,i=1,……,n</code> 满足：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303214618.png" alt="image-20220303214618925" style="zoom:70%;" />

<p>不等式左边为 <code>γ</code> 折扣累积状态值函数，用 <code>Vi*(s)</code> 简记上式，用 <code>Qi*(a1,a2,……,an)</code> 表示动作状态 <code>γ</code> 折扣累积函数，在每个固定状态s的阶段博弈中，利用 <code>Qi*</code> 作为博弈的奖励求解纳什均衡策略，根据贝尔曼方程可得：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303215135.png" alt="image-20220303215135802" style="zoom:70%;" />

<p>多智能体强化学习的纳什策略可改写为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303215312.png" alt="image-20220303215312114" style="zoom:60%;" />

<p>根据每个智能体的奖励函数可以对随机博弈进行分类。若智能体的奖励函数相同，则称为完全合作博弈或团队博弈。若智能体的奖励函数逆号，则称为完全竞争博弈或零和博弈。为了求解随机博弈，需要求解每个状态s的阶段博弈，每个阶段博弈的奖励值就是 <code>Qi(s,•)</code></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h1 id="基础算法"><a href="#基础算法" class="headerlink" title="基础算法"></a>基础算法</h1><p>多智能体强化学习过程基本上是求解一个随机博弈问题，但其实这两个概念还不能完全等价。因为随机博弈中假定每个状态的奖励矩阵是已知的，不需要学习；而多智能体强化学习则是通过与环境的不断交互来学习每个状态的奖励矩阵，再通过这些奖励矩阵来学习得到最优纳什策略。</p>
<p>也就是说，多智能体强化学习和基于价值优化单智能体学习的一样，都需要评估出准确的状态价值，然后根据价值进行决策。只不过单智能体拿到价值后直接采用贪婪或者 ε-greedy 这样的策略即可，但是多智能体没有现成的策略直接用，需要根据价值矩阵推算出一个纳什均衡策略来用（类似于DDPG，估计价值、设计策略）</p>
<p>多智能体强化学习算法中，两个主要的技术指标分别为合理性与收敛性：</p>
<ul>
<li><p>合理性（rationality）</p>
<p>  在对手使用一个恒定策略的情况下，当前智能体能够学习并收敛到一个相对于对手策略的最优策略</p>
</li>
<li><p>收敛性（convergence）</p>
<p>  在其他智能体也使用学习算法时，当前智能体能够学习并收敛到一个稳定的策略</p>
<p>  通常情况下，收敛性针对系统中的所有的智能体使用相同的学习算法</p>
</li>
</ul>
<p>针对应用来分，多智能体强化学习算法可分为<strong>零和博弈算法</strong>与<strong>一般和博弈算法</strong></p>
<h2 id="MiniMax-Q"><a href="#MiniMax-Q" class="headerlink" title="MiniMax-Q"></a>MiniMax-Q</h2><blockquote>
<p>用于<strong>零和随机博弈</strong>，即两个智能体为完全竞争对抗关系 <code>R1=-R2</code> </p>
<ul>
<li><strong>Q</strong> 即指利用 Q-Learning 中的TD方法来迭代学习状态值函数或动作-状态值函数</li>
<li><strong>MiniMax</strong> 即指上一节中利用线性规划求解每个特定状态 s 的阶段博弈的纳什均衡策略</li>
</ul>
</blockquote>
<p><strong>算法思路</strong></p>
<p>在两玩家零和随机博弈中，给定状态 s，则定义第 i 个智能体的状态值函数为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220304104127.png" alt="image-20220304104127171" style="zoom:65%;" />

<p><code>Qi*(s,a_i,a_-i)</code> 是联结动作状态价值函数，若已知，则可以直接用线性规划求解出状态 s 处的纳什均衡策略。但是实际上它一开始肯定是未知的，所以需要用 Q-Learning 或其他方法来更新推算<code>Qi*(s,a_i,a_-i)</code> ，类似于单智能体强化学习中的基于价值的求解思想，一方面评估价值要准确，另一方面根据评估得到的价值矩阵推算纳什均衡策略</p>
<p><strong>算法流程</strong></p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220304142823.png" alt="image-20220304142823181" style="zoom:45%;" />

<p>理想情况，如果算法能够对每一个状态-动作对访问无限次，那么该算法能够收敛到纳什均衡策略</p>
<p><strong>算法缺点</strong></p>
<ol>
<li><p>在第5步中需要不断求解一个线性规划，降低了学习速度，增加计算时间</p>
</li>
<li><p>为了求解第5步，智能体 i 需要知道所有智能体的动作空间，这个在分布式系统中将无法满足</p>
</li>
<li><p>只满足收敛性，不满足合理性</p>
<p> Minimax-Q 算法能够找到多智能体强化学习的纳什均衡策略，但是假设对手使用的不是纳什均衡策略，而是一个较差的策略，则当前智能体并不能根据对手的策略学习到一个更优的策略。该算法无法让智能体根据对手的策略来调节优化自己的策略，而只能找到随机博弈的纳什均衡策略。这是由于Minimax-Q算法是一个对手独立算法（opponent-independent algorithm），不论对手策略是怎么样的，都收敛到该博弈的纳什均衡策略，所以如果对方不讲武德乱出牌，Minimax-Q效果就不好了</p>
</li>
</ol>
<p>&nbsp;</p>
<h2 id="Nash-Q"><a href="#Nash-Q" class="headerlink" title="Nash Q"></a>Nash Q</h2><blockquote>
<p>将Minimax-Q算法从零和博弈扩展到了<strong>多人一般和博弈</strong></p>
<ul>
<li><strong>Q</strong> 即指利用 Q-Learning 中的TD方法来迭代学习状态值函数或动作-状态值函数</li>
<li><strong>Nash</strong> 没有特殊含义，Nash Q 使用二次规划求解纳什均衡点</li>
</ul>
</blockquote>
<p>Nash Q-Learning 算法在合作性均衡或对抗性均衡的环境中能够收敛到纳什均衡点，其收敛性条件是，在每一个状态s的阶段博弈中，都能够找到一个全局最优点或者鞍点，只有满足这个条件，Nash Q-Learning 算法才能够收敛。但同样因为每轮都要求解二次规划，非常耗时，降低了算法的学习速度</p>
<p><strong>算法流程</strong></p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220304142844.png" alt="image-20220304142844901" style="zoom:45%;" />



<p>有一种利用 Minimax-Q 算法进行多人博弈方法为：两队零和博弈，将所有智能体分成两个小组进行零和博弈。两队零和博弈中每一组只有一个 leader 才控制这一队智能体的所有策略，获取的奖励值也是这一个小组的整体奖励值</p>
<p>FFQ 算法没有 team learder，每个人选择自己动作学习自己的策略获得自己的奖励值，但是为了更新 Q 值，每个智能体需要在每一步观测其他所有 friend 与 foe 的执行动作</p>
<p>FFQ 与 Minimax-Q 算法一样都需要利用线性规划，因此算法整体学习速度会变慢</p>
<p>&nbsp;</p>
<h2 id="Friend-or-Foe-Q"><a href="#Friend-or-Foe-Q" class="headerlink" title="Friend-or-Foe Q"></a>Friend-or-Foe Q</h2><blockquote>
<p>Friend-or-Foe Q-Learning（FFQ）算法也是从Minimax-Q算法拓展而来，巧妙地将<strong>n智能体的一般和博弈</strong>转化为两智能体的零和博弈</p>
</blockquote>
<p>FFQ 算法对一个智能体 i，将其他所有智能体分为两组，一组为 i 的 friend，帮助 i 一起最大化其奖励回报；另一组为 i 的 foe ，对抗 i 并降低 i 的奖励回报。因此对每个智能体而言都有两组。这样一个<strong>n智能体的一般和博弈</strong>就转化为了一个两智能体的零和博弈</p>
<p>其纳什均衡策略求解方法为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220304140123.png" alt="image-20220304140123497" style="zoom:65%;" />

<p><strong>算法流程</strong></p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220304151241.png" alt="image-20220304151241784" style="zoom:45%;" />

<p>&nbsp;</p>
<h2 id="WoLF-PHC"><a href="#WoLF-PHC" class="headerlink" title="WoLF-PHC"></a>WoLF-PHC</h2><blockquote>
<p>上述的三种方法都需要在学习过程中维护Q函数，假设动作空间 Ai 与状态空间 S 都是离散，假设每个智能体的动作空间相同，则对于每一个智能体都需要有一个 S×An 大小的空间来存储Q值，因此上述三种方法所需空间非常大</p>
<p>为了解决上述问题，我们期望每个智能体只用知道自己的动作来维护Q值函数，这样空间就降到了 S×A WoLF-PHC 就是这样的算法，每个智能体只用保存自己的动作来完成学习任务</p>
</blockquote>
<p><strong>WoLF</strong></p>
<p>Win or Learn Fast，当智能体做的比期望值好的时候小心缓慢的调整参数，当智能体做的比期望值差的时候，加快步伐调整参数</p>
<p><strong>PHC</strong></p>
<p>一种单智能体在稳定环境下的学习算法，其核心是强化学习的通常思想，增大能够得到最大累积期望的动作的选取概率。具有合理性，能够收敛到最优策略，流程如下：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220304151327.png" alt="image-20220304151327291" style="zoom:45%;" />

<center>最后一行 δ 选取 δl 还是 δw 由实际情况决定</center>



<p><strong>WoLF-PHC</strong></p>
<ul>
<li>为了将 PHC 应用于动态环境中，将WoLF与PHC算法结合，使得智能体获得的奖励在比预期差时能够快速调整适应其他智能体策略变化；当比预期好时谨慎学习，给其他智能体适应策略变化的时间</li>
<li>WoLF-PHC 算法能够收敛到纳什均衡策略，并具备合理性，当其他智能体采用某个固定策略时，其也能收敛到一个目前状况下的最优策略，而不是不管效果好坏无脑收敛到一个纳什均衡策略处</li>
<li>在 WoLF-PHC 算法中，使用一个可变的学习速率 <code>δ</code> 来实现WoLF效果，当策略效果较差时使用 <code>δl</code> ，策略效果较好时使用 <code>δw</code> ，并且满足 <code>δl</code> &gt; <code>δw</code> 。</li>
<li>WoLF-PHC 算法还有一个优势是不用观测其他智能体的策略、动作及奖励值，需要更少的空间去记录Q值</li>
<li>WoLF-PHC 算法是通过 PHC 算法进行学习改进策略的，所以不需要使用线性规划或者二次规划求解纳什均衡，算法速度得到了提高</li>
<li>虽然 WoLF-PHC 算法在实际应用中取得了非常好的效果，并且能够收敛到最优策略，但是其收敛性一直没有理论证明</li>
</ul>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220304154558.png" alt="image-20220304154558146" style="zoom:50%;" />

<p>&nbsp;</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><code>MiniMax-Q</code>、<code>Nash Q</code>、<code>Friend-or-Foe Q</code> 算法的本质是通过训练更新Q表从而准确评估得到状态价值矩阵，然后对每个状态的价值矩阵进行博弈，求出每个状态的纳什均衡策略作为最佳策略</p>
<p><code>WoLF-HPC</code> 比较像单智能体的Q-Learning</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h1 id="矩阵博弈中的分布式学习算法"><a href="#矩阵博弈中的分布式学习算法" class="headerlink" title="矩阵博弈中的分布式学习算法"></a>矩阵博弈中的分布式学习算法</h1><blockquote>
<p>多智能体系统一直是学术界和工业界的热点，其核心领域是关于如何将系统采用分布式的算法控制。在分布式算法中，没有进行总体控制的中心节点。每个智能体通过与环境交互自己学习自己的最优策略，在不知道全局信息的情况下将策略收敛到纳什均衡点。</p>
<p>通常智能体只知道自身获得的奖励值，不知道其他智能体的策略、奖励。分布式算法的核心难点就在于如何在只使用不完整信息的情况下，使每个智能体学到的策略收敛到纳什均衡点。</p>
</blockquote>
<p>上一节提到的简单算法都是基于线性规划、二次规划来求解矩阵博弈的纳什均衡策略，而本节中会给出另外4种学习算法来进行求解。矩阵博弈的学习算法是指每个智能体不断与环境交互，通过获得的奖励值来更新优化其策略，通常可以分为两类：</p>
<ul>
<li>学习自动机</li>
<li>梯度提升</li>
</ul>
<h3 id="学习自动机"><a href="#学习自动机" class="headerlink" title="学习自动机"></a>学习自动机</h3><p>学习自动机是一种通过与环境交互获得奖励来修改动作空间中每个动作的概率分布，从而提升优化策略的方法。学习自动机是一种完全分布式的算法，每个智能体只用知道自己的策略与奖励值，不需要知道环境信息以及其他智能体的信息</p>
<p>学习自动机通常可以用一个元组表示为 <code>(A,r,p,U)</code> ，其中 <code>A=a1,…,am</code> 表示动作集； <code>r</code> 为奖励值； <code>p</code> 为动作集的一个概率分布，即一个要学习的策略； <code>U</code> 表示一个学习算法</p>
<p>下面主要介绍两种算法  <strong>linear reward-inaction</strong> 算法与  <strong>linear reward-penalty</strong> 算法</p>
<h4 id="linear-reward-inaction"><a href="#linear-reward-inaction" class="headerlink" title="linear reward-inaction"></a>linear reward-inaction</h4><blockquote>
<p>适用于：<strong>n智能体矩阵博弈</strong> 或 <strong>双智能体零和博弈</strong></p>
<p>收敛条件：博弈只具有<strong>纯策略严格纳什均衡</strong></p>
</blockquote>
<p>在此算法中，第 i 个智能体的策略更新公式为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306162943.png" alt="image-20220306162943322" style="zoom:65%;" />

<p>其中，<code>k</code> 表示时刻；<code>p</code> 的上标 <code>i</code> 是智能体标号、下标 <code>c</code> 是动作编号，表示该智能体此动作的概率；<code>η</code> 为学习速率；<code>r</code> 表示在k时刻执行动作 <code>c</code> 的奖励值</p>
<p>其意义是将优秀的动作的选择概率提高，为了保证归一性，则将其其他动作被选择的概率降低，只能在博弈只具有纯策略的纳什均衡点的时候可以收敛</p>
<p>完整学习流程为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306162849.png" alt="image-20220306162849763" style="zoom:45%;" />



<h4 id="linear-reward-penalty"><a href="#linear-reward-penalty" class="headerlink" title="linear reward-penalty"></a>linear reward-penalty</h4><blockquote>
<p>适用于：<strong>n智能体矩阵博弈</strong>或<strong>双智能体零和博弈</strong></p>
<p>收敛条件：博弈只有<strong>混合策略的纳什均衡</strong></p>
</blockquote>
<p>通过在 <strong>linear reward-inaction</strong> 算法基础上加入一个惩罚系数得到 <strong>linear reward-penalty</strong> 算法，从而把收敛条件从 纯策略严格纳什均衡 放宽为 混合策略非严格纳什均衡</p>
<p>在此算法中，第 i 个智能体的策略更新公式为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306163818.png" alt="image-20220306163818243" style="zoom:73%;" />

<p>算法中的学习速率应该满足 0 &lt; <code>η1</code>,<code>η2</code> &lt; 1 和 <code>η1</code> &gt; <code>η2</code>，完整流程为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306164128.png" alt="image-20220306164128043" style="zoom:45%;" />

<p>&nbsp;</p>
<h3 id="梯度提升"><a href="#梯度提升" class="headerlink" title="梯度提升"></a>梯度提升</h3><p>梯度提升学习算法在很多地方都有用到，强化学习的随机策略梯度算法SPG、DPG、AC、A2C、A3C等算法都有用到这种思想。使策略的更新方向沿着累积回报增加最大的梯度方向。</p>
<p>虽然有证明，梯度提升学习算法并不能在所有的矩阵博弈中的到收敛解。但是在某些特定的约束博弈中，有一些梯度提升算法可以求解其纳什均衡，常见的有：</p>
<ul>
<li>WoLF-IGA</li>
<li>Lagging Anchor</li>
</ul>
<h4 id="WoLF-IGA"><a href="#WoLF-IGA" class="headerlink" title="WoLF-IGA"></a>WoLF-IGA</h4><blockquote>
<p>适用于：<strong>双智能体双动作</strong>矩阵博弈</p>
<p>收敛条件：<strong>纯策略</strong> 或 <strong>混合策略</strong> 的纳什均衡</p>
</blockquote>
<p>使用 WoLF（Win or learn fast）思想与梯度提升结合。获胜或优秀策略的含义是指当前策略的累积预期奖励大于当前玩家纳什均衡策略和其他玩家使用当前策略的累积预期奖励。当前策略获胜时则谨慎缓慢学习，给其他智能体适应策略变化的时间；当前策略较差时，快速更新调整，使其能够快速调整适应其他智能体策略变化</p>
<p>WoLF-IGA 根据累计奖励关于策略的梯度来修正策略，其目的是使更新后的策略能够获得更大的奖励值，更新方法如下：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306165743.png" alt="image-20220306165742927" style="zoom:63%;" />

<p>其中，<code>p1</code> 表示智能体1选择动作1的概率，则其选择动作2的概率则为 <code>1-p1</code>；<code>η</code> 为学习速率，要足够小；<code>α</code> 的大小变化体现了WoLF的思想；<code>V(p(k), q(k))</code> 表示在时刻k使用策略 <code>(p(k), q(k))</code> 的累计期望奖励； <code>(p*(k), q*(k))</code> 为纳什均衡策略</p>
<p>WoLF-IGA 算法的难点在于需要已知大量信息。其信息包括自身的奖励矩阵、其他玩家的策略以及自己的纳什均衡</p>
<p>虽然智能体知道自己的奖励矩阵，也会得到纳什均衡策略，但这样大量的已知信息导致这个算法并不是一个实用的算法，也不是一个分布式的算法</p>
<h4 id="Lagging-Anchor"><a href="#Lagging-Anchor" class="headerlink" title="Lagging Anchor"></a>Lagging Anchor</h4><blockquote>
<p>适用于：<strong>双智能体零和博弈</strong></p>
<p>收敛条件：<strong>完全混合策略</strong> 的纳什均衡</p>
</blockquote>
<p>定义 <code>v=[p1,p2,…,pm1]</code> 是智能体1对于其 <code>m1</code> 个动作的概率分布，即策略；同理 <code>w=[q1,q2,…,qm2]</code> 为智能体2的策略，策略迭代公式为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306170459.png" alt="image-20220306170459551" style="zoom:63%;" />

<p>其中，<code>η</code> 为学习速率；<code>ηd</code> &gt; 0 定义为拉锚因子（anchor drawing factor），<code>Pmi</code> 是用于维护策略 <code>v</code>、<code>w</code> 归一化的矩阵：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306170816.png" alt="image-20220306170816703" style="zoom:70%;" />

<p><code>Y(k)</code> 是一个单位向量，若智能体2在时刻k采用第i个动作则第i个元素为1，其余元素为0；<code>X(k)</code> 同理，反映智能体1；</p>
<p>Lagging Anchor 算法的核心是每个智能体的策略的加权平均：<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306171046.png" alt="image-20220306171046465" style="zoom:70%;" />，又成为锚参数</p>
<p>由于这个算法需要用到每一个智能体的奖励矩阵 <code>R1</code>、<code>R2</code> ，因此严格来说其不能算作是一个分布式算法，但是在放宽条件以及智能体之间可以通信的情况下，也算是一个不错的算法</p>
<p>&nbsp;</p>
<h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306171521.png" alt="image-20220306171521785"></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h1 id="MADDPG"><a href="#MADDPG" class="headerlink" title="MADDPG"></a>MADDPG</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>MADDPG 是 OpenAI 2017发表在 NIPS 上的一篇文章 <a target="_blank" rel="noopener" href="https://lrk612.com/resources/MADDPG.pdf">Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments </a> 中提出的，主要是将AC算法进行了一系列改进，使其能够适用于传统RL算法无法处理的复杂多智能体场景</p>
<p>传统RL算法在多智能体场景下面临的问题有：</p>
<ul>
<li>由于每个智能体都是在不断学习改进其策略，因此从每一个智能体的角度看，环境是一个动态不稳定的，不符合传统RL收敛条件</li>
<li>在一定程度上，无法通过仅仅改变智能体自身的策略来适应动态不稳定的环境</li>
<li>由于环境的不稳定，将无法直接使用之前的经验回放等DQN的关键技巧</li>
<li>policy gradient 算法会由于智能体数量的变多使得本就有的方差大的问题加剧</li>
</ul>
<p>MADDPG 的三点技巧：</p>
<ol>
<li><p>集中式训练，分布式执行</p>
<p> 训练时采用集中式学习训练 critic 与 actor，使用时 actor 只用知道局部信息就能运行。critic 需要其他智能体的策略信息，文章给了一种估计其他智能体策略的方法，能够只用知道其他智能体的观测与动作</p>
</li>
<li><p>改进了经验回放记录的数据</p>
<p> 为了能够适用于动态环境，每一条信息由 <code>(x,x&#39;,aq,…,an,r1,…,rn)</code> 组成，<code>x=(o1,…,on)</code> 表示每个智能体的观测</p>
</li>
<li><p>利用策略集合效果优化（policy ensemble）</p>
<p> 对每个智能体学习多个策略，改进时利用所有策略的整体效果进行优化，以提高算法的稳定性以及鲁棒性</p>
</li>
</ol>
<p>MADDPG 本质上还是一个 DPG 算法，针对每个智能体训练一个需要全局信息的 Critic 以及一个需要局部信息的 Actor，并且允许每个智能体有自己的奖励函数（reward function），因此可以用于合作任务或对抗任务，并且由于脱胎于 DPG 算法，其动作空间可以是连续的</p>
<h3 id="传统-DRL-思路"><a href="#传统-DRL-思路" class="headerlink" title="传统 DRL 思路"></a>传统 DRL 思路</h3><h4 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h4><p>DQN的思想就是设计一个 <code>Q(s,a|θ)</code> 不断逼近真实的 <code>Q(s,a)</code> 函数。其中主要用到了两个技巧：</p>
<ol>
<li>经验回放</li>
<li>目标网络</li>
</ol>
<p>该技巧主要用来打破数据之间联系，因为神经网络对数据的假设是独立同分布，而MDP过程的数据前后有关联。打破数据的联系可以更好地拟合 <code>Q(s,a)</code> 函数。其代价函数为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306205629.png" alt="image-20220306205629582" style="zoom:75%;" />

<p>其中计算y时使用的网络是目标Q网络，其参数更新与当前Q网络不同步（滞后）</p>
<h4 id="SPG"><a href="#SPG" class="headerlink" title="SPG"></a>SPG</h4><p>stochastic policy gradient 算法不采用拟合Q函数的方式，而是直接优化累积回报来获得使回报最大的策略。假定参数化的策略为 <code>Πθ(a|s)</code>，累积回报为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306205844.png" alt="image-20220306205844149" style="zoom:75%;" />

<p>为了使 <code>J(θ)</code> 最大化，直接对策略参数求导得到策略更新梯度：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306205936.png" alt="image-20220306205936394" style="zoom:75%;" />

<p>Actor-Critic 算法也可由 SPG + DQN 共同推出，按照DQN的方法拟合一个 <code>Q(s,a|θ)</code> 函数，则这个参数化的 <code>Q(s,a|θ)</code> 函数被称为 Critic，<code>Πθ(a|s)</code>  被称为 Actor</p>
<h4 id="DPG"><a href="#DPG" class="headerlink" title="DPG"></a>DPG</h4><p>DQN、SPG 都是针对随机策略的，<code>Πθ(a|s)</code>  是一个在状态 s 对于各个动作 a 的条件概率分布。而 DPG 是针对确定性策略的，<code>μ(a|θ): S——&gt;A</code> 是一个状态空间到动作空间的映射。其思想与 SPG 相同，策略梯度公式为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306210928.png" alt="image-20220306210927967" style="zoom:75%;" />

<p>DPG 可以使用 AC 的方法来估计一个Q函数，DDPG 就是借用了 DQN 经验回放与目标网络的技巧</p>
<p>&nbsp;</p>
<h3 id="MADDPG-1"><a href="#MADDPG-1" class="headerlink" title="MADDPG"></a>MADDPG</h3><h4 id="多智能体-Actor-Critic-设计"><a href="#多智能体-Actor-Critic-设计" class="headerlink" title="多智能体 Actor-Critic 设计"></a>多智能体 Actor-Critic 设计</h4><p>MADDPG是集中式的学习，分布式的应用，允许使用一些额外的信息（全局信息）进行学习，只要在应用的时候使用局部信息进行决策就行。这点就是Q-learning的一个不足之处，Q-learning在学习与应用时必须采用相同的信息。</p>
<p>MADDPG对传统的AC算法进行了一个改进，Critic扩展为可以利用其他智能体的策略进行学习，这点的进一步改进就是每个智能体对其他智能体的策略进行一个函数逼近</p>
<hr>
<p>用 <code>θ=[θ1,…,θn]</code> 表示 n 个智能体策略的参数，<code>Π=[Π1,…,Πn]</code> 表示 n 个智能体的策略。第 i 个智能体的累计期望奖励为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306211805.png" alt="image-20220306211805156" style="zoom:70%;" />

<p>求导的策略梯度：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306211823.png" alt="image-20220306211823813" style="zoom:75%;" />

<p>其中，<code>oi</code> 表示第 i 个智能体的观测，<code>x=[o1,……,on]</code> 表示观测向量，即总状态；第 i 个智能体集中式的状态-动作函数为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306211941.png" alt="image-20220306211940984" style="zoom:70%;" />

<p>由于是每个智能体独立学习自己的状态-动作函数，所以每个智能体可以有不同的奖励函数，完成合作或竞争任务</p>
<hr>
<p>上述为随机策略梯度算法，下面拓展到确定性策略 <code>μθ</code>，梯度公式为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306212139.png" alt="image-20220306212139199" style="zoom:75%;" />

<p>由以上两个梯度公式可以看出该算法与SPG与DPG十分类似，就像是将单体直接扩展到多体。但其实 <code>Qμ</code> 是一个非常厉害的技巧，针对每个智能体建立值函数，极大的解决了传统 RL 算法在 Multi-agent 领域的不足。D 是一个经验池（experience replay buffer），元素组成为 <code>(x,x&#39;,a1,…,an,r1,…,rn)</code> </p>
<p>集中式的 critic 的更新方法借鉴了DQN中TD与目标网络思想：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306212411.png" alt="image-20220306212411756" style="zoom:90%;" />

<center>式 1</center>

<p>其中，y 的计算使用目标Q网络，<code>μ’=[μ’1,…,μ’n]</code> 就是目标策略滞后更新的参数 <code>θ’</code>，其他智能体的策略可以采用拟合逼近的方式得到，不需要通信交互</p>
<p>如上可以看出 critic 借用了全局信息学习，actor 只是用了局部观测信息。MADDPG 的一个启发就是，如果知道所有的智能体的动作，那么环境就是稳定的，就算策略在不断更新环境也是恒定的</p>
<h4 id="估计其他智能体策略"><a href="#估计其他智能体策略" class="headerlink" title="估计其他智能体策略"></a>估计其他智能体策略</h4><p>在 (1) 式中用到了其他智能体的策略，这本需要不断的通信来获取，但是也可以放宽这个条件，通过对其他智能体的策略进行估计来实现</p>
<p>每个智能体维护 n-1 个策略逼近函数 <code>μ^iΦj</code> 表示第 i 个智能体对第j个智能体策略 <code>μj</code> 的函数逼近。其逼近代价为 对数代价函数 加上 策略的熵，可写为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306213205.png" alt="image-20220306213205761" style="zoom:75%;" />

<p>只要最小化上述代价函数，就能得到其他智能体策略的逼近，从而 (1) 式中的 y 可计算为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306213321.png" alt="image-20220306213321401" style="zoom:78%;" />

<p>所以 MADDPG 在更新Critic网络 <code>Qμ</code> 之前，先利用经验回放的一个采样batch更新其他智能体策略预测网络 <code>μ^iΦj</code></p>
<h4 id="策略集合优化（policies-ensemble）"><a href="#策略集合优化（policies-ensemble）" class="headerlink" title="策略集合优化（policies ensemble）"></a>策略集合优化（policies ensemble）</h4><p>这个技巧也是文章的一个亮点，多智能体强化学习一个顽固的问题是由于每个智能体的策略都在更新迭代导致环境针对一个特定的智能体是动态不稳定的。这种情况在竞争任务下尤其严重，经常会出现一个智能体针对其竞争对手过拟合出一个强策略。但是这个强策略是非常脆弱的，因为随着竞争对手策略的更新改变，这个强策略很难去适应新的对手策略</p>
<p>为了能更好的应对上述情况，MADDPG提出了一种策略集合的思想，第i个智能体的策略 <code>μi</code> 由一个具有 <code>K</code> 个子策略的集合构成，在每一个训练 episode 中只是用一个子策略 <code>μi(k)</code> 。对每一个智能体，最大化其策略集合的整体奖励：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306213930.png" alt="image-20220306213930276" style="zoom:75%;" />

<p>并且为每一个子策略k构建一个记忆存储 <code>Di(k)</code> ，优化策略集合的整体效果，由此针对每一个子策略的更新梯度为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220306214020.png" alt="image-20220306214020321" style="zoom:70%;" />

<p>&nbsp;</p>
<h3 id="仿真效果"><a href="#仿真效果" class="headerlink" title="仿真效果"></a>仿真效果</h3><p>// TODO</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Lrk612
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://sharp-rookie.github.io/2022/03/03/%E3%80%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%20&%20%E7%AE%97%E6%B3%95/" title="【多智能体强化学习】基础概念 &amp; 算法">https://sharp-rookie.github.io/2022/03/03/【多智能体强化学习】基础概念 & 算法/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/Algorithm/" rel="tag"># Algorithm</a>
              <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
              <a href="/tags/Mutli-Agent-RL/" rel="tag"># Mutli-Agent RL</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/03/03/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E8%B7%9D%E7%A6%BB%20&%20%E7%9B%B8%E4%BC%BC%E5%BA%A6/" rel="prev" title="【机器学习】距离 & 相似度">
      <i class="fa fa-chevron-left"></i> 【机器学习】距离 & 相似度
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/03/05/%E3%80%90%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E3%80%91MATLAB%20%E5%9B%BE%E5%83%8F%E8%BF%90%E7%AE%97/" rel="next" title="【数字图像处理】Matlab：图像的运算">
      【数字图像处理】Matlab：图像的运算 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B8%8E%E5%8D%9A%E5%BC%88%E8%AE%BA"><span class="nav-text">基础知识与博弈论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%95%E8%A8%80"><span class="nav-text">引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%9A%E5%BC%88%E8%AE%BA%E5%9F%BA%E7%A1%80"><span class="nav-text">博弈论基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%8D%9A%E5%BC%88"><span class="nav-text">矩阵博弈</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1"><span class="nav-text">纳什均衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%A5%E6%A0%BC%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1"><span class="nav-text">严格纳什均衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%8C%E5%85%A8%E6%B7%B7%E5%90%88%E7%AD%96%E7%95%A5"><span class="nav-text">完全混合策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%AF%E7%AD%96%E7%95%A5"><span class="nav-text">纯策略</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%A4%E4%B8%AA%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1"><span class="nav-text">两个智能体的纳什均衡</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9B%B6%E5%92%8C%E5%8D%9A%E5%BC%88"><span class="nav-text">零和博弈</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E8%88%AC%E5%92%8C%E5%8D%9A%E5%BC%88"><span class="nav-text">一般和博弈</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E6%B1%82%E8%A7%A3%E5%8F%8C%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E9%9B%B6%E5%92%8C%E5%8D%9A%E5%BC%88"><span class="nav-text">线性规划求解双智能体的零和博弈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%A0%E4%B8%AA%E5%8D%9A%E5%BC%88%E6%A6%82%E5%BF%B5"><span class="nav-text">几个博弈概念</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-text">多智能体强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%BA%E6%A8%A1"><span class="nav-text">建模</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B"><span class="nav-text">学习过程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95"><span class="nav-text">基础算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MiniMax-Q"><span class="nav-text">MiniMax-Q</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Nash-Q"><span class="nav-text">Nash Q</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Friend-or-Foe-Q"><span class="nav-text">Friend-or-Foe Q</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#WoLF-PHC"><span class="nav-text">WoLF-PHC</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%8D%9A%E5%BC%88%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="nav-text">矩阵博弈中的分布式学习算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%8A%A8%E6%9C%BA"><span class="nav-text">学习自动机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#linear-reward-inaction"><span class="nav-text">linear reward-inaction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#linear-reward-penalty"><span class="nav-text">linear reward-penalty</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87"><span class="nav-text">梯度提升</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#WoLF-IGA"><span class="nav-text">WoLF-IGA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lagging-Anchor"><span class="nav-text">Lagging Anchor</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93-1"><span class="nav-text">小结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MADDPG"><span class="nav-text">MADDPG</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-text">简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9F-DRL-%E6%80%9D%E8%B7%AF"><span class="nav-text">传统 DRL 思路</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DQN"><span class="nav-text">DQN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SPG"><span class="nav-text">SPG</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DPG"><span class="nav-text">DPG</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MADDPG-1"><span class="nav-text">MADDPG</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93-Actor-Critic-%E8%AE%BE%E8%AE%A1"><span class="nav-text">多智能体 Actor-Critic 设计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%B0%E8%AE%A1%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E4%BD%93%E7%AD%96%E7%95%A5"><span class="nav-text">估计其他智能体策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E9%9B%86%E5%90%88%E4%BC%98%E5%8C%96%EF%BC%88policies-ensemble%EF%BC%89"><span class="nav-text">策略集合优化（policies ensemble）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BF%E7%9C%9F%E6%95%88%E6%9E%9C"><span class="nav-text">仿真效果</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Lrk612"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Lrk612</p>
  <div class="site-description" itemprop="description">Lrk's blog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">41</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.csdn.net/" title="https:&#x2F;&#x2F;www.csdn.net&#x2F;" rel="noopener" target="_blank">CSDN</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.google.com/" title="https:&#x2F;&#x2F;www.google.com" rel="noopener" target="_blank">Google</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <!-- 访问量 -->

    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


<!-- 版权 -->
<div class="copyright">
  
  &copy; 2021-12-1 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lrk612</span>
</div>

<!-- ICP备案 -->
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP备案 </a>
      <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211209170739.png" style="display: inline-block;"><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=2021035798" rel="noopener" target="_blank">豫ICP备2021035798号 </a>
  </div>

<!-- 不知道是什么 -->
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'fH6OdGBcCG8D6jC8NeeJlX9b-gzGzoHsz',
      appKey     : '1UoIb6vszMw6wl0n7kreb4Xz',
      placeholder: "Just go go ^_^",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
    var infoEle = document.querySelector('#comments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
      infoEle.childNodes.forEach(function(item) {
        item.parentNode.removeChild(item);
      });
    }
  }, window.Valine);
});
</script>

</body>
</html>
