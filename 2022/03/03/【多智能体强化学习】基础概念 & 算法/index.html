<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sharp-rookie.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

<script> 
   (function(){
          if(''){
              if (prompt('请输入文章密码') !== ''){
                  alert('密码错误！');
                  history.back();
              }
          }
      })();
  </script>
  <meta name="description" content="&amp;emsp;&amp;emsp;因为项目中遇到了多个体共同决策的场景，不同个体的离散动作空间和评价指标都不同，在使用DQN时发现随着个体数量的增加，动作空间会指数式扩张，同时奖励值的设计也很难兼顾所有需求。所以一方面尝试使用策略优化的DRL算法解决离散动作空间维度爆炸问题，另一方面考虑使用多智能体强化学习的算法。 &amp;emsp;&amp;emsp;单智能体强化学习算法有Sutton的神作《Reinforcement">
<meta property="og:type" content="article">
<meta property="og:title" content="【多智能体强化学习】基础概念 &amp; 算法">
<meta property="og:url" content="https://sharp-rookie.github.io/2022/03/03/%E3%80%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%20&%20%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="最忆是江南">
<meta property="og:description" content="&amp;emsp;&amp;emsp;因为项目中遇到了多个体共同决策的场景，不同个体的离散动作空间和评价指标都不同，在使用DQN时发现随着个体数量的增加，动作空间会指数式扩张，同时奖励值的设计也很难兼顾所有需求。所以一方面尝试使用策略优化的DRL算法解决离散动作空间维度爆炸问题，另一方面考虑使用多智能体强化学习的算法。 &amp;emsp;&amp;emsp;单智能体强化学习算法有Sutton的神作《Reinforcement">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303221852.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220310100438.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303193953.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303195023.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303200139.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303205335.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303210048.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303210148.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303211027.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303211303.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303211337.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303211551.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303211615.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303220110.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303220150.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303220311.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303220355.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303220829.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220310100853.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303213014.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303213350.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303213453.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220310101011.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303214618.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303215135.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303215312.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220310144235.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220304104127.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220304142823.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220310135446.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220310135539.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220310135735.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220304142844.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220304140123.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220304151241.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220304151327.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220304154558.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220310105444.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220310140130.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306205629.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306205844.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306205936.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306210928.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220314111520.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306211805.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306211823.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306211941.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306212139.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306212411.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306213205.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306213321.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306213930.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306214020.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220314094130.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220314094348.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220310142821.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220314100438.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220314100735.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220310143539.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220314102144.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220307193631.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220307194034.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220307194618.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220307194709.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306162943.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306162849.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306163818.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306164128.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306165743.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306170459.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306170816.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306171046.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220306171521.png">
<meta property="article:published_time" content="2022-03-03T14:23:42.444Z">
<meta property="article:modified_time" content="2022-04-25T03:39:28.328Z">
<meta property="article:author" content="Lrk612">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Algorithm">
<meta property="article:tag" content="Reinforcement Learning">
<meta property="article:tag" content="Mutli-Agent RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220303221852.png">

<link rel="canonical" href="https://sharp-rookie.github.io/2022/03/03/%E3%80%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%20&%20%E7%AE%97%E6%B3%95/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>【多智能体强化学习】基础概念 & 算法 | 最忆是江南</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
        <a target="_blank" rel="noopener" href="https://github.com/Sharp-rookie" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">最忆是江南</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Love actually is all around</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th-list fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-papers">

    <a href="/papers/" rel="section"><i class="fa fa-book fa-fw"></i>论文阅读</a>

  </li>
        <li class="menu-item menu-item-friendlink">

    <a href="/friendlink/" rel="section"><i class="fa fa-link fa-fw"></i>友链</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-download fa-fw"></i>资源</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sharp-rookie.github.io/2022/03/03/%E3%80%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%20&%20%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Lrk612">
      <meta itemprop="description" content="Lrk's blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="最忆是江南">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【多智能体强化学习】基础概念 & 算法
        </h1>

        <div class="post-meta">
         
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-03 22:23:42" itemprop="dateCreated datePublished" datetime="2022-03-03T22:23:42+08:00">2022-03-03</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/" itemprop="url" rel="index"><span itemprop="name">知识分享</span></a>
                </span>
            </span>

          
            <span id="/2022/03/03/%E3%80%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%20&%20%E7%AE%97%E6%B3%95/" class="post-meta-item leancloud_visitors" data-flag-title="【多智能体强化学习】基础概念 & 算法" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/03/03/%E3%80%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%20&%20%E7%AE%97%E6%B3%95/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/03/03/%E3%80%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%20&%20%E7%AE%97%E6%B3%95/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>&emsp;&emsp;因为项目中遇到了多个体共同决策的场景，不同个体的离散动作空间和评价指标都不同，在使用DQN时发现随着个体数量的增加，动作空间会指数式扩张，同时奖励值的设计也很难兼顾所有需求。所以一方面尝试使用策略优化的DRL算法解决离散动作空间维度爆炸问题，另一方面考虑使用多智能体强化学习的算法。</p>
<p>&emsp;&emsp;单智能体强化学习算法有Sutton的神作《Reinforcement Learning: An introduction》，但是多智能体强化学习方面就没什么系统的网课或者书籍，因此算是比较前沿的研究领域。多智能体强化学习专注于实现具有多个智能体的自主、自学习系统的领域，其中涉及大量博弈论概念，值得一学。</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303221852.png" alt="image-20220303221852425" style="zoom:28%;" />

<p><strong>关键词：</strong> MiniMax-Q、Nash-Q、MADDPG、COMDA、VDN、QMIX、博弈论、纳什均衡</p>
<span id="more"></span>

<p>&nbsp;</p>
<h1 id="基础知识与博弈论"><a href="#基础知识与博弈论" class="headerlink" title="基础知识与博弈论"></a>基础知识与博弈论</h1><h2 id="强化学习与多智能体系统"><a href="#强化学习与多智能体系统" class="headerlink" title="强化学习与多智能体系统"></a>强化学习与多智能体系统</h2><p>强化学习的核心思想是“试错”（trial-and-error）：智能体通过与环境的交互，根据获得的反馈信息迭代地优化。在 RL 领域，待解决的问题通常被描述为<strong>马尔科夫决策过程</strong></p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220310100438.png" alt="image-20220310100438415" style="zoom:50%;" />

<p>当同时存在多个智能体与环境交互时，整个系统就变成一个多智能体系统（multi-agent system）。每个智能体仍然是遵循着强化学习的目标，也就是是最大化能够获得的累积回报，而此时环境全局状态的改变就和所有智能体的联合动作（joint action）相关了。因此在智能体策略学习的过程中，需要考虑联合动作的影响。</p>
<p>基于此背景，如果仍用单智能体解决多个体决策问题，将会因为其他智能体的决策导致环境变化而难以收敛。由此引入多智能体强化学习，为每个个体维护一个做决策的智能体。智能体之间存在合作与竞争，最终实现最大共同利益或者规定的利益。在此问题场景下，引入博弈论概念，将博弈论与强化学习相结合从而更好地处理这些问题</p>
<p>&nbsp;</p>
<h2 id="博弈论基础"><a href="#博弈论基础" class="headerlink" title="博弈论基础"></a>博弈论基础</h2><h3 id="矩阵博弈"><a href="#矩阵博弈" class="headerlink" title="矩阵博弈"></a>矩阵博弈</h3><p>矩阵博弈可以表示为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303193953.png" alt="image-20220303193953815" style="zoom:40%;" />

<p>其中 <code>n</code> 表示智能体数量，<code>A </code>为动作，<code>Ri：A1×A2×……×An——&gt;R</code> 表示第i个智能体的奖励函数，可见奖励函数与每个智能体的联结动作有关，联结动作空间为：<code>A1×A2×……×An</code>（其中每个A都是动作空间而不是单个动作）</p>
<p>每个智能体的策略是一个关于其动作空间的概率分布，每个智能体的目标是最大化其在联结策略 <code>(Π1, Π2,……,Πn)</code>下的奖励值：<code>Vi(Π1, Π2,……,Πn)</code></p>
<h4 id="纳什均衡"><a href="#纳什均衡" class="headerlink" title="纳什均衡"></a>纳什均衡</h4><p>在矩阵博弈中，如果联结策略 <code>(Π1*, Π2*,……,Πn*)</code> 满足：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303195023.png" alt="image-20220303195023004" style="zoom:70%;" />

<center>式 1</center>

<p>则称为一个纳什均衡。也就是说，纳什均衡是一个所有智能体的联结策略，在纳什均衡处，对于所有智能体而言，都不能通过仅改变自身策略的情况下获得更大的奖励值</p>
<p>设联结动作 <code>(a1,a2,……,an)</code> 的期望奖励为 <code>Q(a1,a2,……,an)</code>，令 <code>Πi(ai)</code> 表示第i个智能体选取动作 <code>ai</code></p>
<p>的概率。则纳什均衡还可定义为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303200139.png" alt="image-20220303200139101" style="zoom:60%;" />

<center>式 2</center>



<h4 id="严格纳什均衡"><a href="#严格纳什均衡" class="headerlink" title="严格纳什均衡"></a>严格纳什均衡</h4><p>公式1中取严格大于时，联结策略 <code>(Π1*, Π2*,……,Πn*)</code> 即为严格纳什均衡</p>
<h4 id="完全混合策略"><a href="#完全混合策略" class="headerlink" title="完全混合策略"></a>完全混合策略</h4><p>若一个策略对于智能体动作集中的所有动作的概率都大于0，则这个策略为一个完全混合策略</p>
<h4 id="纯策略"><a href="#纯策略" class="headerlink" title="纯策略"></a>纯策略</h4><p>若智能体的策略对一个动作的概率分布为1，对其余的动作的概率分布为0，则这个策略为一个纯策略</p>
<h3 id="两智能体的纳什均衡"><a href="#两智能体的纳什均衡" class="headerlink" title="两智能体的纳什均衡"></a>两智能体的纳什均衡</h3><p>两智能体的博弈问题是大部分多智能体强化学习算法的基础。双智能体矩阵博弈之于多智能体强化学习就像感知机之于神经网络</p>
<p>在双智能体矩阵博弈中，我们可以设计一个矩阵，矩阵每一个元素的索引坐标表示一个联结动作 <code>[A1=x, A2=y]</code>  ，第 <code>i</code> 个智能体的奖励矩阵 <code>Ri</code> 的元素 <code>rxy</code> 就表示第一个智能体采用动作 <code>x</code>，第二个智能体采用动作 <code>y</code> 时第 <code>i</code> 个智能体获得的奖励，<code>cxy</code> 也同理。通常我们将第一个智能体定义为<strong>行智能体</strong>，第二个智能体定义为<strong>列智能体</strong>，行号表示第一个智能体选取的动作，列号表示第二个智能体选取的动作。则对于只有2个动作的智能体，其奖励矩阵分别可以写为:</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303205335.png" alt="image-20220303205335878" style="zoom:70%;" />



<h4 id="零和博弈"><a href="#零和博弈" class="headerlink" title="零和博弈"></a>零和博弈</h4><p>零和博弈中，两个智能体是完全竞争对抗关系，即 <code>R1=-R2</code> 。在零和博弈中只有一个纳什均衡值，即使可能有很多纳什均衡策略，但是期望的奖励是相同的</p>
<h4 id="一般和博弈"><a href="#一般和博弈" class="headerlink" title="一般和博弈"></a>一般和博弈</h4><p>一般和博弈是指任何类型的矩阵博弈，包括完全对抗博弈、完全合作博弈以及二者的混合博弈。在一般和博弈中可能存在多个纳什均衡点</p>
<p>定义策略 <code>Πi=(Πi(a1),…,Πi(ami))</code> 为智能体 i 的动作集中每个动作的概率集合，<code>mi</code> 为可选的动作数量，则值函数 <code>Vi</code> 可表示为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303210048.png" alt="image-20220303210048275" style="zoom:72%;" />

<p>纳什均衡策略 <code>(Π1*, Π2*)</code> 可表示为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303210148.png" alt="image-20220303210148099" style="zoom:70%;" />

<p>其中 <code>PD(Ai)</code> 表示第 <code>i</code> 个智能体的策略空间，<code>-i</code> 表示另一个智能体</p>
<p>若满足：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303211027.png" alt="image-20220303211027797" style="zoom:75%;" />

<p>则称 <code>(l,f)</code> 满足纯策略严格纳什均衡，<code>-l</code>、<code>-f</code> 表示除了 <code>l</code>、<code>f</code> 的另一个策略</p>
<h3 id="线性规划求解双智能体的零和博弈"><a href="#线性规划求解双智能体的零和博弈" class="headerlink" title="线性规划求解双智能体的零和博弈"></a>线性规划求解双智能体的零和博弈</h3><p>求解双智能体零和博弈的思路是最大化每个智能体在与对手博弈中最差情况下的期望奖励值：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303211303.png" alt="image-20220303211303527" style="zoom:70%;" />

<p>博弈形式为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303211337.png" alt="image-20220303211337168" style="zoom:70%;" />

<p>定义 <code>pj(j=1,2)</code> 为第一个智能体选择动作 <code>J</code>  的概率，<code>qj(j=1,2)</code> 为第二个智能体选择动作 <code>j</code> 的概率。则可对两个智能体列写如下线性规划：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303211551.png" alt="image-20220303211551299" style="zoom:70%;" />

<center>智能体1</center>

<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303211615.png" alt="image-20220303211615167" style="zoom:70%;" />

<center>智能体2</center>

<p>求解即得纳什均衡策略</p>
<h3 id="几个博弈概念"><a href="#几个博弈概念" class="headerlink" title="几个博弈概念"></a>几个博弈概念</h3><table>
<thead>
<tr>
<th>概念</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>随机博弈</td>
<td>马尔可夫决策过程包含一个智能体与多个状态，矩阵博弈包括多个智能体与一个状态。随即博弈 (stochastic game / Markov game)是马尔可夫决策过程与矩阵博弈的结合，具有多个智能体与多个状态，即多智能体强化学习</td>
</tr>
<tr>
<td>静态博弈</td>
<td>静态博弈 (static/stateless game) 是指没有状态s，不存在动力学使状态能够转移的博弈。例如一个矩阵博弈</td>
</tr>
<tr>
<td>阶段博弈</td>
<td>阶段博弈 (stage game) 是随机博弈的组成成分，状态s是固定的，相当于一个状态固定的静态博弈，随机博弈中的Q值函数就是该阶段博弈的奖励函数。若干状态的阶段博弈组成一个随机博弈</td>
</tr>
<tr>
<td>重复博弈</td>
<td>智能体重复访问同一个状态的阶段博弈，并且在访问同一个状态的阶段博弈的过程中收集其他智能体的信息与奖励值，并学习更好的Q值函数与策略</td>
</tr>
</tbody></table>
<p><strong>随机博弈示例</strong></p>
<p>定义一个2*2的网格博弈，两个智能体分别表示为 P1、P2 ，P1的初始位置在左下角，P2的初始位置在右上角，每一个智能体都想以最快的方式达到 G 标志的地方。从初始位置开始，每个智能体都有两个动作可以选择。只要有一个智能体达到 G 则游戏结束，达到 G 的智能体获得奖励10，奖励折扣率为0.9。虚线表示栏杆，智能体穿过栏杆的概率为0.5</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303220110.png" alt="image-20220303220110825" style="zoom:55%;" />

<p>该随机博弈一共包含7个状态，纳什均衡策略是：每个智能体到达邻居位置而不穿过栏杆</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303220150.png" alt="image-20220303220150599" style="zoom:40%;" />

<p>根据上面公式，可得状态值函数：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303220311.png" alt="image-20220303220311352" style="zoom:70%;" />

<p>由此可得动作状态值函数：</p>
<p><img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303220355.png" alt="image-20220303220355121"></p>
<p>求解下图中的矩阵博弈即可得到多智能体强化学习的策略：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303220829.png" alt="image-20220303220829821" style="zoom:50%;" />

<p>&nbsp;</p>
<h2 id="多智能体强化学习（MARL）"><a href="#多智能体强化学习（MARL）" class="headerlink" title="多智能体强化学习（MARL）"></a>多智能体强化学习（MARL）</h2><blockquote>
<p>多智能体强化学习就是一个随机博弈，将每一个状态的阶段博弈的纳什策略组合起来成为一个智能体在动态环境中的策略，并不断与环境交互来更新每一个状态的阶段博弈中的Q值函数（博弈奖励）</p>
</blockquote>
<h3 id="数学建模"><a href="#数学建模" class="headerlink" title="数学建模"></a>数学建模</h3><p>马尔科夫决策过程拓展到多智能体系统，就成为了马尔科夫博弈（又称为随机博弈，Markov/stochastic game），也具有马尔科夫性，下一个状态与奖励只与当前状态与当前的联结动作有关</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220310100853.png" alt="image-20220310100853019" style="zoom:50%;" />

<p>记随机博弈为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303213014.png" alt="image-20220303213014817" style="zoom:70%;" />

<p>其中 <code>n</code> 表示智能体数量，<code>S</code> 表示状态空间，<code>Ai</code> 表示第 <code>i</code> 个智能体的动作空间（不是单个动作），<code>Tr</code> 为状态转移概率：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303213350.png" alt="image-20220303213350704" style="zoom:70%;" />

<p>奖励 <code>R</code> 定义为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303213453.png" alt="image-20220303213453284" style="zoom:70%;" />

<p>即第 <code>i</code> 个智能体在当前状态与联结动作下获得的回报值， <code>γ</code> 表示累积奖励折扣系数，此时第 i 个智能体的累积奖励期望可表示为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220310101011.png" alt="image-20220310101011225" style="zoom:50%;" />



<h3 id="学习过程"><a href="#学习过程" class="headerlink" title="学习过程"></a>学习过程</h3><p>多智能体强化学习过程，就是<strong>找到每一个状态的纳什均衡策略</strong>，然后将这些策略联合起来。<code>Πi: S——&gt;Ai</code>  就是一个智能体 i 的策略，在每个状态选出最优的纳什策略</p>
<blockquote>
<p>值得注意的是，纳什均衡不一定是全局最优，但它是在概率上最容易产生的结果，是在学习时较容易收敛到的状态，特别是如果当前智能体无法知道其他智能体将会采取怎样的策略</p>
</blockquote>
<p>多智能体强化学习最优策略（随机博弈的纳什均衡策略）可以写为  <code>(Π1*, Π2*,……,Πn*)</code>  ，且对 <code>任意s∈S,i=1,……,n</code> 满足：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303214618.png" alt="image-20220303214618925" style="zoom:70%;" />

<p>不等式左边为 <code>γ</code> 折扣累积状态值函数，用 <code>Vi*(s)</code> 简记上式，用 <code>Qi*(a1,a2,……,an)</code> 表示动作状态 <code>γ</code> 折扣累积函数，在每个固定状态s的阶段博弈中，利用 <code>Qi*</code> 作为博弈的奖励求解纳什均衡策略，根据贝尔曼方程可得：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303215135.png" alt="image-20220303215135802" style="zoom:70%;" />

<p>多智能体强化学习的纳什策略可改写为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220303215312.png" alt="image-20220303215312114" style="zoom:60%;" />

<p>根据每个智能体的奖励函数可以对随机博弈进行分类。若智能体的奖励函数相同，则称为完全合作博弈或团队博弈。若智能体的奖励函数逆号，则称为完全竞争博弈或零和博弈，也就是说多智能体的关系分为三种：完全竞争、完全合作、半竞争半合作</p>
<p>为了求解随机博弈，需要求解每个状态s的阶段博弈，每个阶段博弈的奖励值就是 <code>Qi(s,•)</code></p>
<h3 id="对比单体强化学习"><a href="#对比单体强化学习" class="headerlink" title="对比单体强化学习"></a>对比单体强化学习</h3><ul>
<li><p>环境的不稳定性</p>
<blockquote>
<p>智能体在做决策的同时，其他智能体也在采取动作；环境状态的变化与所有智能体的联合动作相关；</p>
</blockquote>
</li>
<li><p>智能体获取信息的局限性</p>
<blockquote>
<p>不一定能够获得全局的信息，智能体仅能获取局部的观测信息，但无法得知其他智能体的观测信息、动作和奖励等信息</p>
</blockquote>
</li>
<li><p>个体的目标一致性</p>
<blockquote>
<p>各智能体的目标可能是最优的全局回报；也可能是各自局部回报的最优；</p>
</blockquote>
</li>
<li><p>可拓展性</p>
<blockquote>
<p>在大规模的多智能体系统中，就会涉及到高维度的状态空间和动作空间，对于模型表达能力和真实场景中的硬件算力有一定的要求</p>
</blockquote>
</li>
</ul>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h1 id="基础求解算法"><a href="#基础求解算法" class="headerlink" title="基础求解算法"></a>基础求解算法</h1><p>多智能体强化学习过程基本上是求解一个随机博弈问题，但其实这两个概念还不能完全等价。因为随机博弈中假定每个状态的奖励矩阵是已知的，不需要学习；而多智能体强化学习则是通过与环境的不断交互来学习每个状态的奖励矩阵，再通过这些奖励矩阵来学习得到最优纳什策略。</p>
<p>也就是说，多智能体强化学习和基于价值优化单智能体学习的一样，都需要评估出准确的状态价值，然后根据价值进行决策。只不过单智能体拿到价值后直接采用贪婪或者 ε-greedy 这样的策略即可，但是多智能体没有现成的策略直接用，需要根据价值矩阵推算出一个纳什均衡策略来用（类似于DDPG，估计价值、设计策略）</p>
<p>多智能体强化学习算法中，两个主要的技术指标分别为合理性与收敛性：</p>
<ul>
<li><p>合理性（rationality）</p>
<blockquote>
<p>在对手使用一个恒定策略的情况下，当前智能体能够学习并收敛到一个相对于对手策略的最优策略</p>
</blockquote>
</li>
<li><p>收敛性（convergence）</p>
<blockquote>
<p>在其他智能体也使用学习算法时，当前智能体能够学习并收敛到一个稳定的策略</p>
<p>通常情况下，收敛性针对系统中的所有的智能体使用相同的学习算法</p>
</blockquote>
</li>
</ul>
<p>由于完全竞争、完全合作、半竞争半合作这3种不同场景的建模不同，求解方法也有所不同，需要分类求解，常用算法如下：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220310144235.png" alt="image-20220310144235073" style="zoom:70%;" />



<h2 id="直观思路"><a href="#直观思路" class="headerlink" title="直观思路"></a>直观思路</h2><p>对于多智能体强化学习问题，一种直接的解决思路：将单智能体强化学习方法直接套用在多智能体系统中，即每个智能体把其他智能体都当做环境中的因素，仍然按照单智能体学习的方式、通过与环境的交互来更新策略；这是 <strong>independent Q-learning</strong> 方法的思想。这种学习方式固然简单也很容易实现，但忽略了其他智能体也具备决策的能力、所有个体的动作共同影响环境的状态，使得它很难稳定地学习并达到良好的效果</p>
<p>另一种直观的思路则是，把其他智能体的状态或动作作为观测值用于自身智能体进行决策动作，即在单体强化学习的基础上，把其他智能体的信息作为观测内容</p>
<p>&nbsp;</p>
<h2 id="完全竞争关系"><a href="#完全竞争关系" class="headerlink" title="完全竞争关系"></a>完全竞争关系</h2><h3 id="MiniMax-Q"><a href="#MiniMax-Q" class="headerlink" title="MiniMax-Q"></a>MiniMax-Q</h3><blockquote>
<p>用于<strong>零和随机博弈</strong>，即两个智能体为完全竞争对抗关系 <code>R1=-R2</code> </p>
<ul>
<li><strong>Q</strong> 即指利用 Q-Learning 中的TD方法来迭代学习状态值函数或动作-状态值函数</li>
<li><strong>MiniMax</strong> 即指上一节中利用线性规划求解每个特定状态 s 的阶段博弈的纳什均衡策略</li>
</ul>
</blockquote>
<p><strong>算法思路</strong></p>
<p>在两玩家零和随机博弈中，给定状态 s，则定义第 i 个智能体的状态值函数为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220304104127.png" alt="image-20220304104127171" style="zoom:65%;" />

<p>其含义是，考虑其他智能体(-i)采取的动作(-a)令自己(i)回报最差(min)的情况下，能够获得最大(max)的回报，即当前智能体在考虑了对手策略的情况下使用贪心选择，这种方式使得智能体容易收敛到纳什均衡策略</p>
<p><code>Qi*(s,a_i,a_-i)</code> 是联结动作状态价值函数，若已知，则可以直接用线性规划求解出状态 s 处的纳什均衡策略。但是实际上它一开始肯定是未知的，所以需要用 Q-Learning 或其他方法来更新推算<code>Qi*(s,a_i,a_-i)</code> ，类似于单智能体强化学习中的基于价值的求解思想，一方面评估价值要准确，另一方面根据评估得到的价值矩阵推算纳什均衡策略</p>
<p><strong>算法流程</strong></p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220304142823.png" alt="image-20220304142823181" style="zoom:45%;" />

<p>理想情况，如果算法能够对每一个状态-动作对访问无限次，那么该算法能够收敛到纳什均衡策略</p>
<p><strong>算法缺点</strong></p>
<ol>
<li><p>在第5步中需要不断求解一个线性规划，降低了学习速度，增加计算时间</p>
</li>
<li><p>为了求解第5步，智能体 i 需要知道所有智能体的动作空间，这个在分布式系统中将无法满足</p>
</li>
<li><p>只满足收敛性，不满足合理性</p>
<p> Minimax-Q 算法能够找到多智能体强化学习的纳什均衡策略，但是假设对手使用的不是纳什均衡策略，而是一个较差的策略，则当前智能体并不能根据对手的策略学习到一个更优的策略。该算法无法让智能体根据对手的策略来调节优化自己的策略，而只能找到随机博弈的纳什均衡策略。这是由于Minimax-Q算法是一个对手独立算法（opponent-independent algorithm），不论对手策略是怎么样的，都收敛到该博弈的纳什均衡策略，所以如果对方不讲武德乱出牌，Minimax-Q效果就不好了</p>
</li>
</ol>
<p>&nbsp;</p>
<h2 id="半竞争半合作"><a href="#半竞争半合作" class="headerlink" title="半竞争半合作"></a>半竞争半合作</h2><h3 id="Nash-Q"><a href="#Nash-Q" class="headerlink" title="Nash Q"></a>Nash Q</h3><blockquote>
<p>将Minimax-Q算法从零和博弈扩展到了<strong>多人一般和博弈</strong></p>
<ul>
<li><strong>Q</strong> 即指利用 Q-Learning 中的TD方法来迭代学习状态值函数或动作-状态值函数</li>
<li><strong>Nash</strong> 没有特殊含义，Nash Q 使用二次规划求解纳什均衡点</li>
</ul>
</blockquote>
<p>Nash Q-Learning 是在随机博弈的每一个阶段博弈中找到一个纳什平衡点来使每一个智能体的状态价值在联合策略 Π 下取得最优值。在状态 s 处，纳什均衡策略表示为</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220310135446.png" alt="image-20220310135446524" style="zoom:70%;" />

<p>其使得对于任何 j ∈ [1,2,……,n]，值函数都满足：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220310135539.png" alt="image-20220310135539846" style="zoom:70%;" />

<p>其中，<code>Π-j</code> 表示除了 j 的联合策略。此算法的核心公式是</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220310135735.png" alt="image-20220310135735352" style="zoom:70%;" />

<p>在纳什平衡点处，每个智能体的策略是在其他智能体策略下的最优策略。Nash-Q的核心就是在每个状态s处的阶段博弈中找到纳什均衡，这样可以保证策略在特定情况下收敛。收敛性条件是，在每一个状态 s 的阶段博弈中，都能够找到一个全局最优点或者鞍点，只有满足这个条件，Nash Q-Learning 算法才能够收敛。但同样因为每轮都要求解二次规划，非常耗时，降低了算法的学习速度</p>
<p><strong>算法流程</strong></p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220304142844.png" alt="image-20220304142844901" style="zoom:45%;" />

<p>单个智能体 i 在使用 Nash Q 值进行更新时，除了需要知道全局状态 s 和其他智能体的动作 a 以外，还需要知道其他所有智能体在下一状态对应的纳什均衡策略 π。进一步地，当前智能体就需要知道其他智能体的 Q(s’) 值。所以，Nash Q-learning 方法对智能体能够获取的其他智能体的信息（包括动作、奖励等）具有较强的假设，在复杂的真实问题中一般不满足这样严格的条件，方法的适用范围受限</p>
<h3 id="Friend-or-Foe-Q"><a href="#Friend-or-Foe-Q" class="headerlink" title="Friend-or-Foe Q"></a>Friend-or-Foe Q</h3><blockquote>
<p>Friend-or-Foe Q-Learning（FFQ）算法也是从Minimax-Q算法拓展而来，巧妙地将<strong>n智能体的一般和博弈</strong>转化为两智能体的零和博弈</p>
</blockquote>
<p>FFQ 算法对一个智能体 i，将其他所有智能体分为两组，一组为 i 的 friend，帮助 i 一起最大化其奖励回报；另一组为 i 的 foe ，对抗 i 并降低 i 的奖励回报。因此对每个智能体而言都有两组。这样一个<strong>n智能体的一般和博弈</strong>就转化为了一个两智能体的零和博弈</p>
<p>其纳什均衡策略求解方法为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220304140123.png" alt="image-20220304140123497" style="zoom:65%;" />

<p><strong>算法流程</strong></p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220304151241.png" alt="image-20220304151241784" style="zoom:45%;" />

<p>有一种利用 Minimax-Q 算法进行多人博弈方法为：两队零和博弈，将所有智能体分成两个小组进行零和博弈。两队零和博弈中每一组只有一个 leader 才控制这一队智能体的所有策略，获取的奖励值也是这一个小组的整体奖励值</p>
<p>FFQ 算法没有 team learder，每个人选择自己动作学习自己的策略获得自己的奖励值，但是为了更新 Q 值，每个智能体需要在每一步观测其他所有 friend 与 foe 的执行动作</p>
<p>FFQ 与 Minimax-Q 算法一样都需要利用线性规划，因此算法整体学习速度会变慢</p>
<h3 id="WoLF-PHC"><a href="#WoLF-PHC" class="headerlink" title="WoLF-PHC"></a>WoLF-PHC</h3><blockquote>
<p>上述的三种方法都需要在学习过程中维护Q函数，假设动作空间 Ai 与状态空间 S 都是离散，假设每个智能体的动作空间相同，则对于每一个智能体都需要有一个 S×An 大小的空间来存储Q值，因此上述三种方法所需空间非常大</p>
<p>为了解决上述问题，我们期望每个智能体只用知道自己的动作来维护Q值函数，这样空间就降到了 S×A WoLF-PHC 就是这样的算法，每个智能体只用保存自己的动作来完成学习任务</p>
</blockquote>
<p><strong>WoLF</strong></p>
<p>Win or Learn Fast，当智能体做的比期望值好的时候小心缓慢的调整参数，当智能体做的比期望值差的时候，加快步伐调整参数</p>
<p><strong>PHC</strong></p>
<p>一种单智能体在稳定环境下的学习算法，其核心是强化学习的通常思想，增大能够得到最大累积期望的动作的选取概率。具有合理性，能够收敛到最优策略，流程如下：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220304151327.png" alt="image-20220304151327291" style="zoom:45%;" />

<center>最后一行 δ 选取 δl 还是 δw 由实际情况决定</center>



<p><strong>WoLF-PHC</strong></p>
<ul>
<li>为了将 PHC 应用于动态环境中，将WoLF与PHC算法结合，使得智能体获得的奖励在比预期差时能够快速调整适应其他智能体策略变化；当比预期好时谨慎学习，给其他智能体适应策略变化的时间</li>
<li>WoLF-PHC 算法能够收敛到纳什均衡策略，并具备合理性，当其他智能体采用某个固定策略时，其也能收敛到一个目前状况下的最优策略，而不是不管效果好坏无脑收敛到一个纳什均衡策略处</li>
<li>在 WoLF-PHC 算法中，使用一个可变的学习速率 <code>δ</code> 来实现WoLF效果，当策略效果较差时使用 <code>δl</code> ，策略效果较好时使用 <code>δw</code> ，并且满足 <code>δl</code> &gt; <code>δw</code> 。</li>
<li>WoLF-PHC 算法还有一个优势是不用观测其他智能体的策略、动作及奖励值，需要更少的空间去记录Q值</li>
<li>WoLF-PHC 算法是通过 PHC 算法进行学习改进策略的，所以不需要使用线性规划或者二次规划求解纳什均衡，算法速度得到了提高</li>
<li>虽然 WoLF-PHC 算法在实际应用中取得了非常好的效果，并且能够收敛到最优策略，但是其收敛性一直没有理论证明</li>
</ul>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220304154558.png" alt="image-20220304154558146" style="zoom:50%;" />

<p>&nbsp;</p>
<p>&nbsp;</p>
<h2 id="完全合作关系"><a href="#完全合作关系" class="headerlink" title="完全合作关系"></a>完全合作关系</h2><p>完全合作意味着多个智能体要共同完成同一个目标，这就涉及是否需要协作的问题。如果最优联合动作只有一个，那么就不需要协作，如果有多个最优联合动作，那么必须通过协作保持步调一致而不是走向不同优化方向</p>
<h3 id="不需要协作机制"><a href="#不需要协作机制" class="headerlink" title="不需要协作机制"></a>不需要协作机制</h3><h4 id="Team-Q-Learning、"><a href="#Team-Q-Learning、" class="headerlink" title="Team Q-Learning、"></a>Team Q-Learning、</h4><p>对每个智能体 i，最优动作通过下式选择</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220310105444.png" alt="image-20220310105444844" style="zoom:70%;" />

<h4 id="Distribute-Q-Learning"><a href="#Distribute-Q-Learning" class="headerlink" title="Distribute Q-Learning"></a>Distribute Q-Learning</h4><p>不同于 Team Q-learning 在选取个体最优动作的时候需要知道其他智能体的动作，Distribute Q-Learning 方法中智能体维护的是只依据自身动作所对应的 Q 值，从而得到个体最优动作</p>
<h3 id="需要协作机制"><a href="#需要协作机制" class="headerlink" title="需要协作机制"></a>需要协作机制</h3><p>在智能体之间需要相互协商、从而达成最优的联合动作的问题中，<strong>个体之间的相互建模</strong>，能够为智能体的决策提供潜在的协调机制</p>
<h4 id="JAL"><a href="#JAL" class="headerlink" title="JAL"></a>JAL</h4><p>智能体 i 会基于观察到的其他智能体 j 的历史动作、对其他智能体 j 的策略进行建模</p>
<h4 id="FMQ"><a href="#FMQ" class="headerlink" title="FMQ"></a>FMQ</h4><p>在个体 Q 值的定义中引入了个体动作所在的联合动作取得最优回报的频率，从而在学习过程中引导智能体选择能够取得最优回报的联合动作中的自身动作，那么所有智能体的最优动作组合被选择的概率也会更高</p>
<p>JAL 和 FMQ 方法的基本思路都是基于均衡求解法，但这类方法通常只能处理小规模（即智能体的数量较少）的多智能体问题。但是在现实问题中，会涉及到大量智能体之间的交互和相互影响，状态空间跟动作空间随着智能体数量的增多而迅速扩大，而一般的均衡求解法受限于计算效率和计算复杂度，很难处理这种情况。在<strong>大规模多智能体学习问题</strong>中，考虑群体联合动作的效应，包括当前智能体受到的影响以及在群体中发挥的作用</p>
<h4 id="MFMARL"><a href="#MFMARL" class="headerlink" title="MFMARL"></a>MFMARL</h4><p>基于平均场理论的多智能体强化学习 <a target="_blank" rel="noopener" href="https://lrk612.com/resources/Mean%20Field%20Multi-Agent%20Reinforcement%20Learning.pdf">Mean Field Multi-Agent Reinforcement Learning</a> 是 UCL 计算机科学系教授汪军在 2018 年 ICML 会议上提出的一种针对大规模群体问题的方法，它将传统强化学习方法（Q-learning）和平均场理论（mean field theory）相结合</p>
<p>平均场理论适用于对复杂的大规模系统建模，它使用了一种简化的建模思想：对于其中的某个个体，所有其他个体产生的联合作用可以用一个 “平均量” 来定义和衡量。即对某个智能体，其他所有智能体对其产生的作用可以用一个均值替代。这样就就将一个智能体与其邻居智能体之间的相互作用简化为两个智能体之间的相互作用（该智能体与其所有邻居的均值）</p>
<p>MFMARL 将值函数 Q 转化为只包含邻居节点之间1相互作用的形式：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220310140130.png" alt="image-20220310140130368" style="zoom:70%;" />

<p>其中，N(j) 表示智能体 j 邻居智能体的个数，该式对智能体之间的交互做了近似，降低了表示智能体交互的复杂度，并且保留了主要的交互作用。虽然对联合动作 a 做了近似，但是状态信息 s 仍然是一个全局信息</p>
<p>MFMARL 与DQN、AC结合可得到两种具体算法：</p>
<ul>
<li>MF-DQN</li>
<li>MF-AC</li>
</ul>
<p>&nbsp;</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><code>MiniMax-Q</code>、<code>Nash Q</code>、<code>Friend-or-Foe Q</code> 算法的本质是通过训练更新Q表从而准确评估得到状态价值矩阵，然后对每个状态的价值矩阵进行博弈，求出每个状态的纳什均衡策略作为最佳策略</p>
<p><code>WoLF-HPC</code> 比较像单智能体的Q-Learning</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h1 id="深度学习求解算法（多智能体深度强化学习）"><a href="#深度学习求解算法（多智能体深度强化学习）" class="headerlink" title="深度学习求解算法（多智能体深度强化学习）"></a>深度学习求解算法（多智能体深度强化学习）</h1><p>随着深度学习的发展，利用神经网络的强大表达能力来搭建价值模型（value approximation）和策略模型（常见于 policy-based 的 DRL 方法）。深度强化学习的方法可以分为基于值函数（value-based）和基于策略（policy-based）两种，在考虑多智能体问题时，主要的方式是在值函数的定义或者是策略的定义中引入多智能体的相关因素，并设计相应的网络结构作为值函数模型和策略模型，最终训练得到的模型能够适应（直接或者是潜在地学习到智能体相互之间的复杂关系）</p>
<h2 id="Policy-based-方法"><a href="#Policy-based-方法" class="headerlink" title="Policy-based 方法"></a>Policy-based 方法</h2><h3 id="MADDPG"><a href="#MADDPG" class="headerlink" title="MADDPG"></a>MADDPG</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>MADDPG 是 OpenAI 2017发表在 NIPS 上的一篇文章 <a target="_blank" rel="noopener" href="https://lrk612.com/resources/MADDPG.pdf">Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments </a> 中提出的，主要是将AC算法进行了一系列改进，使其能够适用于传统RL算法无法处理的复杂多智能体场景</p>
<p>传统RL算法在多智能体场景下面临的问题有：</p>
<ul>
<li>由于每个智能体都是在不断学习改进其策略，因此从每一个智能体的角度看，环境是一个动态不稳定的，不符合传统RL收敛条件</li>
<li>在一定程度上，无法通过仅仅改变智能体自身的策略来适应动态不稳定的环境</li>
<li>由于环境的不稳定，将无法直接使用之前的经验回放等DQN的关键技巧</li>
<li>policy gradient 算法会由于智能体数量的变多使得本就有的方差大的问题加剧</li>
</ul>
<p>MADDPG 的三点技巧：</p>
<ol>
<li><p>集中式训练，分布式执行</p>
<p> 训练时采用集中式学习训练 critic 与 actor，使用时 actor 只用知道局部信息就能运行。critic 需要其他智能体的策略信息，文章给了一种估计其他智能体策略的方法，能够只用知道其他智能体的观测与动作</p>
</li>
<li><p>改进了经验回放记录的数据</p>
<p> 为了能够适用于动态环境，每一条信息由 <code>(x,x&#39;,aq,…,an,r1,…,rn)</code> 组成，<code>x=(o1,…,on)</code> 表示每个智能体的观测</p>
</li>
<li><p>利用策略集合效果优化（policy ensemble）</p>
<p> 对每个智能体学习多个策略，改进时利用所有策略的整体效果进行优化，以提高算法的稳定性以及鲁棒性</p>
</li>
</ol>
<p>MADDPG 本质上还是一个 DPG 算法，针对每个智能体训练一个需要全局信息的 Critic 以及一个需要局部信息的 Actor，并且允许每个智能体有自己的奖励函数（reward function），因此可以用于合作任务或对抗任务，并且由于脱胎于 DPG 算法，其动作空间可以是连续的</p>
<p>&nbsp;</p>
<h4 id="传统-DRL-思路"><a href="#传统-DRL-思路" class="headerlink" title="传统 DRL 思路"></a>传统 DRL 思路</h4><p><strong>DQN</strong></p>
<p>DQN的思想就是设计一个 <code>Q(s,a|θ)</code> 不断逼近真实的 <code>Q(s,a)</code> 函数。其中主要用到了两个技巧：</p>
<ol>
<li>经验回放</li>
<li>目标网络</li>
</ol>
<p>该技巧主要用来打破数据之间联系，因为神经网络对数据的假设是独立同分布，而MDP过程的数据前后有关联。打破数据的联系可以更好地拟合 <code>Q(s,a)</code> 函数。其代价函数为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306205629.png" alt="image-20220306205629582" style="zoom:75%;" />

<p>其中计算y时使用的网络是目标Q网络，其参数更新与当前Q网络不同步（滞后）</p>
<p><strong>SPG</strong></p>
<p>stochastic policy gradient 算法不采用拟合Q函数的方式，而是直接优化累积回报来获得使回报最大的策略。假定参数化的策略为 <code>Πθ(a|s)</code>，累积回报为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306205844.png" alt="image-20220306205844149" style="zoom:75%;" />

<p>为了使 <code>J(θ)</code> 最大化，直接对策略参数求导得到策略更新梯度：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306205936.png" alt="image-20220306205936394" style="zoom:75%;" />

<p>Actor-Critic 算法也可由 SPG + DQN 共同推出，按照DQN的方法拟合一个 <code>Q(s,a|θ)</code> 函数，则这个参数化的 <code>Q(s,a|θ)</code> 函数被称为 Critic，<code>Πθ(a|s)</code>  被称为 Actor</p>
<p><strong>DPG</strong></p>
<p>DQN、SPG 都是针对随机策略的，<code>Πθ(a|s)</code>  是一个在状态 s 对于各个动作 a 的条件概率分布。而 DPG 是针对确定性策略的，<code>μ(a|θ): S——&gt;A</code> 是一个状态空间到动作空间的映射。其思想与 SPG 相同，策略梯度公式为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306210928.png" alt="image-20220306210927967" style="zoom:75%;" />

<p>DPG 可以使用 AC 的方法来估计一个Q函数，DDPG 就是借用了 DQN 经验回放与目标网络的技巧</p>
<h4 id="MADDPG-1"><a href="#MADDPG-1" class="headerlink" title="MADDPG"></a>MADDPG</h4><h5 id="多智能体-Actor-Critic-设计"><a href="#多智能体-Actor-Critic-设计" class="headerlink" title="多智能体 Actor-Critic 设计"></a>多智能体 Actor-Critic 设计</h5><p>MADDPG是集中式的学习，分布式的应用，允许使用一些额外的信息（全局信息）进行学习，只要在应用的时候使用局部信息进行决策就行。这点就是Q-learning的一个不足之处，Q-learning在学习与应用时必须采用相同的信息。</p>
<p>MADDPG对传统的AC算法进行了一个改进，Critic扩展为可以利用其他智能体的策略进行学习，这点的进一步改进就是每个智能体对其他智能体的策略进行一个函数逼近</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220314111520.png" alt="image-20220314111520535" style="zoom:50%;" />

<hr>
<p>用 <code>θ=[θ1,…,θn]</code> 表示 n 个智能体策略的参数，<code>Π=[Π1,…,Πn]</code> 表示 n 个智能体的策略。第 i 个智能体的累计期望奖励为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306211805.png" alt="image-20220306211805156" style="zoom:70%;" />

<p>求导的策略梯度：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306211823.png" alt="image-20220306211823813" style="zoom:75%;" />

<p>其中，<code>oi</code> 表示第 i 个智能体的观测，<code>x=[o1,……,on]</code> 表示观测向量，即总状态；第 i 个智能体集中式的状态-动作函数为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306211941.png" alt="image-20220306211940984" style="zoom:70%;" />

<p>由于是每个智能体独立学习自己的状态-动作函数，所以每个智能体可以有不同的奖励函数，完成合作或竞争任务</p>
<hr>
<p>上述为随机策略梯度算法，下面拓展到确定性策略 <code>μθ</code>，梯度公式为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306212139.png" alt="image-20220306212139199" style="zoom:75%;" />

<p>由以上两个梯度公式可以看出该算法与SPG与DPG十分类似，就像是将单体直接扩展到多体。但其实 <code>Qμ</code> 是一个非常厉害的技巧，针对每个智能体建立值函数，极大的解决了传统 RL 算法在 Multi-agent 领域的不足。D 是一个经验池（experience replay buffer），元素组成为 <code>(x,x&#39;,a1,…,an,r1,…,rn)</code> </p>
<p>集中式的 critic 的更新方法借鉴了DQN中TD与目标网络思想：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306212411.png" alt="image-20220306212411756" style="zoom:90%;" />

<center>式 1</center>

<p>其中，y 的计算使用目标Q网络，<code>μ’=[μ’1,…,μ’n]</code> 就是目标策略滞后更新的参数 <code>θ’</code>，其他智能体的策略可以采用拟合逼近的方式得到，不需要通信交互</p>
<p>如上可以看出 critic 借用了全局信息学习，actor 只是用了局部观测信息。MADDPG 的一个启发就是，如果知道所有的智能体的动作，那么环境就是稳定的，就算策略在不断更新环境也是恒定的</p>
<h5 id="估计其他智能体策略"><a href="#估计其他智能体策略" class="headerlink" title="估计其他智能体策略"></a>估计其他智能体策略</h5><p>在 (1) 式中用到了其他智能体的策略，这本需要不断的通信来获取，但是也可以放宽这个条件，通过对其他智能体的策略进行估计来实现</p>
<p>每个智能体维护 n-1 个策略逼近函数 <code>μ^iΦj</code> 表示第 i 个智能体对第j个智能体策略 <code>μj</code> 的函数逼近。其逼近代价为 对数代价函数 加上 策略的熵，可写为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306213205.png" alt="image-20220306213205761" style="zoom:75%;" />

<p>只要最小化上述代价函数，就能得到其他智能体策略的逼近，从而 (1) 式中的 y 可计算为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306213321.png" alt="image-20220306213321401" style="zoom:78%;" />

<p>所以 MADDPG 在更新Critic网络 <code>Qμ</code> 之前，先利用经验回放的一个采样batch更新其他智能体策略预测网络 <code>μ^iΦj</code></p>
<h5 id="策略集合优化（policies-ensemble）"><a href="#策略集合优化（policies-ensemble）" class="headerlink" title="策略集合优化（policies ensemble）"></a>策略集合优化（policies ensemble）</h5><p>这个技巧也是文章的一个亮点，多智能体强化学习一个顽固的问题是由于每个智能体的策略都在更新迭代导致环境针对一个特定的智能体是动态不稳定的。这种情况在竞争任务下尤其严重，经常会出现一个智能体针对其竞争对手过拟合出一个强策略。但是这个强策略是非常脆弱的，因为随着竞争对手策略的更新改变，这个强策略很难去适应新的对手策略</p>
<p>为了能更好的应对上述情况，MADDPG提出了一种策略集合的思想，第i个智能体的策略 <code>μi</code> 由一个具有 <code>K</code> 个子策略的集合构成，在每一个训练 episode 中只是用一个子策略 <code>μi(k)</code> 。对每一个智能体，最大化其策略集合的整体奖励：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306213930.png" alt="image-20220306213930276" style="zoom:75%;" />

<p>并且为每一个子策略k构建一个记忆存储 <code>Di(k)</code> ，优化策略集合的整体效果，由此针对每一个子策略的更新梯度为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306214020.png" alt="image-20220306214020321" style="zoom:70%;" />



<h3 id="仿真环境"><a href="#仿真环境" class="headerlink" title="仿真环境"></a>仿真环境</h3><p>这篇文章另一个贡献是提供了其仿真环境源代码：<a target="_blank" rel="noopener" href="https://github.com/openai/multiagent-particle-envs">openai/multiagent-particle-envs</a> 。后期很多关于多智能体强化学习的仿真实验都是基于该仿真环境做的研究，包括大量顶会和顶刊上的论文。虽然环境的设置和任务比较简单，但它足够称得上是MARL领域最经典的仿真环境了</p>
<p>该仿真环境模拟的是一个二维平面小球碰撞场景，每个小球可以代表智能体、障碍、目标等。它们的观测信息是位置、速度等物理量，有的时候也包括通信信息。动作维度一共是五维（上下左右），但也可以改成二维（水平方向和垂直方向）。GitHub上有不同的任务的详细介绍，也可以根据自己的任务需要进行修改</p>
<h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>本文以比较直接的形式将DDPG算法扩展到多智能体强化学习中，通过<strong>集中式训练分布式执行</strong>的思路，计算出每个智能体的最优策略。有人觉得在论文贡献上，仿真实验的重要性远大于算法设计的本身</p>
<p>此外，当智能体数量比较大的时候，该算法的计算规模将会非常大。因为每个智能体都对应两个actor网络（另一个是target actor网络）和两个critic网络（另一个是target critic网络）。如果再加上对其它智能体策略的估计，或者策略集合的训练，更是如此</p>
<p>&nbsp;</p>
<h3 id="COMA"><a href="#COMA" class="headerlink" title="COMA"></a>COMA</h3><h4 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h4><p>在合作式的多智能体学习问题中，每个智能体共享奖励（即在同一个时刻获得相同的奖励），此时会存在一个<strong>置信分配</strong>问题（credit assignment）：如何去评估每个智能体对这个共享奖励的贡献？</p>
<blockquote>
<p>在协作任务下的多智能体系统中，奖励函数是全局共享的，所以只需要一个critic网络就可以了。但是那样的话，所有的智能体优化目标都是同一个。因此，对每个actor网络来说，从critic那边反向传播过来的误差都是一样的</p>
<p>这种“吃大锅饭”式的训练方式显然不是最有效的，因为每个actor网络对整体性能的贡献不一样，贡献大的反向传播的误差应该要稍微小些，贡献小的其反向传播误差应该要大一些。最终的目标都是优化整体的性能。所以，作者提出利用<strong>counterfactual baseline</strong>来解决该问题</p>
</blockquote>
<p>反事实多智能体策略梯度法方法是牛津大学学者在论文 <a target="_blank" rel="noopener" href="https://lrk612.com/resources/Counterfactual%20Multi-Agent%20Policy%20Gradients.pdf">Counterfactual Multi-Agent Policy Gradients</a> 中提出的，其在置信分配中利用了一种反事实基线：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">将智能体当前的动作和默认的动作进行比较，如果当前动作能够获得的回报高于默认动作，则说明当前动作提供了好的贡献，反之则说明当前动作提供了坏的贡献；默认动作的回报，则通过当前策略的平均效果来提供（即反事实基线）</span><br></pre></td></tr></table></figure>



<h4 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h4><p>对智能体 <code>a</code> ，用 <code>U_a</code> 表示其动作，<code>τ_a</code> 是其历史观测-动作序列。<code>u</code> 表示所有智能体的联合动作，<code>u_-a</code> 表示所有其它智能体（除了智能体 <code>a</code> ）的联合动作，<code>s</code> 代表系统全局状态，则智能体 <code>a</code> 的优势函数可由下式计算：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220314094130.png" alt="image-20220314094130485" style="zoom:70%;" />

<p>等式右边第二项实际上是在计算关于Q的期望值（反事实基线），需要遍历智能体 <code>a</code> 动作空间里的所有动作，而保持其它智能体动作 <code>u_-a</code> 不变</p>
<hr>
<p>有了目标函数，下一步就是计算梯度并优化更新了。COMA的策略梯度计算公式如下：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220314094348.png" alt="image-20220314094348132" style="zoom:70%;" />

<p>其中，k为迭代次数，θk 为第k次迭代时的参数</p>
<p>此外，<code>A_a(s,u)</code> 的更新采用TD(λ)方法，并使用周期性更新参数的 target 网络来计算目标值</p>
<hr>
<p>COMA 方法结合了集中式训练、分布式执行的思想：分布式的个体策略以局部观测值为输入、输出个体的动作；中心化的 critic 使用特殊的网络结构来输出优势函数值</p>
<p>具体地，critic网络的输入包括其它智能体的动作 <code>U_-t_a</code> ，全局状态 <code>s_t</code> ，该智能体的局部观测 <code>o_a_t</code> ，该智能体的ID，以及上一时刻所有智能体的动作 <code>u_t-1</code> 。c 在输出端直接得到该智能体各个动作的反事实Q值</p>
<p>然后， 再经过 COMA 模块，使用输入其中的智能体当前策略和动作，计算反事实基线以及输出最终的优势函数</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220310142821.png" alt="image-20220310142821585" style="zoom:70%;" />

<center>(a) actor-critic框架   (b)actor网络结构   (c)critic网络结构，其中COMA模块提供优势函数值</center>



<h4 id="对比MADDPG"><a href="#对比MADDPG" class="headerlink" title="对比MADDPG"></a>对比MADDPG</h4><p>COMA同MADDPG算法一样，都是基于策略梯度的算法，都是基于“集中式训练分布式执行”的学习算法，不同的地方在于：</p>
<ol>
<li>COMA针对离散动作，学习的是<strong>随机策略</strong>。而MADDPG针对连续动作，学习的是确定性策略。这在它们策略梯度函数的表达式上能够体现出区别</li>
<li>COMA主要针对多智能体<strong>协作任务</strong>，因此只有一个critic评价团队整体的回报。MADDPG既可以做协作任务，也可以做竞争任务，每个智能体都对应一个奖励函数，因此每个智能体对应一个critic</li>
<li>在原文中，COMA使用了<strong>历史观测、动作序列</strong>作为网络的输入，而MADDPG没有使用历史序列。因此COMA的网络结构中包含了GRU层，而MADDPG的网络均为2-3个隐层的MLP。当然，这个不是特别重要，针对实际问题时，可以比较灵活一些</li>
<li>COMA中所有智能体的actor网络<strong>共享参数</strong>，输入端加上智能体ID以示区别。而MADDPG则没有共享参数</li>
<li>COMA使用<strong>反事实基线</strong>作为actor网络的优化目标，而MADDPG直接使用Q函数作为每个智能体actor网路的优化目标</li>
</ol>
<p>&nbsp;</p>
<h2 id="Value-based-方法"><a href="#Value-based-方法" class="headerlink" title="Value-based 方法"></a>Value-based 方法</h2><p>policy-based 方法中，中心化的值函数是直接使用全局信息进行建模，没有考虑个体的特点。在多智能体系统是由大规模的多个个体构成时，这样的值函数是难以学习或者是训练到收敛的，很难推导出理想的策略。并且仅依靠局部观测值，无法判断当前奖励是由于自身的行为还是环境中其他队友的行为而获得的</p>
<h3 id="VDN"><a href="#VDN" class="headerlink" title="VDN"></a>VDN</h3><h4 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h4><p>值分解网络（value decomposition networks, VDN）由 DeepMind 团队在 2018 年的论文 <a target="_blank" rel="noopener" href="https://lrk612.com/resources/Value-Decomposition%20Networks%20For%20Cooperative%20Multi-Agent%20Learning.pdf">Value-Decomposition Networks For Cooperative Multi-Agent Learning</a> 中提出的。和 COMA 一样，VDN 是考虑协作任务的多智能体强化学习问题，即所有的智能体共享同一个奖励值（或者叫团队奖励），不同的是VDN是一种基于值函数的方法，而COMA基于策略梯度</p>
<p>在 <a href="#COMA">COMA</a> 提到过，智能体共享团队奖励会带来**credit assignment (信用分配)**问题，即利用该团队奖励拟合出的值函数不能评价每个智能体的策略对整体的贡献，在 VDN 中从另一个角度考虑了这个问题</p>
<p>作者认为，由于每个智能体都是局部观测，那么对其中一个智能体来说，其获得的团队奖励很有可能是其队友的行为导致的。也就是说该奖励值对该智能体来说，是**虚假奖励 (spurious reward signals)**。因此，每个智能体独立使用强化学习算法学习 (即independent RL) 往往效果很差。这种虚假奖励还会伴随一种现象，作者称作 **lazy agent (惰性智能体)**。当团队中的部分智能体学习到了比较好的策略并且能够完成任务时，其它智能体不需要做什么也能获得不错的团队奖励，这些智能体就被称作“惰性智能体”。</p>
<p>其实不管是“虚假奖励”还是“惰性智能体”，本质上还是 credit assignment 问题。如果每个智能体都会根据自己对团队的贡献，优化各自的目标函数，就能够解决上述问题。基于这样的动机，作者提出了<strong>值函数分解</strong>的研究思路，将团队整体的值函数分解成 N 个子值函数，分别作为各智能体执行动作的依据</p>
<h4 id="核心思想-1"><a href="#核心思想-1" class="headerlink" title="核心思想"></a>核心思想</h4><p>假设 <code>Q((h1,…,hd),(a1,…,ad))</code> 是多智能体团队的整体Q函数，<code>d</code> 是智能体个数， <code>h_i</code> 是智能体 <code>i</code> 的历史序列信息， <code>a_i</code> 是其动作。该Q函数的输入集中了所有智能体的观测和动作，可通过团队奖励 <code>r</code> 来迭代拟合。文章取各个智能体的子值函数的<strong>直接求和</strong>做为整体Q函数：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220314100438.png" alt="image-20220314100438050" style="zoom:70%;" />

<p>每个子Q函数的输入为该对应智能体的局部观测序列和动作，互相不受影响。这样的分解方式在联合动作 Q 值的结构组成方面考虑了个体行为的特性，使得该 Q 值更易于学习</p>
<p>另一方面，它也能够适配集中式的训练方式，在一定程度上能够克服多智能体系统中环境不稳定的问题</p>
<hr>
<p>在训练过程中，通过联合动作 Q 值来指导策略的优化，同时每个智能体用对应的子值函数来完成其决策（如贪心策略：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220314100735.png" alt="image-20220314100735075" style="zoom: 70%;" />

<p>实现多智能体系统的分布式控制</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220310143539.png" alt="image-20220310143539248" style="zoom:60%;" />

<center>左图是完全分布式的局部Q网络结构，右图是VDN联合动作Q值网络结构</center>



<h4 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h4><p>VDN算法结构简洁，通过它分解得到的 <code>Qi</code> 可以让智能体根据自己的局部观测选择贪婪动作，从而执行分布式策略，其集中式训练方式能够在一定程度上保证整体Q函数的最优性</p>
<p>但是对于一些比较大规模的多智能体优化问题，它的学习能力将会大打折扣。其根本限制在于缺少值函数分解有效性的理论支持。VDN以简单的求和方式将整体Q函数完全分解开，使得多智能体Q网络的拟合能力很受限制</p>
<p>在 2018 年的 ICML 会议上，有研究者提出了改进的方法 QMIX</p>
<h3 id="QMIX"><a href="#QMIX" class="headerlink" title="QMIX"></a>QMIX</h3><h4 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h4><p>QMIX算法由牛津大学的*<a href="https://link.zhihu.com/?target=http://whirl.cs.ox.ac.uk/">Whiteson Research Lab</a>团队*和俄罗斯-亚美尼亚大学（共同一作）合作完成，发表于ICML 2018：<a target="_blank" rel="noopener" href="https://lrk612.com/resources/QMIX%20Monotonic%20Value%20Function%20Factorisation%20for%20Deep%20Multi-Agent%20Reinforcement%20Learning.pdf">QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning</a>，是多智能体强化学习领域广为人知的算法</p>
<p>VDN 中简单的求和方式对整体值函数进行分解，将会导致网络的函数表达能力受到很大限制，难以拟合出真实的 <code>Q_tot</code>。但是如果直接使用普通的<strong>神经网络</strong>对 <code>Q-tot</code> 进行分解，例如 <code>Q_tot=MLP(Q1,…,Qn)</code> ，倒是可以提升网络的函数表达能力，但会遇到另外一个问题：**非单调性（non-monotonicity)**，导致算法难以保证分布式策略的最优性</p>
<p>所谓<strong>单调性</strong>，就是指通过分布式策略计算出来的动作，和通过整体Q函数计算出来的动作，在“性能最优”上需要保持一致，即：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220314102144.png" alt="image-20220314102144012" style="zoom:60%;" />

<center>条件 1</center>

<p>其中，<code>τ</code> 表示历史观测-动作序列，<code>u</code> 表示动作。如果 (1) 式不成立，则分布式策略就不能使 <code>Q_tot</code> 最大化，自然不会是最优策略。而想要满足上式，一个充分不必要条件就是：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220307193631.png" alt="image-20220307193631239" style="zoom:70%;" />

<center>条件 2</center>

<p>因此，QMIX 的研究目标就是设计一个神经网络，输入为所有 agent 的 <code>&#123;Qi&#125;</code> ，输出总Q值 <code>Q_tot</code> ，强制其满足上式中的单调性约束。只要在满足该约束的前提下进行探索，就可以既保证单调性成立，又能够增强网络的函数拟合能力，从而弥补VDN算法的不足</p>
<h4 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h4><p><strong>核心问题</strong></p>
<p>在多智能体强化学习中一个关键的问题就是如何学习<strong>联合动作值函数</strong>，因为该函数的参数会随着智能体数量的增多而成指数增长，如果动作值函数的输入空间过大，则很难拟合出一个合适函数来表示真实的联合动作值函数。</p>
<p>另一个问题就是学得了联合动作值函数后，如何通过联合值函数提取出一个优秀的分布式的策略，这其实是单智能体强化学习拓展到MARL的<strong>核心问题</strong></p>
<p><strong>Dec-POMDP</strong></p>
<p>非中心部分可观马尔可夫决策过程（Dec-POMDP）是一种处理多智能体序列化决策问题的模型，是将POMDP拓展到多智能体系统。其中定义如下：</p>
<ul>
<li>每个智能体的局部观测信息 <code>o_i,t</code> 、动作 <code>a_i,t</code> 、系统状态为 <code>s_t</code> </li>
<li>每个智能体的动作-观测历史为 <code>τi=(ai_0,oi_1,ai_1,oi_2,……,ai_t-1,oi_t)</code>，表示从初始状态开始，该智能体的时序动作-观测记录</li>
<li>联合动作-观测历史 <code>τ=(τ1,τ2,……,τn)</code>，表示从初始状态开始，所有智能体的时序动作-观测记录</li>
<li>每个智能体的分布式策略为 <code>Πi(τi)</code>，其值函数（局部动作值函数）为 <code>Qi(τi,ai;θi)</code>，都与动作-观测历史 <code>τi</code> 有关而不和状态有关</li>
</ul>
<p><strong>IQL</strong></p>
<p>IQL（independent Q-learning）就是非常暴力的给每个智能体执行一个Q-learning算法，因为共享环境，并且环境随着每个智能体策略、状态发生改变，对每个智能体来说，环境是动态不稳定的，因此这个算法也无法收敛，但是在部分应用中也具有较好的效果</p>
<p><strong>DRQN</strong></p>
<p>DRQN是一个用来处理POMDP（部分可观马尔可夫决策过程）的一个算法，其采用LSTM替换DQN卷基层后的一个全连接层，来达到能够记忆历史状态的作用，因此可以在部分可观的情况下提高算法性能</p>
<p>由于QMIX解决的是多智能体的POMDP问题，因此每个智能体采用的是DRQN算法</p>
<h4 id="QMIX-1"><a href="#QMIX-1" class="headerlink" title="QMIX"></a>QMIX</h4><p>QMIX就是采用一个混合网络对单智能体局部值函数进行合并，并在训练学习过程中加入全局状态信息辅助，来提高算法性能</p>
<p>实现单调性的约束，QMIX 采用由 n 个智能体网络 (<strong>Agent networks</strong>)，一个混合网络 <strong>(Mixing network)</strong> 和一组超网络 (<strong>Hypernetworks</strong>) 来表达，其具体结构为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220307194034.png" alt="image-20220307194034120" style="zoom:70%;" />

<p>图（c）为<strong>智能体网络</strong>，表示每个智能体采用一个 DRQN 来拟合自身的Q值函数的到 <code>Qi(τi,ai;θi)</code> ，DRQN 循环输入当前的观测 <code>o_i,t</code> 以及上一时刻的动作 <code>a_i,t-1</code> （<strong>历史动作-观测序列信息</strong>）来得到 Q 值，然后使用 ε-greedy 策略获取动作。DRQN网络结构为：MLP+GRU+MLP</p>
<p>图（b）为<strong>混合网络</strong>，表示混合网络的结构，其输入为每个<strong>智能体网络</strong>的输出。为了满足上述的单调性约束，混合网络的所有权值都是非负数，对偏移量不做限制，这样就可以确保满足单调性约束。（Ps：VDN就没有用到去全局状态信息）</p>
<p>图（a）为混合网络内部的<strong>超网络</strong>组，利用系统的全局状态信息 <code>st</code> 作为输入，输出混合网络的权值及偏移量。为了保证权值的<strong>非负性</strong>，采用一个线性网络以及绝对值激活函数保证输出非负。对偏移量采用同样方式但没有非负性的约束，混合网络最后一层的偏移量通过两层网络以及 ReLU 激活函数得到非线性映射网络。状态信息 <code>st</code> 通过超网络混合到 <code>Qtot</code> 中，而并不是作为混合网络的输入</p>
<hr>
<p>混合网络和超网络组的参数都需要被训练，损失函数定义为:</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220307194618.png" alt="image-20220307194618371" style="zoom:70%;" />

<p>更新用到了 DQN 经验回放的思想，其中 b 表示从经验记忆中采样的样本数量（batch size）：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220307194709.png" alt="image-20220307194709736" style="zoom:70%;" />

<center>训练过程与DQN类似</center>

<p>由于满足上文的单调性约束，对 <code>Q_tot</code> 进行 <code>argmax</code> 操作的计算量就不在是随智能体数量呈指数增长了，而是随智能体数量线性增长，极大的提高了算法效率</p>
<p>并且求解 <code>argmax Q_tot</code> 就等价于求解各智能体的子值函数，大大简化了求解复杂度</p>
<h4 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h4><p>QMIX算法的设计在理论上和实验验证上，都比VDN有更多的优势。作为基于值函数的MARL算法，广受研究人员的欢迎</p>
<p>但是正如作者指出的那样，QMIX在执行策略的时候并不考虑其它智能体的动作。在实际场景中，这是不太合理的。针对合作任务的多智能体场景，只有充分考虑其它智能体可能对自己决策产生的影响，才能更好地进行合作。因此，考虑智能体之间更复杂的关系，例如任务/角色分配、智能体通信等，也是QMIX算法扩展的重要方向</p>
<p>此外，因为条件1是条件2的充分不必要条件，这在一定程度上限制了QMIX网络的函数逼近能力，使得在某些场景下，其拟合出来的 <code>Q_tot</code> 和真实值 <code>Q*_tot</code> 存在一定的差距</p>
<p>后续改进算法有：<strong>WQMIX（加权QMIX）</strong>、<strong>QTRAN</strong> 等</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h1 id="矩阵博弈中的分布式学习算法"><a href="#矩阵博弈中的分布式学习算法" class="headerlink" title="矩阵博弈中的分布式学习算法"></a>矩阵博弈中的分布式学习算法</h1><blockquote>
<p>多智能体系统一直是学术界和工业界的热点，其核心领域是关于如何将系统采用分布式的算法控制。在分布式算法中，没有进行总体控制的中心节点。每个智能体通过与环境交互自己学习自己的最优策略，在不知道全局信息的情况下将策略收敛到纳什均衡点。</p>
<p>通常智能体只知道自身获得的奖励值，不知道其他智能体的策略、奖励。分布式算法的核心难点就在于如何在只使用不完整信息的情况下，使每个智能体学到的策略收敛到纳什均衡点。</p>
</blockquote>
<p>上一节提到的简单算法都是基于线性规划、二次规划来求解矩阵博弈的纳什均衡策略，而本节中会给出另外4种学习算法来进行求解。矩阵博弈的学习算法是指每个智能体不断与环境交互，通过获得的奖励值来更新优化其策略，通常可以分为两类：</p>
<ul>
<li>学习自动机</li>
<li>梯度提升</li>
</ul>
<h3 id="学习自动机"><a href="#学习自动机" class="headerlink" title="学习自动机"></a>学习自动机</h3><p>学习自动机是一种通过与环境交互获得奖励来修改动作空间中每个动作的概率分布，从而提升优化策略的方法。学习自动机是一种完全分布式的算法，每个智能体只用知道自己的策略与奖励值，不需要知道环境信息以及其他智能体的信息</p>
<p>学习自动机通常可以用一个元组表示为 <code>(A,r,p,U)</code> ，其中 <code>A=a1,…,am</code> 表示动作集； <code>r</code> 为奖励值； <code>p</code> 为动作集的一个概率分布，即一个要学习的策略； <code>U</code> 表示一个学习算法</p>
<p>下面主要介绍两种算法  <strong>linear reward-inaction</strong> 算法与  <strong>linear reward-penalty</strong> 算法</p>
<h4 id="linear-reward-inaction"><a href="#linear-reward-inaction" class="headerlink" title="linear reward-inaction"></a>linear reward-inaction</h4><blockquote>
<p>适用于：<strong>n智能体矩阵博弈</strong> 或 <strong>双智能体零和博弈</strong></p>
<p>收敛条件：博弈只具有<strong>纯策略严格纳什均衡</strong></p>
</blockquote>
<p>在此算法中，第 i 个智能体的策略更新公式为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306162943.png" alt="image-20220306162943322" style="zoom:65%;" />

<p>其中，<code>k</code> 表示时刻；<code>p</code> 的上标 <code>i</code> 是智能体标号、下标 <code>c</code> 是动作编号，表示该智能体此动作的概率；<code>η</code> 为学习速率；<code>r</code> 表示在k时刻执行动作 <code>c</code> 的奖励值</p>
<p>其意义是将优秀的动作的选择概率提高，为了保证归一性，则将其其他动作被选择的概率降低，只能在博弈只具有纯策略的纳什均衡点的时候可以收敛</p>
<p>完整学习流程为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306162849.png" alt="image-20220306162849763" style="zoom:45%;" />



<h4 id="linear-reward-penalty"><a href="#linear-reward-penalty" class="headerlink" title="linear reward-penalty"></a>linear reward-penalty</h4><blockquote>
<p>适用于：<strong>n智能体矩阵博弈</strong>或<strong>双智能体零和博弈</strong></p>
<p>收敛条件：博弈只有<strong>混合策略的纳什均衡</strong></p>
</blockquote>
<p>通过在 <strong>linear reward-inaction</strong> 算法基础上加入一个惩罚系数得到 <strong>linear reward-penalty</strong> 算法，从而把收敛条件从 纯策略严格纳什均衡 放宽为 混合策略非严格纳什均衡</p>
<p>在此算法中，第 i 个智能体的策略更新公式为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306163818.png" alt="image-20220306163818243" style="zoom:73%;" />

<p>算法中的学习速率应该满足 0 &lt; <code>η1</code>,<code>η2</code> &lt; 1 和 <code>η1</code> &gt; <code>η2</code>，完整流程为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306164128.png" alt="image-20220306164128043" style="zoom:45%;" />

<p>&nbsp;</p>
<h3 id="梯度提升"><a href="#梯度提升" class="headerlink" title="梯度提升"></a>梯度提升</h3><p>梯度提升学习算法在很多地方都有用到，强化学习的随机策略梯度算法SPG、DPG、AC、A2C、A3C等算法都有用到这种思想。使策略的更新方向沿着累积回报增加最大的梯度方向。</p>
<p>虽然有证明，梯度提升学习算法并不能在所有的矩阵博弈中的到收敛解。但是在某些特定的约束博弈中，有一些梯度提升算法可以求解其纳什均衡，常见的有：</p>
<ul>
<li>WoLF-IGA</li>
<li>Lagging Anchor</li>
</ul>
<h4 id="WoLF-IGA"><a href="#WoLF-IGA" class="headerlink" title="WoLF-IGA"></a>WoLF-IGA</h4><blockquote>
<p>适用于：<strong>双智能体双动作</strong>矩阵博弈</p>
<p>收敛条件：<strong>纯策略</strong> 或 <strong>混合策略</strong> 的纳什均衡</p>
</blockquote>
<p>使用 WoLF（Win or learn fast）思想与梯度提升结合。获胜或优秀策略的含义是指当前策略的累积预期奖励大于当前玩家纳什均衡策略和其他玩家使用当前策略的累积预期奖励。当前策略获胜时则谨慎缓慢学习，给其他智能体适应策略变化的时间；当前策略较差时，快速更新调整，使其能够快速调整适应其他智能体策略变化</p>
<p>WoLF-IGA 根据累计奖励关于策略的梯度来修正策略，其目的是使更新后的策略能够获得更大的奖励值，更新方法如下：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306165743.png" alt="image-20220306165742927" style="zoom:63%;" />

<p>其中，<code>p1</code> 表示智能体1选择动作1的概率，则其选择动作2的概率则为 <code>1-p1</code>；<code>η</code> 为学习速率，要足够小；<code>α</code> 的大小变化体现了WoLF的思想；<code>V(p(k), q(k))</code> 表示在时刻k使用策略 <code>(p(k), q(k))</code> 的累计期望奖励； <code>(p*(k), q*(k))</code> 为纳什均衡策略</p>
<p>WoLF-IGA 算法的难点在于需要已知大量信息。其信息包括自身的奖励矩阵、其他玩家的策略以及自己的纳什均衡</p>
<p>虽然智能体知道自己的奖励矩阵，也会得到纳什均衡策略，但这样大量的已知信息导致这个算法并不是一个实用的算法，也不是一个分布式的算法</p>
<h4 id="Lagging-Anchor"><a href="#Lagging-Anchor" class="headerlink" title="Lagging Anchor"></a>Lagging Anchor</h4><blockquote>
<p>适用于：<strong>双智能体零和博弈</strong></p>
<p>收敛条件：<strong>完全混合策略</strong> 的纳什均衡</p>
</blockquote>
<p>定义 <code>v=[p1,p2,…,pm1]</code> 是智能体1对于其 <code>m1</code> 个动作的概率分布，即策略；同理 <code>w=[q1,q2,…,qm2]</code> 为智能体2的策略，策略迭代公式为：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306170459.png" alt="image-20220306170459551" style="zoom:63%;" />

<p>其中，<code>η</code> 为学习速率；<code>ηd</code> &gt; 0 定义为拉锚因子（anchor drawing factor），<code>Pmi</code> 是用于维护策略 <code>v</code>、<code>w</code> 归一化的矩阵：</p>
<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306170816.png" alt="image-20220306170816703" style="zoom:70%;" />

<p><code>Y(k)</code> 是一个单位向量，若智能体2在时刻k采用第i个动作则第i个元素为1，其余元素为0；<code>X(k)</code> 同理，反映智能体1；</p>
<p>Lagging Anchor 算法的核心是每个智能体的策略的加权平均：<img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306171046.png" alt="image-20220306171046465" style="zoom:70%;" />，又成为锚参数</p>
<p>由于这个算法需要用到每一个智能体的奖励矩阵 <code>R1</code>、<code>R2</code> ，因此严格来说其不能算作是一个分布式算法，但是在放宽条件以及智能体之间可以通信的情况下，也算是一个不错的算法</p>
<p>&nbsp;</p>
<h3 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h3><p><img src="https://my-picture-1311448338.file.myqcloud.com/img/20220306171521.png" alt="image-20220306171521785"></p>
<p>&nbsp;</p>
<p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/column/c_1061939147282915328">MARL多智能体强化学习分享</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/272735656">详解多智能体强化学习的基础和应用</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/column/c_1359575787264421888">文献阅读系列——多智能体强化学习</a></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Lrk612
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://sharp-rookie.github.io/2022/03/03/%E3%80%90%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%20&%20%E7%AE%97%E6%B3%95/" title="【多智能体强化学习】基础概念 &amp; 算法">https://sharp-rookie.github.io/2022/03/03/【多智能体强化学习】基础概念 & 算法/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/Algorithm/" rel="tag"># Algorithm</a>
              <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
              <a href="/tags/Mutli-Agent-RL/" rel="tag"># Mutli-Agent RL</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/03/03/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E8%B7%9D%E7%A6%BB%20&%20%E7%9B%B8%E4%BC%BC%E5%BA%A6/" rel="prev" title="【机器学习】距离 & 相似度">
      <i class="fa fa-chevron-left"></i> 【机器学习】距离 & 相似度
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/03/05/%E3%80%90%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E3%80%91Matlab%EF%BC%9A%E5%9B%BE%E5%83%8F%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/" rel="next" title="【数字图像处理】Matlab：图像基本运算">
      【数字图像处理】Matlab：图像基本运算 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B8%8E%E5%8D%9A%E5%BC%88%E8%AE%BA"><span class="nav-text">基础知识与博弈论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E7%B3%BB%E7%BB%9F"><span class="nav-text">强化学习与多智能体系统</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%9A%E5%BC%88%E8%AE%BA%E5%9F%BA%E7%A1%80"><span class="nav-text">博弈论基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%8D%9A%E5%BC%88"><span class="nav-text">矩阵博弈</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1"><span class="nav-text">纳什均衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%A5%E6%A0%BC%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1"><span class="nav-text">严格纳什均衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%8C%E5%85%A8%E6%B7%B7%E5%90%88%E7%AD%96%E7%95%A5"><span class="nav-text">完全混合策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%AF%E7%AD%96%E7%95%A5"><span class="nav-text">纯策略</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%A4%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1"><span class="nav-text">两智能体的纳什均衡</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9B%B6%E5%92%8C%E5%8D%9A%E5%BC%88"><span class="nav-text">零和博弈</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E8%88%AC%E5%92%8C%E5%8D%9A%E5%BC%88"><span class="nav-text">一般和博弈</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E6%B1%82%E8%A7%A3%E5%8F%8C%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E9%9B%B6%E5%92%8C%E5%8D%9A%E5%BC%88"><span class="nav-text">线性规划求解双智能体的零和博弈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%A0%E4%B8%AA%E5%8D%9A%E5%BC%88%E6%A6%82%E5%BF%B5"><span class="nav-text">几个博弈概念</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88MARL%EF%BC%89"><span class="nav-text">多智能体强化学习（MARL）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1"><span class="nav-text">数学建模</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B"><span class="nav-text">学习过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94%E5%8D%95%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-text">对比单体强化学习</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E6%B1%82%E8%A7%A3%E7%AE%97%E6%B3%95"><span class="nav-text">基础求解算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%B4%E8%A7%82%E6%80%9D%E8%B7%AF"><span class="nav-text">直观思路</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%8C%E5%85%A8%E7%AB%9E%E4%BA%89%E5%85%B3%E7%B3%BB"><span class="nav-text">完全竞争关系</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MiniMax-Q"><span class="nav-text">MiniMax-Q</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%8A%E7%AB%9E%E4%BA%89%E5%8D%8A%E5%90%88%E4%BD%9C"><span class="nav-text">半竞争半合作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Nash-Q"><span class="nav-text">Nash Q</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Friend-or-Foe-Q"><span class="nav-text">Friend-or-Foe Q</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#WoLF-PHC"><span class="nav-text">WoLF-PHC</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%8C%E5%85%A8%E5%90%88%E4%BD%9C%E5%85%B3%E7%B3%BB"><span class="nav-text">完全合作关系</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E9%9C%80%E8%A6%81%E5%8D%8F%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="nav-text">不需要协作机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Team-Q-Learning%E3%80%81"><span class="nav-text">Team Q-Learning、</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Distribute-Q-Learning"><span class="nav-text">Distribute Q-Learning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9C%80%E8%A6%81%E5%8D%8F%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="nav-text">需要协作机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#JAL"><span class="nav-text">JAL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#FMQ"><span class="nav-text">FMQ</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MFMARL"><span class="nav-text">MFMARL</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%B1%82%E8%A7%A3%E7%AE%97%E6%B3%95%EF%BC%88%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%89"><span class="nav-text">深度学习求解算法（多智能体深度强化学习）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Policy-based-%E6%96%B9%E6%B3%95"><span class="nav-text">Policy-based 方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MADDPG"><span class="nav-text">MADDPG</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9F-DRL-%E6%80%9D%E8%B7%AF"><span class="nav-text">传统 DRL 思路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MADDPG-1"><span class="nav-text">MADDPG</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93-Actor-Critic-%E8%AE%BE%E8%AE%A1"><span class="nav-text">多智能体 Actor-Critic 设计</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BC%B0%E8%AE%A1%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E4%BD%93%E7%AD%96%E7%95%A5"><span class="nav-text">估计其他智能体策略</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E9%9B%86%E5%90%88%E4%BC%98%E5%8C%96%EF%BC%88policies-ensemble%EF%BC%89"><span class="nav-text">策略集合优化（policies ensemble）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BF%E7%9C%9F%E7%8E%AF%E5%A2%83"><span class="nav-text">仿真环境</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93-1"><span class="nav-text">小结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#COMA"><span class="nav-text">COMA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B-1"><span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94MADDPG"><span class="nav-text">对比MADDPG</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Value-based-%E6%96%B9%E6%B3%95"><span class="nav-text">Value-based 方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#VDN"><span class="nav-text">VDN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B-2"><span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-1"><span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93-2"><span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#QMIX"><span class="nav-text">QMIX</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B-3"><span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="nav-text">预备知识</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#QMIX-1"><span class="nav-text">QMIX</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93-3"><span class="nav-text">小结</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%8D%9A%E5%BC%88%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="nav-text">矩阵博弈中的分布式学习算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%8A%A8%E6%9C%BA"><span class="nav-text">学习自动机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#linear-reward-inaction"><span class="nav-text">linear reward-inaction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#linear-reward-penalty"><span class="nav-text">linear reward-penalty</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87"><span class="nav-text">梯度提升</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#WoLF-IGA"><span class="nav-text">WoLF-IGA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lagging-Anchor"><span class="nav-text">Lagging Anchor</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93-4"><span class="nav-text">小结</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Lrk612"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Lrk612</p>
  <div class="site-description" itemprop="description">Lrk's blog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">56</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.google.com/" title="https:&#x2F;&#x2F;www.google.com" rel="noopener" target="_blank">Google</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://arxiv.org/" title="https:&#x2F;&#x2F;arxiv.org&#x2F;" rel="noopener" target="_blank">arXiv</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://ieeexplore.ieee.org/Xplore/home.jsp" title="https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;Xplore&#x2F;home.jsp" rel="noopener" target="_blank">IEEE Xplore</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <!-- 访问量 -->

    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


<!-- 版权 -->
<div class="copyright">
  
  &copy; 2021-12-1 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lrk612</span>
</div>

<!-- ICP备案 -->
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP备案 </a>
      <img src="https://my-picture-1311448338.file.myqcloud.com/img/20211209170739.png" style="display: inline-block;"><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=2021035798" rel="noopener" target="_blank">豫ICP备2021035798号 </a>
  </div>

<!-- 不知道是什么 -->
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'fH6OdGBcCG8D6jC8NeeJlX9b-gzGzoHsz',
      appKey     : '1UoIb6vszMw6wl0n7kreb4Xz',
      placeholder: "Just go go ^_^",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
    var infoEle = document.querySelector('#comments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
      infoEle.childNodes.forEach(function(item) {
        item.parentNode.removeChild(item);
      });
    }
  }, window.Valine);
});
</script>

</body>
</html>
