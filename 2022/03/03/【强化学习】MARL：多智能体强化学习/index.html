<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sharp-rookie.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

<script> 
   (function(){
          if(''){
              if (prompt('请输入文章密码') !== ''){
                  alert('密码错误！');
                  history.back();
              }
          }
      })();
  </script>
  <meta name="description" content="&amp;emsp;&amp;emsp;因为项目中遇到了多个体共同决策的场景，不同个体的离散动作空间和评价指标都不同，在使用DQN时发现随着个体数量的增加，动作空间会指数式扩张，同时奖励值的设计也很难兼顾所有需求。所以一方面尝试使用策略优化的DRL算法解决离散动作空间维度爆炸问题，另一方面考虑使用多智能体强化学习的算法。 &amp;emsp;&amp;emsp;单智能体强化学习算法有Sutton的神作《Reinforcement">
<meta property="og:type" content="article">
<meta property="og:title" content="【强化学习】多智能体强化学习">
<meta property="og:url" content="https://sharp-rookie.github.io/2022/03/03/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91MARL%EF%BC%9A%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="最忆是江南">
<meta property="og:description" content="&amp;emsp;&amp;emsp;因为项目中遇到了多个体共同决策的场景，不同个体的离散动作空间和评价指标都不同，在使用DQN时发现随着个体数量的增加，动作空间会指数式扩张，同时奖励值的设计也很难兼顾所有需求。所以一方面尝试使用策略优化的DRL算法解决离散动作空间维度爆炸问题，另一方面考虑使用多智能体强化学习的算法。 &amp;emsp;&amp;emsp;单智能体强化学习算法有Sutton的神作《Reinforcement">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303221852.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303193953.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303195023.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303200139.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303205335.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303210048.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303210148.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211027.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211303.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211337.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211551.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211615.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220110.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220150.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220311.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220355.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220829.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303213014.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303213350.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303213453.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303214618.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303215135.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303215312.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220304104127.png">
<meta property="og:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220304112624.png">
<meta property="article:published_time" content="2022-03-03T14:23:42.444Z">
<meta property="article:modified_time" content="2022-03-04T03:32:02.740Z">
<meta property="article:author" content="Lrk612">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Algorithm">
<meta property="article:tag" content="Reinforcement Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/lrk612/md_picture/raw/master/img/20220303221852.png">

<link rel="canonical" href="https://sharp-rookie.github.io/2022/03/03/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91MARL%EF%BC%9A%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>【强化学习】多智能体强化学习 | 最忆是江南</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
        <a target="_blank" rel="noopener" href="https://github.com/Sharp-rookie" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">最忆是江南</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Love actually is all around</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th-list fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-friendlink">

    <a href="/friendlink/" rel="section"><i class="fa fa-link fa-fw"></i>友链</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-download fa-fw"></i>资源</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sharp-rookie.github.io/2022/03/03/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91MARL%EF%BC%9A%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Lrk612">
      <meta itemprop="description" content="Lrk's blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="最忆是江南">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【强化学习】多智能体强化学习
        </h1>

        <div class="post-meta">
         
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-03 22:23:42" itemprop="dateCreated datePublished" datetime="2022-03-03T22:23:42+08:00">2022-03-03</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB/" itemprop="url" rel="index"><span itemprop="name">知识分享</span></a>
                </span>
            </span>

          
            <span id="/2022/03/03/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91MARL%EF%BC%9A%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="post-meta-item leancloud_visitors" data-flag-title="【强化学习】多智能体强化学习" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/03/03/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91MARL%EF%BC%9A%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/03/03/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91MARL%EF%BC%9A%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>&emsp;&emsp;因为项目中遇到了多个体共同决策的场景，不同个体的离散动作空间和评价指标都不同，在使用DQN时发现随着个体数量的增加，动作空间会指数式扩张，同时奖励值的设计也很难兼顾所有需求。所以一方面尝试使用策略优化的DRL算法解决离散动作空间维度爆炸问题，另一方面考虑使用多智能体强化学习的算法。</p>
<p>&emsp;&emsp;单智能体强化学习算法有Sutton的神作《Reinforcement Learning: An introduction》，但是多智能体强化学习方面就没什么系统的网课或者书籍，因此算是比较前沿的研究领域。多智能体强化学习专注于实现具有多个智能体的自主、自学习系统的领域，其中涉及大量博弈论概念，值得一学。</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303221852.png" alt="image-20220303221852425" style="zoom:28%;" />

<span id="more"></span>

<p>&nbsp;</p>
<h1 id="基础知识与博弈论"><a href="#基础知识与博弈论" class="headerlink" title="基础知识与博弈论"></a>基础知识与博弈论</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在多智能体（Mutli-Agent）系统中，每个智能体通过与环境进行交互获得奖励值来学习改善自己的策略，从而获得该环境下最优策略的过程，就是多智能体强化学习</p>
<p>单智能体强化学习中，智能体所在的环境是稳定不变的，但是如果用单智能体来学习多个体的策略，将会产生许多困难：</p>
<ul>
<li><p>维度爆炸</p>
<p>  如果用单智能体强化学习来解决多个体问题，那么多个个体的联结动作空间（A=[a1, a2, a3,……,an]）会随着个体数量指数式增长，系统维度爆炸（但这一点可以用策略优化的算法来解决）</p>
</li>
<li><p>目标奖励难确定</p>
<p>  多个体系统中，因为每个个体评价好坏的指标不同，并且彼此之间又相互耦合影响，奖励设计的优劣会直接影响学习到的策略的收敛性和效果，通常难以设计</p>
</li>
<li><p>不稳定性</p>
<p>  多个体系统中，多个个体的策略互相影响。当同伴的策略改变时，每个个体自身的最优策略也可能会变化，这将使得单智能体难以兼顾所有个体的性能，算法的收敛性难以保证</p>
</li>
<li><p>收敛慢</p>
<p>  不光要考虑单个个体对环境的探索，所有个体的策略变化都要进行探索。每个个体策略的变化都可能打破其他个体策略的平衡状态，这将使算法很难稳定，学习速度慢</p>
</li>
</ul>
<p>因此，用单智能体解决多个体决策问题是一个很大的挑战，由此引入多智能体强化学习，为每个个体维护一个做决策的智能体。智能体之间存在合作与竞争，最终实现最大共同利益或者规定的利益，基于此，引入博弈的概念，将博弈论与强化学习相结合从而更好地处理这些问题</p>
<p>&nbsp;</p>
<h2 id="博弈论基础"><a href="#博弈论基础" class="headerlink" title="博弈论基础"></a>博弈论基础</h2><h3 id="矩阵博弈"><a href="#矩阵博弈" class="headerlink" title="矩阵博弈"></a>矩阵博弈</h3><p>矩阵博弈可以表示为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303193953.png" alt="image-20220303193953815" style="zoom:40%;" />

<p>其中 <code>n</code> 表示智能体数量，<code>A </code>为动作，<code>Ri：A1×A2×……×An——&gt;R</code> 表示第i个智能体的奖励函数，可见奖励函数与每个智能体的联结动作有关，联结动作空间为：<code>A1×A2×……×An</code>（其中每个A都是动作空间而不是单个动作）</p>
<p>每个智能体的策略是一个关于其动作空间的概率分布，每个智能体的目标是最大化其在联结策略 <code>(Π1, Π2,……,Πn)</code>下的奖励值：<code>Vi(Π1, Π2,……,Πn)</code></p>
<h4 id="纳什均衡"><a href="#纳什均衡" class="headerlink" title="纳什均衡"></a>纳什均衡</h4><p>在矩阵博弈中，如果联结策略 <code>(Π1*, Π2*,……,Πn*)</code> 满足：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303195023.png" alt="image-20220303195023004" style="zoom:70%;" />

<center>式 1</center>

<p>则称为一个纳什均衡。也就是说，纳什均衡是一个所有智能体的联结策略，在纳什均衡处，对于所有智能体而言，都不能通过仅改变自身策略的情况下获得更大的奖励值</p>
<p>设联结动作 <code>(a1,a2,……,an)</code> 的期望奖励为 <code>Q(a1,a2,……,an)</code>，令 <code>Πi(ai)</code> 表示第i个智能体选取动作 <code>ai</code></p>
<p>的概率。则纳什均衡还可定义为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303200139.png" alt="image-20220303200139101" style="zoom:60%;" />

<center>式 2</center>



<h4 id="严格纳什均衡"><a href="#严格纳什均衡" class="headerlink" title="严格纳什均衡"></a>严格纳什均衡</h4><p>公式1中取严格大于时，联结策略 <code>(Π1*, Π2*,……,Πn*)</code> 即为严格纳什均衡</p>
<h4 id="完全混合策略"><a href="#完全混合策略" class="headerlink" title="完全混合策略"></a>完全混合策略</h4><p>若一个策略对于智能体动作集中的所有动作的概率都大于0，则这个策略为一个完全混合策略</p>
<h4 id="纯策略"><a href="#纯策略" class="headerlink" title="纯策略"></a>纯策略</h4><p>若智能体的策略对一个动作的概率分布为1，对其余的动作的概率分布为0，则这个策略为一个纯策略</p>
<h3 id="两个智能体的纳什均衡"><a href="#两个智能体的纳什均衡" class="headerlink" title="两个智能体的纳什均衡"></a>两个智能体的纳什均衡</h3><p>两智能体的博弈问题是大部分多智能体强化学习算法的基础。双智能体矩阵博弈之于多智能体强化学习就像感知机之于神经网络</p>
<p>在双智能体矩阵博弈中，我们可以设计一个矩阵，矩阵每一个元素的索引坐标表示一个联结动作 <code>[A1=x, A2=y]</code>  ，第 <code>i</code> 个智能体的奖励矩阵 <code>Ri</code> 的元素 <code>rxy</code> 就表示第一个智能体采用动作 <code>x</code>，第二个智能体采用动作 <code>y</code> 时第 <code>i</code> 个智能体获得的奖励，<code>cxy</code> 也同理。通常我们将第一个智能体定义为<strong>行智能体</strong>，第二个智能体定义为<strong>列智能体</strong>，行号表示第一个智能体选取的动作，列号表示第二个智能体选取的动作。则对于只有2个动作的智能体，其奖励矩阵分别可以写为:</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303205335.png" alt="image-20220303205335878" style="zoom:70%;" />



<h4 id="零和博弈"><a href="#零和博弈" class="headerlink" title="零和博弈"></a>零和博弈</h4><p>零和博弈中，两个智能体是完全竞争对抗关系，即 <code>R1=-R2</code> 。在零和博弈中只有一个纳什均衡值，即使可能有很多纳什均衡策略，但是期望的奖励是相同的</p>
<h4 id="一般和博弈"><a href="#一般和博弈" class="headerlink" title="一般和博弈"></a>一般和博弈</h4><p>一般和博弈是指任何类型的矩阵博弈，包括完全对抗博弈、完全合作博弈以及二者的混合博弈。在一般和博弈中可能存在多个纳什均衡点</p>
<p>定义策略 <code>Πi=(Πi(a1),…,Πi(ami))</code> 为智能体 i 的动作集中每个动作的概率集合，<code>mi</code> 为可选的动作数量，则值函数 <code>Vi</code> 可表示为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303210048.png" alt="image-20220303210048275" style="zoom:72%;" />

<p>纳什均衡策略 <code>(Π1*, Π2*)</code> 可表示为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303210148.png" alt="image-20220303210148099" style="zoom:70%;" />

<p>其中 <code>PD(Ai)</code> 表示第 <code>i</code> 个智能体的策略空间，<code>-i</code> 表示另一个智能体</p>
<p>若满足：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211027.png" alt="image-20220303211027797" style="zoom:75%;" />

<p>则称 <code>(l,f)</code> 满足纯策略严格纳什均衡，<code>-l</code>、<code>-f</code> 表示除了 <code>l</code>、<code>f</code> 的另一个策略</p>
<h3 id="线性规划求解双智能体的零和博弈"><a href="#线性规划求解双智能体的零和博弈" class="headerlink" title="线性规划求解双智能体的零和博弈"></a>线性规划求解双智能体的零和博弈</h3><p>求解双智能体零和博弈的思路是最大化每个智能体在与对手博弈中最差情况下的期望奖励值：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211303.png" alt="image-20220303211303527" style="zoom:70%;" />

<p>博弈形式为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211337.png" alt="image-20220303211337168" style="zoom:70%;" />

<p>定义 <code>pj(j=1,2)</code> 为第一个智能体选择动作 <code>J</code>  的概率，<code>qj(j=1,2)</code> 为第二个智能体选择动作 <code>j</code> 的概率。则可对两个智能体列写如下线性规划：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211551.png" alt="image-20220303211551299" style="zoom:70%;" />

<center>智能体1</center>

<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303211615.png" alt="image-20220303211615167" style="zoom:70%;" />

<center>智能体2</center>

<p>求解即得纳什均衡策略</p>
<h3 id="几个博弈概念"><a href="#几个博弈概念" class="headerlink" title="几个博弈概念"></a>几个博弈概念</h3><table>
<thead>
<tr>
<th>概念</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>随机博弈</td>
<td>马尔可夫决策过程包含一个智能体与多个状态，矩阵博弈包括多个智能体与一个状态。随即博弈 (stochastic game / Markov game)是马尔可夫决策过程与矩阵博弈的结合，具有多个智能体与多个状态，即多智能体强化学习</td>
</tr>
<tr>
<td>静态博弈</td>
<td>静态博弈 (static/stateless game) 是指没有状态s，不存在动力学使状态能够转移的博弈。例如一个矩阵博弈</td>
</tr>
<tr>
<td>阶段博弈</td>
<td>阶段博弈 (stage game) 是随机博弈的组成成分，状态s是固定的，相当于一个状态固定的静态博弈，随机博弈中的Q值函数就是该阶段博弈的奖励函数。若干状态的阶段博弈组成一个随机博弈</td>
</tr>
<tr>
<td>重复博弈</td>
<td>智能体重复访问同一个状态的阶段博弈，并且在访问同一个状态的阶段博弈的过程中收集其他智能体的信息与奖励值，并学习更好的Q值函数与策略</td>
</tr>
</tbody></table>
<p><strong>随机博弈示例</strong></p>
<p>定义一个2*2的网格博弈，两个智能体分别表示为 P1、P2 ，P1的初始位置在左下角，P2的初始位置在右上角，每一个智能体都想以最快的方式达到 G 标志的地方。从初始位置开始，每个智能体都有两个动作可以选择。只要有一个智能体达到 G 则游戏结束，达到 G 的智能体获得奖励10，奖励折扣率为0.9。虚线表示栏杆，智能体穿过栏杆的概率为0.5</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220110.png" alt="image-20220303220110825" style="zoom:55%;" />

<p>该随机博弈一共包含7个状态，纳什均衡策略是：每个智能体到达邻居位置而不穿过栏杆</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220150.png" alt="image-20220303220150599" style="zoom:40%;" />

<p>根据上面公式，可得状态值函数：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220311.png" alt="image-20220303220311352" style="zoom:70%;" />

<p>由此可得动作状态值函数：</p>
<p><img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220355.png" alt="image-20220303220355121"></p>
<p>求解下图中的矩阵博弈即可得到多智能体强化学习的策略：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303220829.png" alt="image-20220303220829821" style="zoom:50%;" />

<p>&nbsp;</p>
<h2 id="多智能体强化学习"><a href="#多智能体强化学习" class="headerlink" title="多智能体强化学习"></a>多智能体强化学习</h2><blockquote>
<p>多智能体强化学习就是一个随机博弈，将每一个状态的阶段博弈的纳什策略组合起来成为一个智能体在动态环境中的策略，并不断与环境交互来更新每一个状态的阶段博弈中的Q值函数（博弈奖励）</p>
</blockquote>
<h3 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h3><p>记随机博弈为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303213014.png" alt="image-20220303213014817" style="zoom:70%;" />

<p>其中 <code>n</code> 表示智能体数量，<code>S</code> 表示状态空间，<code>Ai</code> 表示第 <code>i</code> 个智能体的动作空间（不是单个动作），<code>Tr</code> 为状态转移概率：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303213350.png" alt="image-20220303213350704" style="zoom:70%;" />

<p>奖励 <code>R</code> 定义为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303213453.png" alt="image-20220303213453284" style="zoom:70%;" />

<p>即第 <code>i</code> 个智能体在当前状态与联结动作下获得的回报值， <code>γ</code> 表示累积奖励折扣系数</p>
<p>随机博弈也具有马尔科夫性，下一个状态与奖励只与当前状态与当前的联结动作有关</p>
<h3 id="学习过程"><a href="#学习过程" class="headerlink" title="学习过程"></a>学习过程</h3><p>多智能体强化学习过程，就是<strong>找到每一个状态的纳什均衡策略</strong>，然后将这些策略联合起来。<code>Πi: S——&gt;Ai</code>  就是一个智能体i的策略，在每个状态选出最优的纳什策略。</p>
<p>多智能体强化学习最优策略（随机博弈的纳什均衡策略）可以写为  <code>(Π1*, Π2*,……,Πn*)</code>  ，且对 <code>任意s∈S,i=1,……,n</code> 满足：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303214618.png" alt="image-20220303214618925" style="zoom:70%;" />

<p>不等式左边为 <code>γ</code> 折扣累积状态值函数，用 <code>Vi*(s)</code> 简记上式，用 <code>Qi*(a1,a2,……,an)</code> 表示动作状态 <code>γ</code> 折扣累积函数，在每个固定状态s的阶段博弈中，利用 <code>Qi*</code> 作为博弈的奖励求解纳什均衡策略，根据贝尔曼方程可得：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303215135.png" alt="image-20220303215135802" style="zoom:70%;" />

<p>多智能体强化学习的纳什策略可改写为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220303215312.png" alt="image-20220303215312114" style="zoom:60%;" />

<p>根据每个智能体的奖励函数可以对随机博弈进行分类。若智能体的奖励函数相同，则称为完全合作博弈或团队博弈。若智能体的奖励函数逆号，则称为完全竞争博弈或零和博弈。为了求解随机博弈，需要求解每个状态s的阶段博弈，每个阶段博弈的奖励值就是 <code>Qi(s,•)</code></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h1 id="基础算法"><a href="#基础算法" class="headerlink" title="基础算法"></a>基础算法</h1><p>多智能体强化学习过程基本上是求解一个随机博弈问题，但其实这两个概念还不能完全等价。因为随机博弈中假定每个状态的奖励矩阵是已知的，不需要学习；而多智能体强化学习则是通过与环境的不断交互来学习每个状态的奖励矩阵，再通过这些奖励矩阵来学习得到最优纳什策略。</p>
<p>也就是说，多智能体强化学习和基于价值优化单智能体学习的一样，都需要评估出准确的状态价值，然后根据价值进行决策。只不过单智能体拿到价值后直接采用贪婪或者 ε-greedy 这样的策略即可，但是多智能体没有现成的策略直接用，需要根据价值矩阵推算出一个纳什均衡策略来用（类似于DDPG，估计价值、设计策略）</p>
<p>多智能体强化学习算法中，两个主要的技术指标分别为合理性与收敛性：</p>
<ul>
<li><p>合理性（rationality）</p>
<p>  在对手使用一个恒定策略的情况下，当前智能体能够学习并收敛到一个相对于对手策略的最优策略</p>
</li>
<li><p>收敛性（convergence）</p>
<p>  在其他智能体也使用学习算法时，当前智能体能够学习并收敛到一个稳定的策略</p>
<p>  通常情况下，收敛性针对系统中的所有的智能体使用相同的学习算法</p>
</li>
</ul>
<p>针对应用来分，多智能体强化学习算法可分为<strong>零和博弈算法</strong>与<strong>一般和博弈算法</strong></p>
<h2 id="MiniMax-Q"><a href="#MiniMax-Q" class="headerlink" title="MiniMax-Q"></a>MiniMax-Q</h2><blockquote>
<p>用于<strong>零和随机博弈</strong>，即两个智能体为完全竞争对抗关系 <code>R1=-R2</code> </p>
</blockquote>
<ul>
<li><strong>Q</strong> 即指利用 Q-Learning 中的TD方法来迭代学习状态值函数或动作-状态值函数</li>
<li><strong>MiniMax</strong> 即指上一节中利用线性规划求解每个特定状态 s 的阶段博弈的纳什均衡策略</li>
</ul>
<p><strong>算法思路</strong></p>
<p>在两玩家零和随机博弈中，给定状态 s，则定义第 i 个智能体的状态值函数为：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220304104127.png" alt="image-20220304104127171" style="zoom:65%;" />

<p><code>Qi*(s,a_i,a_-i)</code> 是联结动作状态价值函数，若已知，则可以直接用线性规划求解出状态 s 处的纳什均衡策略。但是实际上它一开始肯定是未知的，所以需要用 Q-Learning 或其他方法来更新推算<code>Qi*(s,a_i,a_-i)</code> ，类似于单智能体强化学习中的基于价值的求解思想，一方面评估价值要准确，另一方面根据评估得到的价值矩阵推算纳什均衡策略：</p>
<img src="https://gitee.com/lrk612/md_picture/raw/master/img/20220304112624.png" alt="image-20220304112554105" style="zoom:42%;" />

<p>理想情况，如果算法能够对每一个状态-动作对访问无限次，那么该算法能够收敛到纳什均衡策略</p>
<p><strong>算法缺点</strong></p>
<ol>
<li><p>在第5步中需要不断求解一个线性规划，降低了学习速度，增加计算时间</p>
</li>
<li><p>为了求解第5步，智能体 i 需要知道所有智能体的动作空间，这个在分布式系统中将无法满足</p>
</li>
<li><p>只满足收敛性，不满足合理性</p>
<p> Minimax-Q 算法能够找到多智能体强化学习的纳什均衡策略，但是假设对手使用的不是纳什均衡策略，而是一个较差的策略，则当前智能体并不能根据对手的策略学习到一个更优的策略。该算法无法让智能体根据对手的策略来调节优化自己的策略，而只能找到随机博弈的纳什均衡策略。这是由于Minimax-Q算法是一个对手独立算法（opponent-independent algorithm），不论对手策略是怎么样的，都收敛到该博弈的纳什均衡策略，所以如果对方不讲武德乱出牌，Minimax-Q效果就不好了</p>
</li>
</ol>
<p>&nbsp;</p>
<h2 id="Nash-Q"><a href="#Nash-Q" class="headerlink" title="Nash Q"></a>Nash Q</h2><blockquote>
<p>将Minimax-Q算法从零和博弈扩展到了<strong>多人一般和博弈</strong></p>
</blockquote>
<p>&nbsp;</p>
<h2 id="FFQ"><a href="#FFQ" class="headerlink" title="FFQ"></a>FFQ</h2><p>&nbsp;</p>
<h2 id="WoLF-PHC"><a href="#WoLF-PHC" class="headerlink" title="WoLF-PHC"></a>WoLF-PHC</h2>
    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Lrk612
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://sharp-rookie.github.io/2022/03/03/%E3%80%90%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%91MARL%EF%BC%9A%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" title="【强化学习】多智能体强化学习">https://sharp-rookie.github.io/2022/03/03/【强化学习】MARL：多智能体强化学习/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/Algorithm/" rel="tag"># Algorithm</a>
              <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/03/03/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E8%B7%9D%E7%A6%BB%20&%20%E7%9B%B8%E4%BC%BC%E5%BA%A6/" rel="prev" title="【机器学习】距离 & 相似度">
      <i class="fa fa-chevron-left"></i> 【机器学习】距离 & 相似度
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B8%8E%E5%8D%9A%E5%BC%88%E8%AE%BA"><span class="nav-text">基础知识与博弈论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%95%E8%A8%80"><span class="nav-text">引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%9A%E5%BC%88%E8%AE%BA%E5%9F%BA%E7%A1%80"><span class="nav-text">博弈论基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%8D%9A%E5%BC%88"><span class="nav-text">矩阵博弈</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1"><span class="nav-text">纳什均衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%A5%E6%A0%BC%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1"><span class="nav-text">严格纳什均衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%8C%E5%85%A8%E6%B7%B7%E5%90%88%E7%AD%96%E7%95%A5"><span class="nav-text">完全混合策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%AF%E7%AD%96%E7%95%A5"><span class="nav-text">纯策略</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%A4%E4%B8%AA%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1"><span class="nav-text">两个智能体的纳什均衡</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9B%B6%E5%92%8C%E5%8D%9A%E5%BC%88"><span class="nav-text">零和博弈</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E8%88%AC%E5%92%8C%E5%8D%9A%E5%BC%88"><span class="nav-text">一般和博弈</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E6%B1%82%E8%A7%A3%E5%8F%8C%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E9%9B%B6%E5%92%8C%E5%8D%9A%E5%BC%88"><span class="nav-text">线性规划求解双智能体的零和博弈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%A0%E4%B8%AA%E5%8D%9A%E5%BC%88%E6%A6%82%E5%BF%B5"><span class="nav-text">几个博弈概念</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-text">多智能体强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%BA%E6%A8%A1"><span class="nav-text">建模</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B"><span class="nav-text">学习过程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95"><span class="nav-text">基础算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MiniMax-Q"><span class="nav-text">MiniMax-Q</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Nash-Q"><span class="nav-text">Nash Q</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FFQ"><span class="nav-text">FFQ</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#WoLF-PHC"><span class="nav-text">WoLF-PHC</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Lrk612"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Lrk612</p>
  <div class="site-description" itemprop="description">Lrk's blog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">39</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.csdn.net/" title="https:&#x2F;&#x2F;www.csdn.net&#x2F;" rel="noopener" target="_blank">CSDN</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.google.com/" title="https:&#x2F;&#x2F;www.google.com" rel="noopener" target="_blank">Google</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <!-- 访问量 -->

    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


<!-- 版权 -->
<div class="copyright">
  
  &copy; 2021-12-1 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lrk612</span>
</div>

<!-- ICP备案 -->
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP备案 </a>
      <img src="https://gitee.com/lrk612/md_picture/raw/master/img/20211209170739.png" style="display: inline-block;"><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=2021035798" rel="noopener" target="_blank">豫ICP备2021035798号 </a>
  </div>

<!-- 不知道是什么 -->
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'fH6OdGBcCG8D6jC8NeeJlX9b-gzGzoHsz',
      appKey     : '1UoIb6vszMw6wl0n7kreb4Xz',
      placeholder: "Just go go ^_^",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
    var infoEle = document.querySelector('#comments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
      infoEle.childNodes.forEach(function(item) {
        item.parentNode.removeChild(item);
      });
    }
  }, window.Valine);
});
</script>

</body>
</html>
