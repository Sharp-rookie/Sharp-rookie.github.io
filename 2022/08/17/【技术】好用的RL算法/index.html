<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sharp-rookie.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

<script> 
   (function(){
          if(''){
              if (prompt('请输入文章密码') !== ''){
                  alert('密码错误！');
                  history.back();
              }
          }
      })();
  </script>
  <meta name="description" content="强化学习不需要模型内部原理，通过求解马尔可夫过程中的最优决策方案给出复杂问题的最优或较优解，在一些具体应用场景中有着良好的效果，这里记录一些好用的经典算法：  PPO：Proximal Policy Optimization SAC：Soft Actor Critic TD3：Twin Delayed Deep Deterministic Policy Gradient  同时针对什么样的场景选择">
<meta property="og:type" content="article">
<meta property="og:title" content="经典好用的RL算法——“开盖即食”">
<meta property="og:url" content="https://sharp-rookie.github.io/2022/08/17/%E3%80%90%E6%8A%80%E6%9C%AF%E3%80%91%E5%A5%BD%E7%94%A8%E7%9A%84RL%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="最忆是江南">
<meta property="og:description" content="强化学习不需要模型内部原理，通过求解马尔可夫过程中的最优决策方案给出复杂问题的最优或较优解，在一些具体应用场景中有着良好的效果，这里记录一些好用的经典算法：  PPO：Proximal Policy Optimization SAC：Soft Actor Critic TD3：Twin Delayed Deep Deterministic Policy Gradient  同时针对什么样的场景选择">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208172151622.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131028589.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131029538.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131033952.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131256231.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131407636.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131421834.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131437068.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131441487.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208151024506.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131453341.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/20220209161905.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208241338496.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131529996.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131535372.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131543783.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131437068.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131600558.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131602761.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131625776.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131611863.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131706107.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131616274.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208131735571.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208171618913.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208171646069.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208171632549.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208171632432.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208171633260.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208171639923.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208171649725.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208171651283.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208171112205.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208171116684.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208171326974.png">
<meta property="og:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208171332044.png">
<meta property="article:published_time" content="2022-08-17T13:52:24.577Z">
<meta property="article:modified_time" content="2022-08-24T05:38:51.818Z">
<meta property="article:author" content="Lrk612">
<meta property="article:tag" content="Algorithm">
<meta property="article:tag" content="优化问题">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://my-picture-1311448338.file.myqcloud.com/img/202208172151622.png">

<link rel="canonical" href="https://sharp-rookie.github.io/2022/08/17/%E3%80%90%E6%8A%80%E6%9C%AF%E3%80%91%E5%A5%BD%E7%94%A8%E7%9A%84RL%E7%AE%97%E6%B3%95/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>经典好用的RL算法——“开盖即食” | 最忆是江南</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
        <a target="_blank" rel="noopener" href="https://github.com/Sharp-rookie" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">最忆是江南</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Love actually is all around</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-title_page">

    <a href="/title_page/" rel="section"><i class="fa fa-book fa-fw"></i>title_page</a>

  </li>
        <li class="menu-item menu-item-story">

    <a href="/story/" rel="section"><i class="fa fa-book fa-fw"></i>story</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sharp-rookie.github.io/2022/08/17/%E3%80%90%E6%8A%80%E6%9C%AF%E3%80%91%E5%A5%BD%E7%94%A8%E7%9A%84RL%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Lrk612">
      <meta itemprop="description" content="Lrk's blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="最忆是江南">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          经典好用的RL算法——“开盖即食”
        </h1>

        <div class="post-meta">
         
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-17 21:52:24" itemprop="dateCreated datePublished" datetime="2022-08-17T21:52:24+08:00">2022-08-17</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/" itemprop="url" rel="index"><span itemprop="name">技术分享</span></a>
                </span>
            </span>

          
            <span id="/2022/08/17/%E3%80%90%E6%8A%80%E6%9C%AF%E3%80%91%E5%A5%BD%E7%94%A8%E7%9A%84RL%E7%AE%97%E6%B3%95/" class="post-meta-item leancloud_visitors" data-flag-title="经典好用的RL算法——“开盖即食”" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/08/17/%E3%80%90%E6%8A%80%E6%9C%AF%E3%80%91%E5%A5%BD%E7%94%A8%E7%9A%84RL%E7%AE%97%E6%B3%95/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/08/17/%E3%80%90%E6%8A%80%E6%9C%AF%E3%80%91%E5%A5%BD%E7%94%A8%E7%9A%84RL%E7%AE%97%E6%B3%95/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>强化学习不需要模型内部原理，通过求解马尔可夫过程中的最优决策方案给出复杂问题的最优或较优解，在一些具体应用场景中有着良好的效果，这里记录一些好用的经典算法：</p>
<ul>
<li><strong>PPO</strong>：Proximal Policy Optimization</li>
<li><strong>SAC</strong>：Soft Actor Critic</li>
<li><strong>TD3</strong>：Twin Delayed Deep Deterministic Policy Gradient</li>
</ul>
<p>同时针对什么样的场景选择什么样的算法也给出了参考建议</p>
<p><img src="https://my-picture-1311448338.file.myqcloud.com/img/202208172151622.png" alt="QQ图片20220817215142" style="zoom: 33%;" /></p>
<span id="more"></span>
<p>&nbsp;</p>
<h1 id="经典而好用的RL算法"><a href="#经典而好用的RL算法" class="headerlink" title="经典而好用的RL算法"></a>经典而好用的RL算法</h1><blockquote>
<p>本部分包括：PPO、SAC、TD3 的基本原理、示例代码</p>
<p>在 PPO 的部分重温了策略梯度算法的核心概念，并介绍了 A2C、A3C 等 Actor-Critic 算法</p>
<p>在 SAC 的部分介绍了最大熵思想的强化学习概念</p>
<p>在 TD3 的部分重温了 DQN、DDPG 的核心概念</p>
</blockquote>
<h2 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h2><blockquote>
<p>近端策略优化算法，Proximal Policy Optimization</p>
<p>OpenAI 建议的 Baseline 算法，简单好用</p>
<p>属于随机策略梯度，在普通 Actor-Critic 基础上做了改进：<strong>Imporance Sampling</strong> 和 <strong>KL散度保持一致性</strong></p>
<p>虽然本质是 on-policy 算法，但是拥有 off-policy 算法 “一次采集多次使用” 的节省时间优点</p>
<p>从 TRPO 算法改进而来，二者其实相差不大</p>
</blockquote>
<h3 id="先导知识"><a href="#先导知识" class="headerlink" title="先导知识"></a>先导知识</h3><blockquote>
<p>要懂 PPO，先搞懂策略梯度、Actor-Critic 里的概念再说</p>
</blockquote>
<h4 id="策略梯度基本概念"><a href="#策略梯度基本概念" class="headerlink" title="策略梯度基本概念"></a>策略梯度基本概念</h4><blockquote>
<p>是与 Q-Learning、DQN 等基于价值算法相对立的另一种决策问题的求解思路</p>
<p>详见：<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/easy-rl/#/chapter4/chapter4">EasyRL 策略梯度</a></p>
</blockquote>
<p>PPO算法是在on-policy的随机策略梯度算法基础上改进而来，因此需要先了解策略梯度算法的优化逻辑、更新方式，其中包括：</p>
<ul>
<li><p><strong>随机策略</strong>是指给定状态s下，各个动作的选择概率，也就是一个概率分布，记为：<code>p_θ(τ)</code></p>
<p>  实际上，策略网络输出的是action分布的数字特征，例如一个离散动作的环境，随机策略就是给定状态下，每个动作的概率分布函数。策略网络的输出就是这个分布的均值或方差等数字特征，策略梯度上升其实就是让其输出不断逼近真实分布特征的过程</p>
</li>
<li><p><strong>目标函数</strong>有多种可选的物理含义，但公示的形式一致，都是给定策略下奖励的期望值，记为：</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131028589.png" alt="image-20220813102819492" style="zoom:33%;" /></p>
<p>  进行数学变形，可以等效为：</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131029538.png" alt="image-20220813102956495" style="zoom:33%;" /></p>
</li>
<li><p><strong>优化方法</strong>（梯度上升），没什么好说的</p>
</li>
<li><p>一些 <strong>trick</strong>：用 优势函数A(t) 代替奖励期望值中的 R(t)，记为：</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131033952.png" alt="image-20220813103308917" style="zoom: 67%;" /></p>
<p>  其中，<code>R-b</code> 这个部分就是优势函数，表示所选动作相对其他动作的好坏，<code>b</code>可以是所有动作的平均奖励值</p>
</li>
<li><p><strong>R</strong> 的计算也采用了优化 tirck，即不再统一使用全回合的总奖励值，而是改为从当前动作开始连着后续几个动作奖励的折扣加权，即凸显出该动作对当前和后续的影响，不考虑在其之前的那些动作的价值（因为是与其无关的）：</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131256231.png" alt="image-20220813125613144" style="zoom: 33%;" /></p>
</li>
<li><p>基本算法：<strong>REINFORCE</strong></p>
<p>  使用<strong>蒙特卡洛法</strong>来估计奖励值，也就是先运行一个完整回合，然后用收集到的每个状态和动作的平均奖励值来当作其理论奖励进行折扣加权，进行策略的梯度上升训练，奖励的计算为：</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131407636.png" alt="image-20220813140754592" style="zoom: 67%;" /></p>
<p>  完整的梯度计算公示为：</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131421834.png" alt="image-20220813142101797" style="zoom: 67%;" /></p>
<p>  奖励值减去 <code>b</code> 得到优势值，为正则增加在这个状态采取这个动作的机率；为负则减少</p>
<p>  <strong>弊端：</strong></p>
<p>  用 G 来表示累积奖励，但 G 其实非常不稳定。因为互动的过程本身是有随机性的，G 其实是一个随机变量，有一个固定的分布，所以在某一个状态 s 采取某一个动作 a，然后计算累积奖励，每次算出来的结果都是不一样的 。</p>
<p>  假设在每次更新参数之前，都可以采样足够的次数，那其实没有什么问题。但是每次做 policy gradient，每次更新参数之前采样的次数不可能太多，只能够做非常少量的采样。</p>
</li>
</ul>
<h4 id="策略梯度经典算法：Actor-Critic"><a href="#策略梯度经典算法：Actor-Critic" class="headerlink" title="策略梯度经典算法：Actor-Critic"></a>策略梯度经典算法：Actor-Critic</h4><blockquote>
<p>最知名的方法就是 <code>A3C(Asynchronous Advantage Actor-Critic)</code></p>
<ul>
<li>如果去掉 Asynchronous，只有 <code>Advantage Actor-Critic</code>，就叫做 <code>A2C</code>。</li>
<li>如果加了 Asynchronous，变成 <code>Asynchronous Advantage Actor-Critic</code>，就变成 <code>A3C</code></li>
</ul>
<p>详见：<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/easy-rl/#/chapter9/chapter9">EasyRL 演员-评论家算法</a></p>
</blockquote>
<p>PPO算法在代码实现中，大多是基于 Actor-Critic 来做的，只不过是加入了其优化改进的点，因此有必要搞清楚 Actor-Critic 的相关概念，包括：</p>
<ul>
<li>采用<strong>时序差分</strong>思想，引入值函数，可以单步更新，规避了 REINFORCE 中训练一次需要遍历整个完整回合的蒙特卡洛法弊端</li>
<li><strong>演员(Actor)</strong>是指策略函数 <code>πθ(a∣s)</code>，即学习一个策略来得到尽量高的回报</li>
<li><p><strong>评论家(Critic)</strong>是指值函数 <code>Vπ(s)</code>，对当前策略的值函数进行估计，即评估演员的好坏；具体而言，Critic有两种，分别估计状态价值和动作价值：</p>
<ol>
<li>critic 是 V^π^<em>(</em>s)，代表 actor 跟环境做互动，当看到状态 s 的时候，接下来累积奖励的期望值有多少；V^π^ 输入 s，输出一个标量</li>
<li>critic 是 Q^π^(s,a)，代表在状态 s 采取动作 a，接下来 actor 跟环境进行互动累积奖励的期望是多少；Q^π^ 输入 s，输出给每个 a 都分配一个 Q value</li>
</ol>
</li>
<li><p><strong>优势函数的估计：GAE</strong></p>
<p>  很明显，从定义就可知，优势函数中，前半部分的动作价值就是 Q^π^(s,a)，而减去的 <code>b</code> 选取为状态价值 V^π^<em>(</em>s)，由此，优势函数就等于 动作价值 减 状态价值，这就是 GAE：</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131437068.png" alt="image-20220813143744987" style="zoom: 33%;" /></p>
<p>  但是，总不能用两个 Critic 网络分别估计 动作价值 和 状态价值，这样误差太大了！利用 状态价值 和 动作价值间的贝尔曼方程转化关系，可以看出实际上<strong>只用</strong>一个 Critic 网络估计 <strong>状态价值V</strong> 即可</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131441487.png" alt="image-20220813144100434" style="zoom: 40%;" /></p>
  <center>实际上，估计Q，推出V也是可以的，但是实验证明估计V更好</center>

<p>  不过这样还是引入了随机性，r 是一个随机变量。但是因为 r 是某一个步骤会得到的奖励，而 G 是所有未来会得到的奖励的总和。所以 G 的方差比较大，r 的方差会比 G 要小</p>
</li>
</ul>
<h5 id="A2C算法：Advantage-Actor-Critic"><a href="#A2C算法：Advantage-Actor-Critic" class="headerlink" title="A2C算法：Advantage Actor-Critic"></a>A2C算法：Advantage Actor-Critic</h5><p>A2C就是按照上面说的来估计 优势函数 进行策略优化的算法</p>
<p><img src="https://my-picture-1311448338.file.myqcloud.com/img/202208151024506.png" alt="image-20220815102413408" style="zoom: 33%;" /></p>
<p>整个流程就是，有一个策略 <code>π</code>，有个初始的 actor 去跟环境做互动，先收集资料。在 policy gradient 方法里面收集资料以后，就要拿去更新 policy。但是在 actor-critic 方法里面，不是直接拿那些资料去更新 policy。而是先拿这些资料去估计价值函数，可以用 TD 或 MC 来估计价值函数，即：网络输出 或 遍历回合收集取平均，估计出的价值函数用于训练 Critic 网络。接下来，再基于价值函数，套用上面的梯度式子去更新策略 <code>π</code></p>
<p><strong>两个 tips：</strong></p>
<ol>
<li><p>因为 actor 和 critic 的输入都是 s，所以共享前面几个层(layer)，用于特征提取</p>
<p> <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131453341.png" alt="image-20220813145332277" style="zoom: 35%;" /></p>
</li>
<li><p>加一个探索(exploration)的机制。对 π 的输出的分布下一个约束。希望这个分布的熵(entropy)不要太小，也就是希望不同的动作被采用的概率平均一点。这样才会多尝试各种不同的动作，把环境探索地比较好</p>
</li>
</ol>
<h5 id="A3C算法：Asynchronous-Advantage-Actor-Critic"><a href="#A3C算法：Asynchronous-Advantage-Actor-Critic" class="headerlink" title="A3C算法：Asynchronous Advantage Actor-Critic"></a>A3C算法：Asynchronous Advantage Actor-Critic</h5><p><img src="https://my-picture-1311448338.file.myqcloud.com/img/20220209161905.png" alt="image-20220209161905574" style="zoom: 33%;" /></p>
<p>更新思路：</p>
<ul>
<li><p>A3C 一开始有一个 global network。上面提到，policy network 跟 value network 是绑(tie)在一起的，它们的前几个层会被绑一起。global network 包含 policy 的部分和 value 的部分</p>
</li>
<li><p>假设 global network 的参数是 θ1，这时开很多个 worker。每一个 worker 就用一张 CPU 去跑，每一个 worker 工作前都会把 global network 的参数复制过来。</p>
</li>
<li><p>接下来每个 worker 的 actor 跟环境做互动，能够收集到有多样性的数据</p>
</li>
<li><p>每一个 actor 跟环境互动完之后就会计算出梯度。计算出梯度以后就把梯度传回给中央控制中心，然后中央的控制中心就会拿这个梯度去更新 global network 的参数</p>
</li>
<li><p>每一个 actor 就是各做各的，不管彼此，这就是所谓 “Asynchronous”</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208241338496.png" alt="image-20220824133841379" style="zoom: 40%;" /></p>
</li>
</ul>
<h5 id="GAN-与-Actor-Critic"><a href="#GAN-与-Actor-Critic" class="headerlink" title="GAN 与 Actor-Critic"></a>GAN 与 Actor-Critic</h5><p>其实 GAN 跟 Actor-Critic 的方法是非常类似的，有一篇 paper 叫做 <code>Connecting Generative Adversarial Network and Actor-Critic Methods</code>。GAN 跟 Actor-Critic 都是以难训练而闻名的，这篇文献上收集了各式各样的方法，告诉说怎么样可以把 GAN 训练起来，怎么样可以把 Actor-Critic 训练起来。因为做 GAN 跟 Actor-Critic 的是两群人，所以这篇 paper 里面就列出说在 GAN 上面有哪些技术是有人做过的，在 Actor-Critic 上面，有哪些技术是有人做过的。在 GAN 上面有试过的技术，可以试着应用在 Actor-Critic 上，在 Actor-Critic 上面做过的技术，可以试着应用在 GAN 上面</p>
<h4 id="OnPolicy-策略梯度的不足"><a href="#OnPolicy-策略梯度的不足" class="headerlink" title="OnPolicy 策略梯度的不足"></a>OnPolicy 策略梯度的不足</h4><blockquote>
<p>以上 REINFORCE、Actor-Critic 算法都是 On Policy 的，每次更新都要重新与环境交互，效率低</p>
<p>PPO 的一大改进就行规避了这个问题，具有了 Off Policy 一次交互重复更新的优点</p>
</blockquote>
<p><strong>On-Policy 对比 Off-Policy：</strong></p>
<ul>
<li><p>如果要学习的 agent 跟和环境互动的 agent 是同一个的话， 这个叫做<code>on-policy(同策略)</code></p>
</li>
<li><p>如果要学习的 agent 跟和环境互动的 agent 不是同一个的话， 那这个叫做<code>off-policy(异策略)</code></p>
</li>
<li><p>由于 On-Policy 与环境互动和更新的是同一个 agent 的参数，这意味着每次更新后 agent 就不再是原来的那个 agent，之前与环境交互得到的采样也不在可用，因此需要重新交互获取用于更新的样本经验</p>
<p>  所以 policy gradient 是一个会花很多时间来采样数据的算法，大多数时间都在采样数据</p>
</li>
<li><p>但如果是 Off-Policy，用一个 policy 下的 actor θ′ 去跟环境做互动（θ′ 是固定的），以此收集到的数据去训练另一个 agent 的参数 θ（这里先不管为什么可以这样做），那么就可以把 θ′ 收集到的数据用非常多次，不需要每更新一次就交互一波</p>
</li>
</ul>
<h3 id="PPO-算法"><a href="#PPO-算法" class="headerlink" title="PPO 算法"></a>PPO 算法</h3><blockquote>
<p>简而言之，基于策略梯度（具体而言是 Actor-Critic 算法），PPO 采用<strong>重要性采样</strong> (Importance Sampling) 和 <strong>KL散度约束</strong>，规避了 On Policy 的弊端</p>
<p>优势函数的选取默认使用 GAE</p>
<p>另外，PPO 的前身是 TRPO，TRPO里把KL散度作为额外约束而不是惩罚项，因此并不易于学习，一般用PPO</p>
<p>详见：<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/easy-rl/#/chapter5/chapter5">EasyRL 近端策略优化 (PPO) 算法</a></p>
</blockquote>
<ul>
<li><p><strong>重要性采样</strong>：Importance Sampling</p>
<p>  对 p 采样的期望值可以通过对 q 采样乘一个 权重因子 后的期望来估算：</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131529996.png" alt="image-20220813152953929" style="zoom:50%;" /></p>
<p>  权重因子 <code>p(x)/q(x)</code> 称为“重要性权重”，相当于修正因子</p>
<p>  <strong>注意：</strong></p>
<ol>
<li><p>q(x) 的概率是 0 时，p(x) 的概率也要是 0</p>
</li>
<li><p>从 p 采样的期望 和 从 q 采样并修正后的期望相等，但是方差理论上是不等的！这是因为权重因子 <code>p(x)/q(x)</code> 的存在：</p>
<p> <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131535372.png" alt="image-20220813153518324" style="zoom: 50%;" /></p>
<p> 所以，如果 <code>p(x)/q(x)</code> 差距很大的话，<code>f(x)*p(x)/q(x)</code> 的方差就会很大。虽然理论上它们的期望值一样，也就是说，只要采样够多，得到的结果会是一样的。但是如果采样次数不够，因为它们的方差差距很大，所以可能得到非常大的差别。所以这个值越接近1越好，即 p 和 q 的分布越相似就越准确</p>
</li>
</ol>
</li>
</ul>
<ul>
<li><p>利用重要性采样，<strong>转换 On-Policy 为 Off-Policy</strong></p>
<p>  之前是拿 <code>θ</code> 这个 policy 去跟环境做互动，采样出轨迹 <code>τ</code>，然后计算 <code>R(τ)∇logpθ(τ)</code></p>
<p>  现在不用 <code>θ</code> 跟环境做互动，假设有另外一个 policy <code>θ′</code>，它就是另外一个 actor，去做示范给 <code>θ</code> 看，告诉  <code>θ</code> 说，它跟环境做互动会发生什么事，借此来训练 <code>θ</code></p>
<p>  利用重要性采样，为 <code>θ′</code> 与环境交互得到的奖励值的概率分布加上一项修正因子，就能得到 <code>θ</code> 的奖励值，从而更新  <code>θ</code>，策略梯度为：</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131543783.png" alt="image-20220813154346692" style="zoom: 33%;" /></p>
  <center>第三行约去的一项是因为认为状态出现的概率与参数无关，所以二者相等</center>

<p>  重要性因子的<strong>具体</strong>计算方法很简单，就是某一个轨迹 <code>τ</code> 用 <code>θ</code> 算出来的动作概率除以这个轨迹用 <code>θ′</code> 算出来的动作概率</p>
<p>  至此，已将 <code>θ′</code> 的采样转化为 <code>θ</code> 的等效采样，从而实现利用 actor‘ <code>θ′</code> 交互，演示给真正的 actor 看，对 <code>θ</code> 进行训练的效果</p>
</li>
</ul>
<ul>
<li><p><strong>优势函数</strong>计算：</p>
<p>  如同 Actor-Critic 算法中，优势函数同样采用 <strong>GAE</strong> (General Advantafe Estimate) ，即 动作价值减去状态价值：</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131437068.png" alt="image-20220813143744987" style="zoom: 33%;" /></p>
<p>  但是这里一般不需要像 A2C 算法那样把动作价值转化为状态价值，通常是对 agent‘ 与环境交互得到的数据使用蒙特卡洛法来估计得到动作价值，即带上折扣系数后求和，而状态价值就是Critic的输出</p>
</li>
</ul>
<ul>
<li><p><strong>KL 散度</strong>维持一致性</p>
<p>  上面说了，重要性采样效果准确的前提是，重要性因子 <code>p(x)/q(x)</code> 不能太大，越接近1越好，即 <code>θ′</code> 和 <code>θ</code> 对统一状态选出的动作尽可能相一致</p>
<p>  因此，PPO 加入了一个惩罚项，即：衡量上述二者距离的KL散度</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131600558.png" alt="image-20220813160039516" style="zoom:45%;" /></p>
<p>  具体而言，PPO 的更新公式在引入了惩罚项后为：</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131602761.png" alt="image-20220813160234702" style="zoom: 33%;" /></p>
  <center>β 是惩罚项的系数</center>

<p>  可以说，PPO 就是以上述公式对 Actor 网络  <code>θ</code>  进行更新的 Off-Policy 的 Actor-Critic 算法</p>
</li>
</ul>
<ul>
<li><p><strong>算法流程：</strong></p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131625776.png" alt="image-20220813162544663" style="zoom: 40%;" /></p>
<p>  第 6 行公式换成上述的即可，是更新 actor 的；第 7 行是 MSELoss 更新 Critic 网络</p>
</li>
</ul>
<h4 id="PPO1：PPO-penalty"><a href="#PPO1：PPO-penalty" class="headerlink" title="PPO1：PPO-penalty"></a>PPO1：PPO-penalty</h4><blockquote>
<p>PPO 的第一个变种</p>
</blockquote>
<p>基本与上述内容一致，惩罚系数 β 动态变化：</p>
<p><img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131611863.png" alt="image-20220813161153786" style="zoom: 33%;" /></p>
<h4 id="PPO2：PPO-Clip"><a href="#PPO2：PPO-Clip" class="headerlink" title="PPO2：PPO-Clip"></a>PPO2：PPO-Clip</h4><blockquote>
<p>PPO 的第二个变种，大部分 PPO 都是用的这个，而不是用 KL散度</p>
<p>详见：<a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#ppo">Policy Gradient Algorithms</a></p>
</blockquote>
<p>更新的公式与原始 PPO 有一定差异，可以看出没有使用 KL散度 作为惩罚项来保持两个 actor 的一致性，而是使用了另一种方法：</p>
<p><img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131706107.png" alt="image-20220813170626062" style="zoom:60%;" /></p>
<p>其实这样做是限制二者的比值在 1 附近，相当于钳制了二者的相似性，ε 是个超参数，一般取 0.1 或 0.2，作用为：</p>
<p><img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131616274.png" alt="image-20220813161623228" style="zoom:33%;" /></p>
<center>红色的线就是取 min 后的输出</center>

<p>但是要<strong>注意</strong>：</p>
<p>通常在更新网络参数时，Actor 和 Critic 使用同一个 loss 进行更新，并不是分开进行的，loss为：</p>
<p><img src="https://my-picture-1311448338.file.myqcloud.com/img/202208131735571.png" alt="image-20220813173538534" style="zoom: 67%;" /></p>
<p>其中第一项就是上面的优势函数项，评估Actor网络的表现；第二项是 Critic网络估计的状态价值 与 根据与环境交互得出的状态价值（用 TD 或 MC 来估计价值函数 <code>V_target</code>）间的MSE损失；第三项是策略输出动作的熵值，鼓励探索</p>
<p><strong>示例代码：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> MultivariateNormal</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Memory</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.actions = []</span><br><span class="line">        self.states = []</span><br><span class="line">        self.logprobs = [] <span class="comment"># log[p(a_t|s_t)]，用于计算重要性因子</span></span><br><span class="line">        self.action_values = [] <span class="comment"># 即 reward</span></span><br><span class="line">        self.is_terminals = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clear_memory</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">del</span> self.actions[:]</span><br><span class="line">        <span class="keyword">del</span> self.states[:]</span><br><span class="line">        <span class="keyword">del</span> self.logprobs[:]</span><br><span class="line">        <span class="keyword">del</span> self.action_values[:]</span><br><span class="line">        <span class="keyword">del</span> self.is_terminals[:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActorCritic</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_dim, action_dim, action_std</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ActorCritic, self).__init__()</span><br><span class="line">        self.actor = nn.Sequential(</span><br><span class="line">            nn.Linear(state_dim, <span class="number">64</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">32</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">32</span>, action_dim),</span><br><span class="line">            nn.Tanh() <span class="comment"># action mean range -1 to 1</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># critic</span></span><br><span class="line">        self.critic = nn.Sequential(</span><br><span class="line">            nn.Linear(state_dim, <span class="number">64</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">32</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">32</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 方差</span></span><br><span class="line">        self.action_var = torch.full((action_dim,), action_std * action_std).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 手动设置异常</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">act</span>(<span class="params">self, state, memory</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;执行act与环境交互的的是policy_old网络，也就是θ’而不是θ</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        action_mean = self.actor(state) <span class="comment"># 对于策略梯度，actor网络给出的是可选动作的概率分布，在这里就是每个动作对应正态分布的均值</span></span><br><span class="line">        cov_mat = torch.diag(self.action_var).to(device) <span class="comment"># 每个动作的正态分布标准差在这里为固定值</span></span><br><span class="line">        dist = MultivariateNormal(action_mean, cov_mat) <span class="comment"># 本例为连续动作，采用多元正态分布，目的是逐渐逼近真正的动作选择的正态分布</span></span><br><span class="line">        action = dist.sample() <span class="comment"># 从给定策略对应的动作概率分布中采样获得本次动作</span></span><br><span class="line">        action_logprob = dist.log_prob(action) <span class="comment"># 选择此动作对应的概率的log，用于计算重要性因子</span></span><br><span class="line"></span><br><span class="line">        memory.states.append(state)</span><br><span class="line">        memory.actions.append(action)</span><br><span class="line">        memory.logprobs.append(action_logprob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> action.detach()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params">self, state, action</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;执行evaluate从而进行训练更新的是policy网络，也就是θ</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        action_mean = self.actor(state)</span><br><span class="line">        action_var = self.action_var.expand_as(action_mean)</span><br><span class="line">        cov_mat = torch.diag_embed(action_var).to(device)</span><br><span class="line">        dist = MultivariateNormal(action_mean, cov_mat)</span><br><span class="line">        action_logprobs = dist.log_prob(action)</span><br><span class="line">        dist_entropy = dist.entropy()</span><br><span class="line">        state_value = self.critic(state)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> action_logprobs, torch.squeeze(state_value), dist_entropy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PPO</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;这里是PPO-clip算法</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip</span>):</span></span><br><span class="line">        self.lr = lr</span><br><span class="line">        self.betas = betas</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.eps_clip = eps_clip</span><br><span class="line">        self.K_epochs = K_epochs</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用于更新的agent θ</span></span><br><span class="line">        self.policy = ActorCritic(state_dim, action_dim, action_std).to(device)</span><br><span class="line">        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用于与环境交互的agent θ’</span></span><br><span class="line">        self.policy_old = ActorCritic(state_dim, action_dim, action_std).to(device)</span><br><span class="line">        self.policy_old.load_state_dict(self.policy.state_dict())</span><br><span class="line"></span><br><span class="line">        self.MseLoss = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">select_action</span>(<span class="params">self, state, memory</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;θ’ 与环境交互，选择动作</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        state = torch.FloatTensor(state.reshape(<span class="number">1</span>, -<span class="number">1</span>)).to(device)</span><br><span class="line">        <span class="keyword">return</span> self.policy_old.act(state, memory).cpu().data.numpy().flatten()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, memory</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;训练更新 θ 参数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Monte Carlo法 估计动作价值 action_values</span></span><br><span class="line">        action_values = []</span><br><span class="line">        discounted_reward = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> reward, is_terminal <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">reversed</span>(memory.action_values), <span class="built_in">reversed</span>(memory.is_terminals)):</span><br><span class="line">            <span class="keyword">if</span> is_terminal:</span><br><span class="line">                discounted_reward = <span class="number">0</span></span><br><span class="line">            discounted_reward = reward + (self.gamma * discounted_reward)</span><br><span class="line">            action_values.insert(<span class="number">0</span>, discounted_reward)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalizing the action_values:</span></span><br><span class="line">        action_values = torch.tensor(action_values, dtype=torch.float32).to(device)</span><br><span class="line">        action_values = (action_values - action_values.mean()) / (action_values.std() + <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert list to tensor</span></span><br><span class="line">        old_states = torch.squeeze(torch.stack(memory.states).to(device), <span class="number">1</span>).detach()</span><br><span class="line">        old_actions = torch.squeeze(torch.stack(memory.actions).to(device), <span class="number">1</span>).detach()</span><br><span class="line">        old_logprobs = torch.squeeze(torch.stack(memory.logprobs), <span class="number">1</span>).to(device).detach()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># agent θ’与环境交互一波后，agent θ 用获取的memory数据训练更新 K 轮</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.K_epochs):</span><br><span class="line">            <span class="comment"># Evaluating old actions and state_values:</span></span><br><span class="line">            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算重要性因子 (pi_theta / pi_theta__old)</span></span><br><span class="line">            ratios = torch.exp(logprobs - old_logprobs.detach())</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 按PPO-clip的公式计算策略梯度</span></span><br><span class="line">            advantages = action_values - state_values.detach()</span><br><span class="line">            surr1 = ratios * advantages</span><br><span class="line">            surr2 = torch.clamp(ratios, <span class="number">1</span> - self.eps_clip, <span class="number">1</span> + self.eps_clip) * advantages</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 这里总的loss = Actor网络的策略梯度 + Critic网络的MSE损失 + Actor选择动作的熵值（鼓励探索）</span></span><br><span class="line">            loss = -torch.<span class="built_in">min</span>(surr1, surr2) + <span class="number">0.5</span> * self.MseLoss(state_values, action_values) - <span class="number">0.01</span> * dist_entropy</span><br><span class="line"></span><br><span class="line">            <span class="comment"># take gradient step</span></span><br><span class="line">            self.optimizer.zero_grad()</span><br><span class="line">            loss.mean().backward()</span><br><span class="line">            self.optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Copy new weights into old policy: θ --&gt; θ’</span></span><br><span class="line">        self.policy_old.load_state_dict(self.policy.state_dict())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="comment">############## Hyperparameters ##############</span></span><br><span class="line">    env_name = <span class="string">&quot;BipedalWalker-v2&quot;</span></span><br><span class="line">    render = <span class="literal">False</span></span><br><span class="line">    solved_reward = <span class="number">300</span>  <span class="comment"># stop training if avg_reward &gt; solved_reward</span></span><br><span class="line">    log_interval = <span class="number">20</span>  <span class="comment"># print avg reward in the interval</span></span><br><span class="line">    max_episodes = <span class="number">10000</span>  <span class="comment"># max training episodes</span></span><br><span class="line">    max_timesteps = <span class="number">1500</span>  <span class="comment"># max timesteps in one episode</span></span><br><span class="line"></span><br><span class="line">    update_timestep = <span class="number">4000</span>  <span class="comment"># update policy every n timesteps</span></span><br><span class="line">    action_std = <span class="number">0.5</span>  <span class="comment"># constant std for action distribution (Multivariate Normal)</span></span><br><span class="line">    K_epochs = <span class="number">80</span>  <span class="comment"># update policy for K epochs</span></span><br><span class="line">    eps_clip = <span class="number">0.2</span>  <span class="comment"># clip parameter for PPO-clip</span></span><br><span class="line">    gamma = <span class="number">0.99</span>  <span class="comment"># discount factor</span></span><br><span class="line"></span><br><span class="line">    lr = <span class="number">0.0003</span>  <span class="comment"># parameters for Adam optimizer</span></span><br><span class="line">    betas = (<span class="number">0.9</span>, <span class="number">0.999</span>)</span><br><span class="line"></span><br><span class="line">    random_seed = <span class="literal">None</span></span><br><span class="line">    <span class="comment">#############################################</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># creating environment</span></span><br><span class="line">    env = gym.make(env_name)</span><br><span class="line">    state_dim = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line">    action_dim = env.action_space.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Random Seed</span></span><br><span class="line">    <span class="keyword">if</span> random_seed:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Random Seed: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(random_seed))</span><br><span class="line">        torch.manual_seed(random_seed)</span><br><span class="line">        env.seed(random_seed)</span><br><span class="line">        np.random.seed(random_seed)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Memory Pool and Agent</span></span><br><span class="line">    memory = Memory()</span><br><span class="line">    ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Logging variables</span></span><br><span class="line">    running_reward = <span class="number">0</span></span><br><span class="line">    avg_length = <span class="number">0</span></span><br><span class="line">    time_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training loop</span></span><br><span class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, max_episodes + <span class="number">1</span>):</span><br><span class="line">        state = env.reset()</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(max_timesteps):</span><br><span class="line">            time_step += <span class="number">1</span></span><br><span class="line">            <span class="comment"># Agent’ 与环境交互积累观测数据:</span></span><br><span class="line">            action = ppo.select_action(state, memory)</span><br><span class="line">            state, reward, done, _ = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Saving reward and is_terminals:</span></span><br><span class="line">            memory.action_values.append(reward)</span><br><span class="line">            memory.is_terminals.append(done)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 观测够一定次数后，Agent拿来训练多轮</span></span><br><span class="line">            <span class="keyword">if</span> time_step % update_timestep == <span class="number">0</span>:</span><br><span class="line">                ppo.update(memory)</span><br><span class="line">                memory.clear_memory()</span><br><span class="line">                time_step = <span class="number">0</span></span><br><span class="line">            running_reward += reward</span><br><span class="line">            <span class="keyword">if</span> render:</span><br><span class="line">                env.render()</span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        avg_length += t</span><br><span class="line"></span><br><span class="line">        <span class="comment"># stop training if avg_reward &gt; solved_reward</span></span><br><span class="line">        <span class="keyword">if</span> running_reward &gt; (log_interval * solved_reward):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;########## Solved! ##########&quot;</span>)</span><br><span class="line">            torch.save(ppo.policy.state_dict(), <span class="string">&#x27;./PPO_continuous_solved_&#123;&#125;.pth&#x27;</span>.<span class="built_in">format</span>(env_name))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># save every 500 episodes</span></span><br><span class="line">        <span class="keyword">if</span> i_episode % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            torch.save(ppo.policy.state_dict(), <span class="string">&#x27;./PPO_continuous_&#123;&#125;.pth&#x27;</span>.<span class="built_in">format</span>(env_name))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># logging</span></span><br><span class="line">        <span class="keyword">if</span> i_episode % log_interval == <span class="number">0</span>:</span><br><span class="line">            avg_length = <span class="built_in">int</span>(avg_length / log_interval)</span><br><span class="line">            running_reward = <span class="built_in">int</span>((running_reward / log_interval))</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Episode &#123;&#125; \t Avg length: &#123;&#125; \t Avg reward: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i_episode, avg_length, running_reward))</span><br><span class="line">            running_reward = <span class="number">0</span></span><br><span class="line">            avg_length = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>&nbsp;</p>
<h2 id="SAC"><a href="#SAC" class="headerlink" title="SAC"></a>SAC</h2><blockquote>
<p>Soft Actor Critic 是一种基于最大熵的RL算法</p>
<p>是 Off-Policy 的算法</p>
<p>属于随机策略梯度，在普通 Actor-Critic 基础上做了改进：<strong>最大熵约束</strong> 和 <strong>基于能量的策略</strong></p>
</blockquote>
<h3 id="先导知识-1"><a href="#先导知识-1" class="headerlink" title="先导知识"></a>先导知识</h3><blockquote>
<p>要懂 PPO，先理解最大熵思想的RL算法有什么特点</p>
</blockquote>
<h4 id="基于最大熵的RL算法"><a href="#基于最大熵的RL算法" class="headerlink" title="基于最大熵的RL算法"></a>基于最大熵的RL算法</h4><ul>
<li><p><strong>熵</strong>：衡量策略网络Actor所选动作的随机性</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208171618913.png" alt="image-20220817161822866" style="zoom: 50%;" /></p>
</li>
<li><p>引入最大熵后的<strong>目标策略</strong>：</p>
<p>  比原本的RL算法，优化目标不止是最大化累积奖励，同时也最大化策略的熵值</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208171646069.png" alt="image-20220817164642027" style="zoom: 67%;" /></p>
  <center>α 是温度系数，调整对熵值的重视程度</center>
</li>
<li><p>动作价值 Ｑ 和 状态价值 Ｖ 的新定义：</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208171632549.png" alt="image-20220817163219514" style="zoom: 60%;" /></p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208171632432.png" alt="image-20220817163229392" style="zoom: 60%;" /></p>
<p>  二者递推关系：</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208171633260.png" alt="image-20220817163327218" style="zoom: 60%;" /></p>
<p>  一般算法中不会用两个网络分别估计 Ｑ 和 Ｖ，更多是估计 Ｑ 然后用递推式计算 Ｖ</p>
</li>
</ul>
<h3 id="SAC-算法"><a href="#SAC-算法" class="headerlink" title="SAC 算法"></a>SAC 算法</h3><blockquote>
<p>基本上就是引入最大熵思想的 Actor-Critic 算法，目的是保证探索性</p>
</blockquote>
<p><strong>核心概念：</strong></p>
<ul>
<li><p><strong>Critic 网络</strong>的更新：(价值迭代)</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208171639923.png" alt="image-20220817163924881" style="zoom:67%;" /></p>
  <center>带有熵项的贝尔曼方程</center>

<p>  实际损失函数为：（MSELoss）</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208171649725.png" alt="image-20220817164956685" style="zoom:67%;" /></p>
</li>
<li><p><strong>Actor 网络</strong>的更新：(策略优化)</p>
<p>  最小化 KL散度</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208171651283.png" alt="image-20220817165113253" style="zoom:67%;" /></p>
</li>
</ul>
<p>&nbsp;</p>
<h2 id="TD3"><a href="#TD3" class="headerlink" title="TD3"></a>TD3</h2><blockquote>
<p>双延迟深度确定性策略梯度算法，Twin Delayed Deep Deterministic Policy Gradient</p>
<p>属于确定性策略梯度，在普通 DDPG 基础上做了改进：<strong>双 Critic 估计</strong>、<strong>延迟策略更新</strong> 和 <strong>目标策略平滑</strong></p>
<p>详见：<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/easy-rl/#/chapter12/chapter12?id=twin-delayed-ddpgtd3">Twin Delayed DDPG</a></p>
</blockquote>
<h3 id="先导知识-2"><a href="#先导知识-2" class="headerlink" title="先导知识"></a>先导知识</h3><h4 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h4><ul>
<li><p>基于价值的算法，目标是训练Q网络以准确估计状态动作价值（Q值），以供决策</p>
</li>
<li><p>训练方法大多采用时序差分，从而可以单步训练（蒙特卡洛的话需要回合制训练，并且方差大）</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208171112205.png" alt="image-20220817111225104" style="zoom: 67%;" /></p>
</li>
</ul>
<h4 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h4><ul>
<li><p>基于策略的算法，改进了DQN只能处理离散动作空间的问题，与 Actor-Critic 的最大区别是<strong>确定性</strong>策略</p>
</li>
<li><p>引入 Actor 网络，代替 DQN 中的 ε-greedy 策略进行动作的选择</p>
</li>
<li><p>训练目标是让 Critic 网络准确估计状态动作价值（同DQN），并且让 Actor 能选出状态动作价值最大的动作</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208171116684.png" alt="image-20220817111603555" style="zoom: 35%;" /></p>
</li>
<li><p>前期在 Actor 的输出上加入噪声以保证探索性</p>
</li>
<li><p>官方论文算法的代码实现</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Implementation of Deep Deterministic Policy Gradients (DDPG)</span></span><br><span class="line"><span class="comment"># Paper: https://arxiv.org/abs/1509.02971</span></span><br><span class="line"><span class="comment"># [Not the implementation used in the TD3 paper]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_dim, action_dim, max_action</span>):</span></span><br><span class="line">		<span class="built_in">super</span>(Actor, self).__init__()</span><br><span class="line"></span><br><span class="line">		self.l1 = nn.Linear(state_dim, <span class="number">400</span>)</span><br><span class="line">		self.l2 = nn.Linear(<span class="number">400</span>, <span class="number">300</span>)</span><br><span class="line">		self.l3 = nn.Linear(<span class="number">300</span>, action_dim)</span><br><span class="line">		</span><br><span class="line">		self.max_action = max_action</span><br><span class="line"></span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, state</span>):</span></span><br><span class="line">		a = F.relu(self.l1(state))</span><br><span class="line">		a = F.relu(self.l2(a))</span><br><span class="line">		<span class="keyword">return</span> self.max_action * torch.tanh(self.l3(a))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_dim, action_dim</span>):</span></span><br><span class="line">		<span class="built_in">super</span>(Critic, self).__init__()</span><br><span class="line"></span><br><span class="line">		self.l1 = nn.Linear(state_dim, <span class="number">400</span>)</span><br><span class="line">		self.l2 = nn.Linear(<span class="number">400</span> + action_dim, <span class="number">300</span>)</span><br><span class="line">		self.l3 = nn.Linear(<span class="number">300</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, state, action</span>):</span></span><br><span class="line">		q = F.relu(self.l1(state))</span><br><span class="line">		q = F.relu(self.l2(torch.cat([q, action], <span class="number">1</span>)))</span><br><span class="line">		<span class="keyword">return</span> self.l3(q)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DDPG</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_dim, action_dim, max_action, discount=<span class="number">0.99</span>, tau=<span class="number">0.001</span></span>):</span></span><br><span class="line">		self.actor = Actor(state_dim, action_dim, max_action).to(device)</span><br><span class="line">		self.actor_target = copy.deepcopy(self.actor)</span><br><span class="line">		self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line">		self.critic = Critic(state_dim, action_dim).to(device)</span><br><span class="line">		self.critic_target = copy.deepcopy(self.critic)</span><br><span class="line">		self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), weight_decay=<span class="number">1e-2</span>)</span><br><span class="line"></span><br><span class="line">		self.discount = discount</span><br><span class="line">		self.tau = tau</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">select_action</span>(<span class="params">self, state</span>):</span></span><br><span class="line">		state = torch.FloatTensor(state.reshape(<span class="number">1</span>, -<span class="number">1</span>)).to(device)</span><br><span class="line">		<span class="keyword">return</span> self.actor(state).cpu().data.numpy().flatten()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self, replay_buffer, batch_size=<span class="number">64</span></span>):</span></span><br><span class="line">		<span class="comment"># Sample replay buffer </span></span><br><span class="line">		state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Compute the target Q value</span></span><br><span class="line">		target_Q = self.critic_target(next_state, self.actor_target(next_state))</span><br><span class="line">		target_Q = reward + (not_done * self.discount * target_Q).detach()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Get current Q estimate</span></span><br><span class="line">		current_Q = self.critic(state, action)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Compute critic loss</span></span><br><span class="line">		critic_loss = F.mse_loss(current_Q, target_Q)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Optimize the critic</span></span><br><span class="line">		self.critic_optimizer.zero_grad()</span><br><span class="line">		critic_loss.backward()</span><br><span class="line">		self.critic_optimizer.step()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Compute actor loss</span></span><br><span class="line">		actor_loss = -self.critic(state, self.actor(state)).mean()</span><br><span class="line">		</span><br><span class="line">		<span class="comment"># Optimize the actor </span></span><br><span class="line">		self.actor_optimizer.zero_grad()</span><br><span class="line">		actor_loss.backward()</span><br><span class="line">		self.actor_optimizer.step()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Update the frozen target models</span></span><br><span class="line">		<span class="keyword">for</span> param, target_param <span class="keyword">in</span> <span class="built_in">zip</span>(self.critic.parameters(), self.critic_target.parameters()):</span><br><span class="line">			target_param.data.copy_(self.tau * param.data + (<span class="number">1</span> - self.tau) * target_param.data)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> param, target_param <span class="keyword">in</span> <span class="built_in">zip</span>(self.actor.parameters(), self.actor_target.parameters()):</span><br><span class="line">			target_param.data.copy_(self.tau * param.data + (<span class="number">1</span> - self.tau) * target_param.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">save</span>(<span class="params">self, filename</span>):</span></span><br><span class="line">		torch.save(self.critic.state_dict(), filename + <span class="string">&quot;_critic&quot;</span>)</span><br><span class="line">		torch.save(self.critic_optimizer.state_dict(), filename + <span class="string">&quot;_critic_optimizer&quot;</span>)</span><br><span class="line">		</span><br><span class="line">		torch.save(self.actor.state_dict(), filename + <span class="string">&quot;_actor&quot;</span>)</span><br><span class="line">		torch.save(self.actor_optimizer.state_dict(), filename + <span class="string">&quot;_actor_optimizer&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">load</span>(<span class="params">self, filename</span>):</span></span><br><span class="line">		self.critic.load_state_dict(torch.load(filename + <span class="string">&quot;_critic&quot;</span>))</span><br><span class="line">		self.critic_optimizer.load_state_dict(torch.load(filename + <span class="string">&quot;_critic_optimizer&quot;</span>))</span><br><span class="line">		self.critic_target = copy.deepcopy(self.critic)</span><br><span class="line"></span><br><span class="line">		self.actor.load_state_dict(torch.load(filename + <span class="string">&quot;_actor&quot;</span>))</span><br><span class="line">		self.actor_optimizer.load_state_dict(torch.load(filename + <span class="string">&quot;_actor_optimizer&quot;</span>))</span><br><span class="line">		self.actor_target = copy.deepcopy(self.actor)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="TD3-算法"><a href="#TD3-算法" class="headerlink" title="TD3 算法"></a>TD3 算法</h3><blockquote>
<p>DDPG 常见的问题是已经学习好的 Q 函数开始显著地高估 Q 值，然后导致策略选不出好动作</p>
</blockquote>
<p><strong>TD3的改进之处：</strong></p>
<ul>
<li><p><strong>截断的双 Q 学习 (Clipped Dobule Q-learning)</strong> </p>
<p>  学习两个 Q-function（因此名字中有 “twin”），通过最小化均方误差来同时学习两个 Q-function：Q_ϕ1 和 Q_ϕ2，评价和更新时取二者中的较小者</p>
<p>  更新时使用同一个目标函数：</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208171326974.png" alt="image-20220817132652933" style="zoom:67%;" /></p>
</li>
<li><p><strong>延迟的策略更新 (“Delayed” Policy Updates)</strong></p>
<p>  同步训练动作网络和评价网络，却不使用目标网络，会导致训练过程不稳定；但是仅固定动作网络时，评价网络往往能够收敛到正确的结果</p>
<p>  因此 TD3 算法以较低的频率更新动作网络，较高频率更新评价网络，通常每更新两次评价网络就更新一次策略</p>
</li>
<li><p><strong>目标策略平滑 (Target Policy smoothing)</strong></p>
<p>  引入了 smoothing 的思想，在目标动作中加入噪音，通过平滑 Q 沿动作的变化，使策略更难被 Critic估计 Q 的误差影响：</p>
<p>  <img src="https://my-picture-1311448338.file.myqcloud.com/img/202208171332044.png" alt="image-20220817133204995" style="zoom:67%;" /></p>
<p>  其中，ε 是正态分布噪声项</p>
</li>
</ul>
<p><strong>示例代码：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参考官方仓库：https://github.com/sfujim/TD3/</span></span><br><span class="line"><span class="comment"># TD3作者实现时，基于的DDPG和DDPG原始算法在网络参数和学习率上有一定区别，详见其仓库下ourDDPG.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################################### utils ####################################</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_dim, action_dim, max_size=<span class="built_in">int</span>(<span class="params"><span class="number">1e6</span></span>)</span>):</span></span><br><span class="line">		self.max_size = max_size</span><br><span class="line">		self.ptr = <span class="number">0</span></span><br><span class="line">		self.size = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">		self.state = np.zeros((max_size, state_dim))</span><br><span class="line">		self.action = np.zeros((max_size, action_dim))</span><br><span class="line">		self.next_state = np.zeros((max_size, state_dim))</span><br><span class="line">		self.reward = np.zeros((max_size, <span class="number">1</span>))</span><br><span class="line">		self.not_done = np.zeros((max_size, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">		self.device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">self, state, action, next_state, reward, done</span>):</span></span><br><span class="line">		self.state[self.ptr] = state</span><br><span class="line">		self.action[self.ptr] = action</span><br><span class="line">		self.next_state[self.ptr] = next_state</span><br><span class="line">		self.reward[self.ptr] = reward</span><br><span class="line">		self.not_done[self.ptr] = <span class="number">1.</span> - done</span><br><span class="line"></span><br><span class="line">		self.ptr = (self.ptr + <span class="number">1</span>) % self.max_size</span><br><span class="line">		self.size = <span class="built_in">min</span>(self.size + <span class="number">1</span>, self.max_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, batch_size</span>):</span></span><br><span class="line">		ind = np.random.randint(<span class="number">0</span>, self.size, size=batch_size)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> (</span><br><span class="line">			torch.FloatTensor(self.state[ind]).to(self.device),</span><br><span class="line">			torch.FloatTensor(self.action[ind]).to(self.device),</span><br><span class="line">			torch.FloatTensor(self.next_state[ind]).to(self.device),</span><br><span class="line">			torch.FloatTensor(self.reward[ind]).to(self.device),</span><br><span class="line">			torch.FloatTensor(self.not_done[ind]).to(self.device)</span><br><span class="line">		)</span><br><span class="line"></span><br><span class="line"><span class="comment">#################################### TD3 #####################################</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_dim, action_dim, max_action</span>):</span></span><br><span class="line">		<span class="built_in">super</span>(Actor, self).__init__()</span><br><span class="line"></span><br><span class="line">		self.l1 = nn.Linear(state_dim, <span class="number">256</span>)</span><br><span class="line">		self.l2 = nn.Linear(<span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">		self.l3 = nn.Linear(<span class="number">256</span>, action_dim)</span><br><span class="line">		</span><br><span class="line">		self.max_action = max_action</span><br><span class="line">		</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, state</span>):</span></span><br><span class="line">		a = F.relu(self.l1(state))</span><br><span class="line">		a = F.relu(self.l2(a))</span><br><span class="line">		<span class="keyword">return</span> self.max_action * torch.tanh(self.l3(a))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_dim, action_dim</span>):</span></span><br><span class="line">		<span class="built_in">super</span>(Critic, self).__init__()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Q1 architecture</span></span><br><span class="line">		self.l1 = nn.Linear(state_dim + action_dim, <span class="number">256</span>)</span><br><span class="line">		self.l2 = nn.Linear(<span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">		self.l3 = nn.Linear(<span class="number">256</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Q2 architecture</span></span><br><span class="line">		self.l4 = nn.Linear(state_dim + action_dim, <span class="number">256</span>)</span><br><span class="line">		self.l5 = nn.Linear(<span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">		self.l6 = nn.Linear(<span class="number">256</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, state, action</span>):</span></span><br><span class="line">		sa = torch.cat([state, action], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">		q1 = F.relu(self.l1(sa))</span><br><span class="line">		q1 = F.relu(self.l2(q1))</span><br><span class="line">		q1 = self.l3(q1)</span><br><span class="line"></span><br><span class="line">		q2 = F.relu(self.l4(sa))</span><br><span class="line">		q2 = F.relu(self.l5(q2))</span><br><span class="line">		q2 = self.l6(q2)</span><br><span class="line">		<span class="keyword">return</span> q1, q2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">Q1</span>(<span class="params">self, state, action</span>):</span></span><br><span class="line">		sa = torch.cat([state, action], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">		q1 = F.relu(self.l1(sa))</span><br><span class="line">		q1 = F.relu(self.l2(q1))</span><br><span class="line">		q1 = self.l3(q1)</span><br><span class="line">		<span class="keyword">return</span> q1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TD3</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">		self,</span></span></span><br><span class="line"><span class="params"><span class="function">		state_dim,</span></span></span><br><span class="line"><span class="params"><span class="function">		action_dim,</span></span></span><br><span class="line"><span class="params"><span class="function">		max_action,</span></span></span><br><span class="line"><span class="params"><span class="function">		discount=<span class="number">0.99</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		tau=<span class="number">0.005</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		policy_noise=<span class="number">0.2</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		noise_clip=<span class="number">0.5</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		policy_freq=<span class="number">2</span></span></span></span><br><span class="line"><span class="params"><span class="function">	</span>):</span></span><br><span class="line"></span><br><span class="line">		self.actor = Actor(state_dim, action_dim, max_action).to(device)</span><br><span class="line">		self.actor_target = copy.deepcopy(self.actor)</span><br><span class="line">		self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=<span class="number">3e-4</span>)</span><br><span class="line"></span><br><span class="line">		self.critic = Critic(state_dim, action_dim).to(device)</span><br><span class="line">		self.critic_target = copy.deepcopy(self.critic)</span><br><span class="line">		self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=<span class="number">3e-4</span>)</span><br><span class="line"></span><br><span class="line">		self.max_action = max_action</span><br><span class="line">		self.discount = discount</span><br><span class="line">		self.tau = tau</span><br><span class="line">		self.policy_noise = policy_noise</span><br><span class="line">		self.noise_clip = noise_clip</span><br><span class="line">		self.policy_freq = policy_freq</span><br><span class="line"></span><br><span class="line">		self.total_it = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">select_action</span>(<span class="params">self, state</span>):</span></span><br><span class="line">		state = torch.FloatTensor(state.reshape(<span class="number">1</span>, -<span class="number">1</span>)).to(device)</span><br><span class="line">		<span class="keyword">return</span> self.actor(state).cpu().data.numpy().flatten()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self, replay_buffer, batch_size=<span class="number">256</span></span>):</span></span><br><span class="line">		self.total_it += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">		<span class="comment"># Sample replay buffer </span></span><br><span class="line">		state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">with</span> torch.no_grad():</span><br><span class="line">			<span class="comment"># Select action according to policy and add clipped noise</span></span><br><span class="line">			noise = (</span><br><span class="line">				torch.randn_like(action) * self.policy_noise</span><br><span class="line">			).clamp(-self.noise_clip, self.noise_clip)</span><br><span class="line">			</span><br><span class="line">			next_action = (</span><br><span class="line">				self.actor_target(next_state) + noise</span><br><span class="line">			).clamp(-self.max_action, self.max_action)</span><br><span class="line"></span><br><span class="line">			<span class="comment"># Compute the target Q value</span></span><br><span class="line">			target_Q1, target_Q2 = self.critic_target(next_state, next_action)</span><br><span class="line">			target_Q = torch.<span class="built_in">min</span>(target_Q1, target_Q2)</span><br><span class="line">			target_Q = reward + not_done * self.discount * target_Q</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Get current Q estimates</span></span><br><span class="line">		current_Q1, current_Q2 = self.critic(state, action)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Compute critic loss</span></span><br><span class="line">		critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Optimize the critic</span></span><br><span class="line">		self.critic_optimizer.zero_grad()</span><br><span class="line">		critic_loss.backward()</span><br><span class="line">		self.critic_optimizer.step()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Delayed policy updates</span></span><br><span class="line">		<span class="keyword">if</span> self.total_it % self.policy_freq == <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">			<span class="comment"># Compute actor losse</span></span><br><span class="line">			actor_loss = -self.critic.Q1(state, self.actor(state)).mean()</span><br><span class="line">			</span><br><span class="line">			<span class="comment"># Optimize the actor </span></span><br><span class="line">			self.actor_optimizer.zero_grad()</span><br><span class="line">			actor_loss.backward()</span><br><span class="line">			self.actor_optimizer.step()</span><br><span class="line"></span><br><span class="line">			<span class="comment"># Update the frozen target models</span></span><br><span class="line">			<span class="keyword">for</span> param, target_param <span class="keyword">in</span> <span class="built_in">zip</span>(self.critic.parameters(), self.critic_target.parameters()):</span><br><span class="line">				target_param.data.copy_(self.tau * param.data + (<span class="number">1</span> - self.tau) * target_param.data)</span><br><span class="line"></span><br><span class="line">			<span class="keyword">for</span> param, target_param <span class="keyword">in</span> <span class="built_in">zip</span>(self.actor.parameters(), self.actor_target.parameters()):</span><br><span class="line">				target_param.data.copy_(self.tau * param.data + (<span class="number">1</span> - self.tau) * target_param.data)</span><br><span class="line"></span><br><span class="line"><span class="comment">################################### Train ####################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Runs policy for X episodes and returns average reward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_policy</span>(<span class="params">policy, env_name, seed, eval_episodes=<span class="number">10</span></span>):</span></span><br><span class="line">	eval_env = gym.make(env_name)</span><br><span class="line">	eval_env.seed(seed + <span class="number">100</span>) <span class="comment"># fixed seed is used for the eval environment</span></span><br><span class="line"></span><br><span class="line">	avg_reward = <span class="number">0.</span></span><br><span class="line">	<span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(eval_episodes):</span><br><span class="line">		state, done = eval_env.reset(), <span class="literal">False</span></span><br><span class="line">		<span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">			action = policy.select_action(np.array(state))</span><br><span class="line">			state, reward, done, _ = eval_env.step(action)</span><br><span class="line">			avg_reward += reward</span><br><span class="line"></span><br><span class="line">	avg_reward /= eval_episodes</span><br><span class="line"></span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;---------------------------------------&quot;</span>)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&quot;Evaluation over <span class="subst">&#123;eval_episodes&#125;</span> episodes: <span class="subst">&#123;avg_reward:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;---------------------------------------&quot;</span>)</span><br><span class="line">	<span class="keyword">return</span> avg_reward</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line"></span><br><span class="line">    random_seed = <span class="number">0</span> <span class="comment"># Sets Gym, PyTorch and Numpy seeds</span></span><br><span class="line">    start_timesteps = <span class="number">25e3</span> <span class="comment"># Time steps initial random policy is used</span></span><br><span class="line">    eval_freq = <span class="number">5e3</span> <span class="comment"># How often (time steps) we evaluate</span></span><br><span class="line">    max_timesteps = <span class="number">1e6</span> <span class="comment"># Max time steps to run environment</span></span><br><span class="line">    expl_noise = <span class="number">0.1</span> <span class="comment"># Std of Gaussian exploration noise</span></span><br><span class="line">    batch_size = <span class="number">256</span> <span class="comment"># Batch size for both actor and critic</span></span><br><span class="line">    discount = <span class="number">0.99</span> <span class="comment"># Discount factor</span></span><br><span class="line">    tau = <span class="number">0.005</span> <span class="comment"># Target network update rate</span></span><br><span class="line">    policy_noise = <span class="number">0.2</span> <span class="comment"># Noise added to target policy during critic update</span></span><br><span class="line">    noise_clip = <span class="number">0.5</span> <span class="comment"># Range to clip target policy noise</span></span><br><span class="line">    policy_freq = <span class="number">2</span> <span class="comment"># Frequency of delayed policy updates</span></span><br><span class="line"></span><br><span class="line">	env = gym.make(<span class="string">&quot;HalfCheetah-v2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set seeds</span></span><br><span class="line">	env.seed(random_seed)</span><br><span class="line">	env.action_space.seed(random_seed)</span><br><span class="line">	torch.manual_seed(random_seed)</span><br><span class="line">	np.random.seed(random_seed)</span><br><span class="line">	</span><br><span class="line">	state_dim = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line">	action_dim = env.action_space.shape[<span class="number">0</span>] </span><br><span class="line">	max_action = <span class="built_in">float</span>(env.action_space.high[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">	kwargs = &#123;</span><br><span class="line">		<span class="string">&quot;state_dim&quot;</span>: state_dim,</span><br><span class="line">		<span class="string">&quot;action_dim&quot;</span>: action_dim,</span><br><span class="line">		<span class="string">&quot;max_action&quot;</span>: max_action,</span><br><span class="line">        <span class="string">&quot;discount&quot;</span>: discount,</span><br><span class="line">		<span class="string">&quot;tau&quot;</span>: tau,</span><br><span class="line">        <span class="string">&quot;policy_noise&quot;</span>: policy_noise * max_action,</span><br><span class="line">        <span class="string">&quot;noise_clip&quot;</span>: noise_clip * max_action,</span><br><span class="line">        <span class="string">&quot;policy_freq&quot;</span>: policy_freq,</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment"># Initialize policy</span></span><br><span class="line">	policy = TD3.TD3(**kwargs)</span><br><span class="line"></span><br><span class="line">	replay_buffer = ReplayBuffer(state_dim, action_dim)</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># Evaluate untrained policy</span></span><br><span class="line">	evaluations = [eval_policy(policy, env)]</span><br><span class="line"></span><br><span class="line">	state, done = env.reset(), <span class="literal">False</span></span><br><span class="line">	episode_reward = <span class="number">0</span></span><br><span class="line">	episode_timesteps = <span class="number">0</span></span><br><span class="line">	episode_num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(max_timesteps)):</span><br><span class="line">		</span><br><span class="line">		episode_timesteps += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">		<span class="comment"># Select action randomly or according to policy</span></span><br><span class="line">		<span class="keyword">if</span> t &lt; start_timesteps:</span><br><span class="line">			action = env.action_space.sample()</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			action = (</span><br><span class="line">				policy.select_action(np.array(state))</span><br><span class="line">				+ np.random.normal(<span class="number">0</span>, max_action * expl_noise, size=action_dim)</span><br><span class="line">			).clip(-max_action, max_action)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Perform action</span></span><br><span class="line">		next_state, reward, done, _ = env.step(action) </span><br><span class="line">		done_bool = <span class="built_in">float</span>(done) <span class="keyword">if</span> episode_timesteps &lt; env._max_episode_steps <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">		<span class="comment"># Store data in replay buffer</span></span><br><span class="line">		replay_buffer.add(state, action, next_state, reward, done_bool)</span><br><span class="line"></span><br><span class="line">		state = next_state</span><br><span class="line">		episode_reward += reward</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Train agent after collecting sufficient data</span></span><br><span class="line">		<span class="keyword">if</span> t &gt;= start_timesteps:</span><br><span class="line">			policy.train(replay_buffer, batch_size)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> done: </span><br><span class="line">			<span class="comment"># +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True</span></span><br><span class="line">			<span class="built_in">print</span>(<span class="string">f&quot;Total T: <span class="subst">&#123;t+<span class="number">1</span>&#125;</span> Episode Num: <span class="subst">&#123;episode_num+<span class="number">1</span>&#125;</span> Episode T: <span class="subst">&#123;episode_timesteps&#125;</span> Reward: <span class="subst">&#123;episode_reward:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br><span class="line">			<span class="comment"># Reset environment</span></span><br><span class="line">			state, done = env.reset(), <span class="literal">False</span></span><br><span class="line">			episode_reward = <span class="number">0</span></span><br><span class="line">			episode_timesteps = <span class="number">0</span></span><br><span class="line">			episode_num += <span class="number">1</span> </span><br><span class="line"></span><br><span class="line">		<span class="comment"># Evaluate episode</span></span><br><span class="line">		<span class="keyword">if</span> (t + <span class="number">1</span>) % eval_freq == <span class="number">0</span>:</span><br><span class="line">            evaluations.append(eval_policy(policy, env, random_seed))</span><br></pre></td></tr></table></figure>
<p>&nbsp;</p>
<h1 id="算法的选择经验"><a href="#算法的选择经验" class="headerlink" title="算法的选择经验"></a>算法的选择经验</h1><blockquote>
<p>参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/342919579">如何选择深度强化学习算法</a></p>
</blockquote>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Lrk612
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://sharp-rookie.github.io/2022/08/17/%E3%80%90%E6%8A%80%E6%9C%AF%E3%80%91%E5%A5%BD%E7%94%A8%E7%9A%84RL%E7%AE%97%E6%B3%95/" title="经典好用的RL算法——“开盖即食”">https://sharp-rookie.github.io/2022/08/17/【技术】好用的RL算法/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Algorithm/" rel="tag"># Algorithm</a>
              <a href="/tags/%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98/" rel="tag"># 优化问题</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/07/23/%E3%80%90%E8%AE%BA%E6%96%87%E3%80%91%E4%B8%80%E7%A7%8D%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%A7%86%E9%A2%91%E5%8E%8B%E7%BC%A9%E6%96%B9%E6%B3%95/" rel="prev" title="一种端到端的深度学习视频压缩方法">
      <i class="fa fa-chevron-left"></i> 一种端到端的深度学习视频压缩方法
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%8F%E5%85%B8%E8%80%8C%E5%A5%BD%E7%94%A8%E7%9A%84RL%E7%AE%97%E6%B3%95"><span class="nav-text">经典而好用的RL算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#PPO"><span class="nav-text">PPO</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%88%E5%AF%BC%E7%9F%A5%E8%AF%86"><span class="nav-text">先导知识</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-text">策略梯度基本概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%EF%BC%9AActor-Critic"><span class="nav-text">策略梯度经典算法：Actor-Critic</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#A2C%E7%AE%97%E6%B3%95%EF%BC%9AAdvantage-Actor-Critic"><span class="nav-text">A2C算法：Advantage Actor-Critic</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#A3C%E7%AE%97%E6%B3%95%EF%BC%9AAsynchronous-Advantage-Actor-Critic"><span class="nav-text">A3C算法：Asynchronous Advantage Actor-Critic</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GAN-%E4%B8%8E-Actor-Critic"><span class="nav-text">GAN 与 Actor-Critic</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OnPolicy-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E4%B8%8D%E8%B6%B3"><span class="nav-text">OnPolicy 策略梯度的不足</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PPO-%E7%AE%97%E6%B3%95"><span class="nav-text">PPO 算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#PPO1%EF%BC%9APPO-penalty"><span class="nav-text">PPO1：PPO-penalty</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PPO2%EF%BC%9APPO-Clip"><span class="nav-text">PPO2：PPO-Clip</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SAC"><span class="nav-text">SAC</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%88%E5%AF%BC%E7%9F%A5%E8%AF%86-1"><span class="nav-text">先导知识</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E7%9A%84RL%E7%AE%97%E6%B3%95"><span class="nav-text">基于最大熵的RL算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SAC-%E7%AE%97%E6%B3%95"><span class="nav-text">SAC 算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TD3"><span class="nav-text">TD3</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%88%E5%AF%BC%E7%9F%A5%E8%AF%86-2"><span class="nav-text">先导知识</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DQN"><span class="nav-text">DQN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DDPG"><span class="nav-text">DDPG</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TD3-%E7%AE%97%E6%B3%95"><span class="nav-text">TD3 算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E7%9A%84%E9%80%89%E6%8B%A9%E7%BB%8F%E9%AA%8C"><span class="nav-text">算法的选择经验</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Lrk612"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Lrk612</p>
  <div class="site-description" itemprop="description">Lrk's blog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">61</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.google.com/" title="https:&#x2F;&#x2F;www.google.com" rel="noopener" target="_blank">Google</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://arxiv.org/" title="https:&#x2F;&#x2F;arxiv.org&#x2F;" rel="noopener" target="_blank">arXiv</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://ieeexplore.ieee.org/Xplore/home.jsp" title="https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;Xplore&#x2F;home.jsp" rel="noopener" target="_blank">IEEE Xplore</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <!-- 访问量 -->

    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


<!-- 版权 -->
<div class="copyright">
  
  &copy; 2021-12-1 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lrk612</span>
</div>

<!-- ICP备案 -->
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">ICP备案 </a>
      <img src="https://my-picture-1311448338.file.myqcloud.com/img/20211209170739.png" style="display: inline-block;"><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=2021035798" rel="noopener" target="_blank">豫ICP备2021035798号 </a>
  </div>

<!-- 不知道是什么 -->
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'fH6OdGBcCG8D6jC8NeeJlX9b-gzGzoHsz',
      appKey     : '1UoIb6vszMw6wl0n7kreb4Xz',
      placeholder: "Just go go ^_^",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
    var infoEle = document.querySelector('#comments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
      infoEle.childNodes.forEach(function(item) {
        item.parentNode.removeChild(item);
      });
    }
  }, window.Valine);
});
</script>

</body>
</html>
